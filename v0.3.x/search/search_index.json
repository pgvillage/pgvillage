{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"],"fields":{"title":{"boost":1000.0},"text":{"boost":1.0},"tags":{"boost":1000000.0}}},"docs":[{"location":"about/license/","title":"License","text":"<p>The MIT License (MIT)</p> <p>Copyright (c) 2016 Andrew Rothstein</p> <p>Permission is hereby granted, free of charge, to any person obtaining a copy of this software and associated documentation files (the \"Software\"), to deal in the Software without restriction, including without limitation the rights to use, copy, modify, merge, publish, distribute, sublicense, and/or sell copies of the Software, and to permit persons to whom the Software is furnished to do so, subject to the following conditions:</p> <p>The above copyright notice and this permission notice shall be included in all copies or substantial portions of the Software.</p> <p>THE SOFTWARE IS PROVIDED \"AS IS\", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY, FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM, OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE SOFTWARE.</p>"},{"location":"administrators-guide/deployment_and_maintenance/ansible/","title":"Ansible","text":"<p>This document describes how to deploy the PostgreSQL standard building block using Ansible.</p> <p>Before running Ansible, ensure that all prerequisites are correctly configured.</p>"},{"location":"administrators-guide/deployment_and_maintenance/ansible/#1-prerequisites","title":"1. Prerequisites","text":""},{"location":"administrators-guide/deployment_and_maintenance/ansible/#11-ssh-setup","title":"1.1 SSH Setup","text":"<p>Ensure that SSH access is properly configured.</p> <ul> <li>In our predefinied deployments (<code>pgv_azure</code> and <code>pgv_vagrant</code>) this is already taken care of.   For on-prem deployments, make sure that a user with proper permissions and ssh authentication is created.</li> <li>Git clone and Ansible setup of the Ansible code (this work instruction)</li> </ul>"},{"location":"administrators-guide/deployment_and_maintenance/ansible/#2-materials-needed","title":"2. Materials needed","text":"<p>To perform this procedure, you will need:</p> <ul> <li>Access to the management server   (See also the SSH documentation)</li> <li>Access to the Ansible code repository: https://github.com/pgvillage/pgvillage</li> </ul>"},{"location":"administrators-guide/deployment_and_maintenance/ansible/#3-working-instruction","title":"3. Working instruction","text":""},{"location":"administrators-guide/deployment_and_maintenance/ansible/#step-1-clone-the-repository","title":"Step 1: Clone the repository","text":"<p>Create a Git folder in your home directory, navigate into it, and clone the repository:</p> <pre><code>mkdir -p ~/git\ncd ~/git\ngit clone git@github.com:pgvillage/pgvillage.git\n</code></pre>"},{"location":"administrators-guide/deployment_and_maintenance/ansible/#step-2-optional-adjust-the-inventory-configuration","title":"Step 2: (Optional) Adjust the inventory configuration","text":"<p>Optionally adjust the inventory configuration to suit your environment. For detailed steps, refer to: From Server to Running Database</p>"},{"location":"administrators-guide/deployment_and_maintenance/ansible/#step-3-run-the-ansible-playbook","title":"Step 3: Run the Ansible Playbook","text":""},{"location":"administrators-guide/deployment_and_maintenance/ansible/#31-navigate-to-the-ansible-directory","title":"3.1 Navigate to the Ansible directory","text":"<p>Go to the cloned Ansible repository directory:</p> <pre><code>cd ~/git/ansible-postgres\n</code></pre>"},{"location":"administrators-guide/deployment_and_maintenance/ansible/#32-run-everything","title":"3.2 Run everything","text":"<p>Execute all roles for the selected environment:</p> <pre><code>ansible-playbook -i environments/[ENV] functional-all.yml\n</code></pre>"},{"location":"administrators-guide/deployment_and_maintenance/ansible/#33-run-specific-roles-example","title":"3.3 Run specific roles (example)","text":"<p>If you want to run only specific roles (for example, <code>stolon</code> and <code>avchecker</code>):</p> <pre><code>ansible-playbook -i environments/[ENV] functional-all.yml --tags stolon,avchecker\n</code></pre>"},{"location":"administrators-guide/deployment_and_maintenance/ansible/#34-additional-examples","title":"3.4 Additional examples","text":"<p>For other related examples and procedures, refer to the following documentation:</p> <ul> <li>Chainsmith</li> <li>Inventory</li> </ul>"},{"location":"administrators-guide/deployment_and_maintenance/inventory/","title":"Inventory","text":"<p>Once the new servers are available, they can be deployed as new database, backup, and (optionally) routing servers.</p> <p>This is done using Ansible. This description can be used to load the inventory with the correct information and run Ansible to create a running database cluster.</p>"},{"location":"administrators-guide/deployment_and_maintenance/inventory/#requirements","title":"Requirements","text":"<ul> <li>3 (or more) database servers, 1 backup server (and optionally 2 router servers)</li> <li>Requested through a change request (see [from database request to server request]   (inventory.md) for more information).</li> <li>Delivered by SHS</li> <li>Management server with the correct configuration: ssh config, ansible config</li> </ul>"},{"location":"administrators-guide/deployment_and_maintenance/inventory/#instruction-manual","title":"Instruction Manual","text":""},{"location":"administrators-guide/deployment_and_maintenance/inventory/#1-verify-that-the-delivered-servers-meet-the-requirements","title":"1. Verify that the delivered servers meet the requirements","text":""},{"location":"administrators-guide/deployment_and_maintenance/inventory/#iptables","title":"iptables","text":"<ul> <li>Database servers: Ports <code>5432</code>, <code>25432</code>, <code>2379</code>, and <code>2380</code> must be open.</li> <li>Backup server: Port <code>9091</code> must be open.</li> <li>Router servers (if applicable): Ports <code>5432</code> and <code>5433</code> must be open.</li> </ul>"},{"location":"administrators-guide/deployment_and_maintenance/inventory/#storage","title":"Storage","text":"<ul> <li>Database servers: <code>/data/postgres/data</code> and <code>/data/postgres/wal</code> must exist and have sufficient space.</li> <li>Backup server: <code>/data/postgres/backup</code> must exist and have sufficient space.</li> </ul>"},{"location":"administrators-guide/deployment_and_maintenance/inventory/#cpu-and-memory","title":"CPU and Memory","text":"<ul> <li>Database server:  See server request form.</li> <li>Backup server: 1 CPU, 4 GB RAM is sufficient.</li> <li>Router servers: 1 CPU, 4 GB RAM is sufficient.</li> </ul>"},{"location":"administrators-guide/deployment_and_maintenance/inventory/#2-ensure-the-inventory-has-been-created-and-filled-out-correctly","title":"2. Ensure the inventory has been created and filled out correctly","text":"<p>The inventory can (for example) be copied from an existing inventory and possibly even assembled itself.</p> <p>Examples:</p> <ul> <li><code>environments/poc</code> explains how it works in the POC environment: PG14, including router configuration.</li> <li><code>environments/geo_a</code> relates to a solution with PG14 and PostGIS.</li> <li><code>environments/vbe_a</code> pertains to a solution with PostgreSQL 12, router, foreign data wrapper, and pgQuartz jobs.</li> </ul> <p>Let's move on to the following:</p>"},{"location":"administrators-guide/deployment_and_maintenance/inventory/#postgresql-version","title":"PostgreSQL Version","text":"<ul> <li><code>environments/[ENV]/group_vars/all/generic.yml</code>:\u00a0postgresql_version</li> <li><code>environments/[ENV]/group_vars/all/packages.yml</code>:\u00a0linux_rhsm_poolids<ul> <li>see examples in\u00a0<code>environments/vbe_t/group_vars/all/packages.yml</code> (PG12)             and\u00a0<code>environments/ geo_a/group_vars/all/packages.yml</code> (PG14)</li> </ul> </li> </ul>"},{"location":"administrators-guide/deployment_and_maintenance/inventory/#router-configuration","title":"Router Configuration","text":"<ul> <li><code>environments/[ENV]/group_vars/all/generic.yml</code>:</li> <li><code>postgresql_vip_fqdn</code></li> <li><code>postgresql_vip_ip</code></li> <li><code>postgresql_subnet</code></li> <li><code>haproxy_rw_backends</code></li> <li><code>haproxy_ro_backends</code></li> </ul>"},{"location":"administrators-guide/deployment_and_maintenance/inventory/#foreign-data-wrapper","title":"Foreign Data Wrapper","text":"<ul> <li><code>environments/[ENV]/group_vars/all/generic.yml</code>:<ul> <li><code>stolon_keeper_extra_env_vars</code></li> <li><code>stolon_package_names</code></li> </ul> </li> </ul>"},{"location":"administrators-guide/deployment_and_maintenance/inventory/#pgquartz-configuration","title":"PgQuartz Configuration","text":"<ul> <li><code>environments/[ENV]/group_vars/all/generic.yml</code>: <ul> <li><code>pgquartz_definitions</code></li> <li><code>pgquartz_jobs</code></li> </ul> </li> </ul>"},{"location":"administrators-guide/deployment_and_maintenance/inventory/#postgis","title":"PostGIS","text":"<ul> <li><code>environments/[ENV]/group_vars/hacluster/packages.yml</code>: <code>linux_packages.postgres</code></li> </ul>"},{"location":"administrators-guide/deployment_and_maintenance/inventory/#pg_hba-configuration","title":"pg_hba Configuration","text":"<ul> <li><code>environments/[ENV]/group_vars/all/generic.yml</code>: <code>stolon_pg_hba</code></li> <li> <p>The first required pg_hba entries are:</p> </li> </ul> <p>The remaining lines must follow the Host Based Access table from the database request form.</p>"},{"location":"administrators-guide/deployment_and_maintenance/inventory/#local-all-all-ident-hostssl-postgres-avchecker-samenet-cert","title":"<pre><code>local all all ident\nhostssl postgres avchecker samenet cert\n</code></pre>","text":""},{"location":"administrators-guide/deployment_and_maintenance/inventory/#creating-a-new-inventory-based-on-geo_a","title":"Creating a New Inventory Based on geo_a","text":"<ol> <li>Create a new branch:</li> </ol> <pre><code>NEW_ENV=[ENV]_[OMGEVING]\ngit checkout -b feature/$NEW_ENV dev\n</code></pre> <ol> <li>Copy existing inventory</li> </ol> <pre><code>rsync -av environments/geo_a environments/$NEW_ENV\n</code></pre> <ol> <li> <p>Modify required files:</p> </li> <li> <p><code>environments/[ENV]/hosts</code> - fill in correct hostnames  </p> </li> <li><code>environments/[ENV]/group_vars/all/generic.yml</code> - configure pg_hba  </li> </ol> <ol> <li> <p>Create the client certificates according to the work instruction generate new certificates</p> </li> <li> <p>Commit and Push the New Environment Create a new commit, push and make a merge request (create it as a draft MR)</p> </li> </ol> <pre><code>NEW_ENV=[ENV]_[OMGEVING]\ngit add environments/$NEW_ENV\ngit commit -m \"New environment $NEW_ENV\"\ngit push\n#glab, or follow the link in the output of the `git push` command\nglab mr create\n</code></pre> <ol> <li>Execute Ansible Deployment</li> <li>check the output and resolve any issues according to the Ansible documentation</li> </ol> <p>Check the result:</p> <ul> <li>Ensure that Postgres is working and check the PostgreSQL version:</li> </ul> <p><pre><code>psql --version\n\nssh gurus-dbtdb-server3.int.corp.com\n\nLast login: Thu Oct 2015 10:06 from 10.0.6.100\n\n[me@gurus-dbtdb-server3~]$ sudo iu postgres\n\n#### ClusterInfo\n\nMasterKeeper: gurus_dbtdb_server2\n\n#### Keepers/DBtree\n\ngurus_dbtdb_server2 (master)\n\u2502\u2500\u2500 gurus_dbtdb_server3\n\u2514\u2500\u2500 gurus_dbtdb_server1\n\n[postgres@gurus-dbtdb-server3~]$ psql service=proxy\n\npsql(14.5)\n\nSSL connection (protocol: TLSv1.3, cipher: TLS_AES_256_GCM_SHA384, bits: 256, compression: off)\n\nType \"help\" for help.\n\npostgres=#\n</code></pre> - Check the status of avchecker - Create users as per Extra database en/of user aanmaken</p> <p>If everything is okay after the rollout, mark the Merge Request as Ready and ensure that the Merge Request gets merged.</p>"},{"location":"administrators-guide/deployment_and_maintenance/modify_existing/","title":"Modifying an existing deployment","text":""},{"location":"administrators-guide/deployment_and_maintenance/modify_existing/#modifying-an-existing-deployment","title":"Modifying an existing deployment","text":"<p>As part of creating a new cluster, database users and databases must also be created.</p> <p>The ambition is to manage this automatically based on PGFA.</p> <p>For now, we perform this manually.</p>"},{"location":"administrators-guide/deployment_and_maintenance/modify_existing/#dependencies","title":"Dependencies","text":"<ul> <li>Ansible setup according to Ansible documentation</li> <li>A properly stored database request form in teams:</li> <li>Acme-IV-BI-Ops &gt; General &gt; Files &gt; Database Request Forms &gt;</li> <li>A running PostgreSQL cluster. Optionally, you can:</li> <li>request servers according to From database request to server request</li> <li>deploy via [link label] from servers to running database<ul> <li>This procedure is part of that procedure, so it should be good after this.</li> </ul> </li> </ul>"},{"location":"administrators-guide/deployment_and_maintenance/modify_existing/#work-instruction","title":"Work Instruction","text":""},{"location":"administrators-guide/deployment_and_maintenance/modify_existing/#1-create-user-and-database-using-ps","title":"1. Create user and database using ps","text":"<p>Create users using the psql tool:</p> <pre><code>me@gurus-dbabh-server1 ~&gt; ssh gurus-pgsdb-server1.int.corp.com\n\n[me@gurus-pgsdb-server1 ~] $ sudo -iu postgres\n\n#### Cluster Information\n\nMaster Keeper: gurus_pgssdb_l10\n\ngurus_pgsdb_server1 (master)\n\u251c\u2500gurus_pg_s_db_server2\n\u2514\u2500gurus_pg_s_db_server3\n\n[postgres@gurus-pgsdb-server1 ~]$ psql service=master\n\npsql (14.5)\n\nSSL connection (protocol: TLSv1.3, cipher: TLS_AES_256_GCM_SHA384, bits: 256, compression: off)\n\nType \"help\" for help.\n\npostgres=# CREATE USER new_user;\n\nCREATE ROLE\n\ncreate_new_db=# \\password create_new_user\n\nEnter new password for user \"new_user\":\n\nEnter it again:\n\npostgres=# CREATE DATABASE new_db OWNER new_user;\n\nCREATE DATABASE\n\nREVOKE CONNECT ON DATABASE new_db FROM PUBLIC;\n\nREVOKE\n\nnew_db=GRANT CONNECT ON DATABASE new_db TO new_user;\n\nGRANT\n\nnew_db=#\n</code></pre>"},{"location":"administrators-guide/deployment_and_maintenance/modify_existing/#2-adjustments-to-hbaconf","title":"2. Adjustments to hba.conf","text":"<p>Execute everything on the gurus-dbabh-server1:</p> <pre><code>#### Create a feature branch\n\nENV=poc\ngit checkout -b feature/changed*hba*$ENV dev\n\n#### Adjust pg_hba configuration\n\nAdjust the HBA configuration in `environments/$ENV/group_vars/all/generic.yml`\nAdjust the HBA configuration as needed.\n\nBe aware that for traffic via stolon-proxy, an accompanying SELinux rule must also be created.\n\n#### Apply the updated configuration\n\nENV=poc\nexportANSIBLE_VAULT_PASSWORD_FILE=~/git/ansible-postgres/bin/gpgvault\nansible-playbook -i environments/$ENV functional-all.yml --tags stolon\n\n#### Create a Merge Request\n\nENV=poc\ngit add environments/$ENV\ngit commit -m \"HBA adjustments $ENV\"\ngit push\n#glab, or follow the link in the output of the `git push` command\nglab mr create\n\n---\n\n#### Check if everything has been rolled out properly:\n\nme@gurus-dbabh-server1~&gt; ssh gurus-pgsdb-server1.int.corp.com\n[me@gurus-pgsdb-server1~] $ sudo -i postgres\n\n===ClusterInfo===\n\n#### MasterKeeper: gurus_pgssdb_server1\n\n---\n\n==Keepers/DBtree==\n\ngurus_pgsdb_server1 (master)\n\u251c\u2500gurus_pgsdb_server2\n\u2514\u2500gurus_pgsdb_server3\n\n[postgres@gurus-pgsdb-server1~] $ psql service=master\n</code></pre>"},{"location":"administrators-guide/deployment_and_maintenance/modify_existing/#psql-145","title":"<code>psql (14.5)</code>","text":"<pre><code>SSL connection (protocol: TLSv1.3, cipher: TLS_AES_256_GCM_SHA384, bits: 256, compression: off)\n\nType \"help\" for help.\n\npostgres=# select \\* from pg_hba_file_rules;\n\n line_number |   type    |   database   |  user_name  |    address     |    netmask     | auth_method |          options           | error\n-------------+-----------+--------------+-------------+----------------+----------------+-------------+-----------------------------+-------\n 1           | local     | postgres     | postgres    |                |                | peer        |                             |\n 2           | local     | replication  | postgres    |                |                | peer        |                             |\n 3           | hostssl   | all          | postgres    | 10.0.5.67      | 255.255.255.255| cert        | clientcert=verify-full      |\n 4           | hostssl   | replication  | postgres    | 10.0.5.67      | 255.255.255.255| cert        | clientcert=verify-full      |\n 5           | hostssl   | all          | postgres    | 10.0.5.68      | 255.255.255.255| cert        | clientcert=verify-full      |\n 6           | hostssl   | replication  | postgres    | 10.0.5.68      | 255.255.255.255| cert        | clientcert=verify-full      |\n 7           | local     | all          | all         |                |                | peer        |                             |\n 8           | hostssl   | postgres     | avchecker   | samenet        |                | cert        | clientcert=verify-full      |\n 9           | hostssl   | all          | all         | samenet        |                | cert        | clientcert=verify-full      |\n(9 rows)\n\n# postgres=#\n</code></pre> <p>If everything looks good, the status of the Merge Request can be changed to Ready.</p>"},{"location":"administrators-guide/deployment_and_maintenance/modify_existing/#3-nieuwe-client-certificaten","title":"3. Nieuwe client certificaten","text":"<p>If necessary, these can be created according to the procedure for new client certificates.</p>"},{"location":"administrators-guide/deployment_and_maintenance/patching_procedure/","title":"Patching","text":"<p>The SBB platform is specifically designed to guarantee High Availability (HA).</p> <p>Patching should have little to no impact on the running system.</p> <p>This documentation explains how patching is performed in this HA environment and what HA guarantees remain during the process.</p>"},{"location":"administrators-guide/deployment_and_maintenance/patching_procedure/#dependencies","title":"Dependencies","text":"<ul> <li>Satellite: </li> <li>https://gurus-satl6-server1.int.corp.com/</li> <li> <p>Here, the various patchsets per environment are maintained</p> </li> <li> <p>SHS Patch Process</p> </li> <li>In this process, servers are divided into server groups (A, B, C, and D)</li> <li>The server groups are patched as a whole</li> <li>Patching is done entirely automatically</li> <li> <p>The DBA team remains informed about the patching</p> </li> <li> <p>Demote Script</p> </li> <li>~postgres/bin/demote.sh (Ansible managed in the stolon role)</li> <li>Is executed before patching is performed</li> <li>Ensures that \"this server is no longer a master\"</li> <li>Only works if everything comes back correctly after patching</li> </ul>"},{"location":"administrators-guide/deployment_and_maintenance/patching_procedure/#facts","title":"Facts","text":"<ul> <li>The starting point is not that there are no issues</li> <li>the starting point is that all issues in A are encountered and resolved</li> <li> <p>the starting point is that at P all issues have been encountered and resolved</p> </li> <li> <p><code>~postgres/bin/demote.sh</code></p> </li> <li>is executed by the patching process so that switchover can be carried out quickly</li> <li>This has an impact on running transactions (they are canceled, application may reinitiate them)</li> <li> <p>This has an impact on maintenance jobs</p> </li> <li> <p>After\u00a0~postgres/bin/demote.sh, patching and reboot have almost no more impact</p> </li> <li>The impact is a catch-up when the database server is back from the reboot</li> <li> <p>The impact is also that read-only (RO) queries are terminated if the server reboots (see router).</p> </li> <li> <p>Application clients must</p> </li> <li>use Client Connect Failover, or</li> <li> <p>use a router with a VIP</p> </li> <li> <p>The router uses KeepAliveD and is configured without preferred master</p> </li> <li>the server that was master and reboots causes a failover to the other router</li> <li>per patch round, a maximum of 2 failovers can occur</li> <li>servers are patched separately, so there is always one available<ul> <li>as long as after a patch round of a group an intake takes place</li> </ul> </li> </ul>"},{"location":"administrators-guide/deployment_and_maintenance/point_in_time_restore/","title":"Point in time restore","text":"<ol> <li> <p>In almost all cases, the reason for a point-in-time restore is not due to an error in the PostgreSQL architecture or by the DBA.</p> <p>Therefore, in almost all cases, a Point-in-Time Restore does not cause application downtime.</p> <p>Take the time to perform a proper Point-in-Time Restore.</p> </li> <li> <p>In a Shared Cluster setup, it is important that a single database can be restored while the rest of the cluster remains operational.</p> <p>Actually, this is not a feature of the standard building block, but it can be executed.</p> <p>See chapter Restore of Some Databases or Tables for more information.</p> </li> </ol>"},{"location":"administrators-guide/deployment_and_maintenance/point_in_time_restore/#restore-to-point-in-time","title":"Restore to point in time","text":"<p>WAL-g supports performing a restore to Point in Time,  but the procedure in combination with stolon (High Availability Cluster Management) is complex.</p> <p>Therefore, the process is fully automated in an Ansible playbook, which performs these steps:</p> <ul> <li>Stop the stolon-keeper on all nodes</li> <li>Remove the Postgres data and recreate the folders with the correct permissions</li> <li>Generate the stolon custom_config required to perform a Point In Time Restore using stolon and wal-g</li> <li>Load the <code>custom_config</code> into the etcd config of stolon</li> <li>Start stolon-keeper (one by one per node)</li> <li>Stolon starts on the master and<ul> <li>Finds pitr as the init mode and executes the restore script.</li> <li>The restore script runs wal-g backup-fetch to restore data to the last backup for the point in time</li> <li>Starts Postgres afterwards, which performs recovery up to a specific point in time</li> <li>Uses the restore_command to retrieve WAL-files using wal-g</li> <li>Once Postgres is done with recovery (up to the point in time), PostgreSQL becomes available</li> </ul> </li> <li>PostgreSQL is again available for the application</li> <li>Meanwhile, the standbys start, they clone from the master and become ReadOnly available</li> </ul>"},{"location":"administrators-guide/deployment_and_maintenance/point_in_time_restore/#performance","title":"Performance","text":"<p>Point-in-time restore is executed from the management server.</p>"},{"location":"administrators-guide/deployment_and_maintenance/point_in_time_restore/#1-ssh-to-the-management-server","title":"1. SSH to the management server","text":""},{"location":"administrators-guide/deployment_and_maintenance/point_in_time_restore/#2-ensure-correct-ansible-configuration","title":"2. Ensure correct Ansible configuration:","text":"<ul> <li>Ansible</li> </ul>"},{"location":"administrators-guide/deployment_and_maintenance/point_in_time_restore/#3-perform-pitr-using-ansible-examples","title":"3. Perform PITR using Ansible (Examples)","text":"<p><pre><code>cd ~/git/ansible-postgres\nexport ANSIBLE_VAULT_PASSWORD_FILE=$PWD/bin/gpgvault\n\n# Restore to August 30, 2022 at 09:10:11 AM:\nansible-playbook -i environments/poc/ ./restore.yml -e 'restore_target=\"2022-08-30T09:10:11\"'\n\n# Restore to the label mylabel1:\nansible-playbook -i environments/poc/ ./restore.yml -e restore_target=\"mylabel1\"\n\n# Restore to transaction ID 50851\nansible-playbook -i environments/poc/ ./restore.yml -e restore_target=\"50851\"\n</code></pre> For a restore until the last moment, simply do not specify <code>-e restore_target=...</code>!</p> <p>!!! Notes:</p> <pre><code>The `RESTORETARGETINPUT` is written (appended) to `/etc/default/wal-g`. Even if the restore goes well, it will not be removed! **Check manually and adjust for starting restore.**\n</code></pre> <p>Also check for <code>backup</code> files in the data directory that are created after a restore on the master and make a backup after the restore!</p>"},{"location":"administrators-guide/deployment_and_maintenance/point_in_time_restore/#restore-some-database-or-table","title":"Restore some database or table","text":"<p>In a Shared Cluster setup, it's important that a single database is restored while the rest remains available.</p> <p>This is not a standard feature of the building block, but it can be executed manually:</p>"},{"location":"administrators-guide/deployment_and_maintenance/point_in_time_restore/#steps","title":"Steps","text":"<ol> <li>Prepare a standby node </li> <li>Make a standby available (stop stolon-keeper as root or with adm account and sudo)</li> <li>Optionally free up or expand disk space there</li> <li>Set PGRESTORE to a value other than PGDATA (etc. export <code>PGRESTORE=</code>)</li> <li> <p>Run the restore script with a restoration location, restore target, etc.:</p> </li> <li> <p>Run the restore script <pre><code>/opt/wal-g/scripts/restore.sh \"/data/postgres/data/restore\" \"2022-08-30 09:10:11\"\n</code></pre></p> </li> <li>Start the restored instance manually</li> </ol> <p><pre><code>/usr/pgsql-12/bin/pg_ctl start -D \"/data/postgres/data/restore\"\n````\n\n!!! Note:\n\n     This can be set up alongside an existing instance, as it starts on port 5433!!!\n\n\n4. Copy the restored data to the master\n\n- Restore a schema:\n\n```bash\npg_dump [database] -n [myschema] | psql service=master\n- Table (first truncate):\n- pg_dump [database] -t \"[myschema].[mytable]\" | psql service=master\n</code></pre> 5. Stop and clean up the restore instance</p> <p><pre><code>/usr/pgsql-12/bin/pg_ctl stop -D \"/data/postgres/data/restore\"\nrm -rf \"/data/postgres/data/restore\"  \n</code></pre> 6. Restart the Stolon Keeper  Run as root or with the adm account using sudo.</p>"},{"location":"administrators-guide/deployment_and_maintenance/refresh_certificates/","title":"Refreshing certificates","text":"<p>In the SBB PostgreSQL, an mTLS chain is used with server- and client- certificates.</p> <p>The chain is generated using Chainsmith and (re)generated using this procedure.</p>"},{"location":"administrators-guide/deployment_and_maintenance/refresh_certificates/#dependencies","title":"Dependencies","text":"<ul> <li>chainsmith</li> <li>nieuwe uitrol</li> <li>ansible-postgres</li> <li>rollout new certs</li> <li>chainsmith config</li> </ul>"},{"location":"administrators-guide/deployment_and_maintenance/refresh_certificates/#work-instructions","title":"Work Instructions","text":""},{"location":"administrators-guide/deployment_and_maintenance/refresh_certificates/#1-check-the-database-request-form-and-update-chainsmith-configuration-if-needed","title":"1. Check the database request form and update Chainsmith configuration (if needed)","text":"<p>Adjust:</p> <pre><code>ansible/config/chainsmith_[ENV].yml\n\n# If configuration changes are needed, create a merge request:\n\nENV=poc\ngit checkout dev -b \"feature/chainsmith_$(printenv ENV).yml\"\ngit add config/chainsmith\\_$ENV.yml\ngit commit -m \"New chainsmith config $ENV\"\ngit push\n#Use `glab`, or follow the link in the output of the `git push` command.\nglab mr create\n</code></pre> <p>Ensure correct certificate extensions</p> <ul> <li>JDBC requires the following extensions (both client and server):</li> <li> <p>keyUsages:</p> <ul> <li>keyEncipherment</li> <li>dataEncipherment</li> <li>digitalSignature</li> </ul> </li> <li> <p>extendedKeyUsages:</p> <ul> <li>serverAuth</li> </ul> </li> </ul>"},{"location":"administrators-guide/deployment_and_maintenance/refresh_certificates/#2-generate-the-new-certificates","title":"2. Generate the new certificates","text":"<p>There are actually three options:</p> <ol> <li>Restart Chainsmith and replace certificates with downtime</li> <li>Ideal for new environments</li> <li>Easiest, but involves downtime</li> <li>ENV=poc</li> </ol> <pre><code>  rm environments/$ENV/group_vars/all/certs{,.vault}.yml\n  bin/chainsmith.sh $ENV\n</code></pre> <ul> <li> <p>Roll out the new certificates afterwards</p> </li> <li> <p>A procedure where server and client certificates are replaced with minimal impact</p> </li> <li> <p>Manual work, but little to no downtime</p> </li> <li> <p>New client certificates added to the existing bundle</p> </li> <li>Easy and no downtime, but does not help fix issues like expiry ``` Follow one of the three procedures above. Once the certificate rollout is complete, PostgreSQL and the application will run using the refreshed certificates.</li> </ul>"},{"location":"administrators-guide/deployment_and_maintenance/replacing_certificates/","title":"Refreshing certificates (2)","text":""},{"location":"administrators-guide/deployment_and_maintenance/replacing_certificates/#refreshing-certificates-2","title":"Refreshing certificates (2)","text":"<p>Certificates are internally generated using an automation tool called chainsmith.</p> <p>The basic idea is to generate a new certificate chain and replace the old one in a single step.</p> <p>This also means there will be a period during which the application will no longer accept the PostgreSQL server certificate, and PostgreSQL will no longer accept the client certificate.</p> <p>This documentation describes a method in which certificates can be replaced through a few manual steps.</p> <p>In the best-case scenario, this results in only a few reloads of PostgreSQL and the application. In practice, a couple of restarts may be required \u2014 but even then, this is still minimal downtime compared to a downtime window of several hours.</p>"},{"location":"administrators-guide/deployment_and_maintenance/replacing_certificates/#dependencies","title":"Dependencies","text":"<ul> <li>Knowledge of:</li> <li>mTLS</li> <li>Note  that this page is a guide, but reading it does not make one an expert in mTLS</li> <li>Knowledge of Postgres and how it functions with mTLS</li> <li>Knowledge of:</li> <li>Ansible</li> <li>Ansible inventory- Ansible-vault</li> <li>Knowledge of the application and the associated PostgreSQL client and how it works with mTLS</li> <li>The option to execute this in a POC environment, a test, and an acceptance environment</li> </ul>"},{"location":"administrators-guide/deployment_and_maintenance/replacing_certificates/#work-instruction","title":"Work Instruction","text":"<p>Summary of the process:</p> <ol> <li> <p>Generate a new certificate chain    Based on: Procedure for replacing certificates with minimal downtime</p> </li> <li> <p>Adjust the configuration so the root certificates or chains include both:</p> </li> <li>Old certificate bundle</li> <li> <p>New certificate bundle</p> <p>This is done in:   - Ansible inventory   - Application configuration (reload or restart required)</p> <p>From this point:   - The application accepts both old and new server certificates   - Client still connects with old certificate</p> <p>For PostgreSQL and building block components:   - PostgreSQL accepts both old and new certificates   - PostgreSQL immediately switches to using the new certificate   - Tools such as pgQuartz, pgRoute66, and AVChecker will authenticate using the new certificate   - Application connections still use the old client certificate</p> </li> <li> <p>Update the application to use the new client certificate</p> <ul> <li>From this moment, the old bundles are no longer needed</li> </ul> </li> <li> <p>Update the bundles so that only the new certificates remain</p> <ul> <li>Critical: if a certificate in the chain expires, authentication will fail</li> <li>Adjust the Ansible inventory  </li> <li>Roll out through Ansible</li> </ul> </li> </ol>"},{"location":"administrators-guide/deployment_and_maintenance/replacing_certificates/#cancelled","title":"Cancelled","text":"<p>1: Generate a new chain according to Procedure for replacing certificates with minimal downtime.</p> <p>Save the new certificates in a Merge Request. - Create a new merge request if necessary</p> <p><pre><code>ENV=poc\ngit checkout -b \"feature/new_certs_$ENV\" dev\n#glab, or follow the link in the output of the git push command\nglab mr create\n\n# Ensure all changes are included:\n\nENV=poc\ngit add config/chainsmith\\_$ENV.yml environments/$ENV/group_vars/all/certs{,.vault}.yml\ngit commit -m \"New chainsmith configuration and certificates for $ENV\"\ngit push\n</code></pre> 2: Adjust the configuration so that the root certificates/chains contain both the old and new certificate bundles.</p> <p>The old chain can be found on the existing database servers:</p> <ul> <li><code>~postgres/.postgresql/root.crt</code></li> <li><code>/data/postgres/data/certs/root.crt</code></li> </ul> <p>The easiest way is to prepend these with spaces:</p> <pre><code>[root@gurus-pgsdb-server1 ~]# sed 's/^/    /' ~postgres/.postgresql/root.crt\n-----BEGIN CERTIFICATE-----\n\nMIIGRTCCBC2gAwIBAgIBATANBgkqhkiG9w0BAQsFADCB1zELMAkGA1UEBhMCTkwx\nEDAOBgNVBBEMBzM3MjEgTUExEDAOBgNVBAgMB1V0cmVjaHQxEjAQBgNVBAcMCUJp\nbHRob3ZlbjEiMCAGA1UECQwZQW50b25pZSB2IExlZXV3ZW5ob2VrbG4gOTE9MDsG\nA1UECgw0Umlqa3NpbnN0aXR1dXQgdm9vciBWb2xrc2dlem9uZGhlaWQgZW4gTWls\naWV1IChSSVZNKTEaMBgGA1UECwwRUG9zdGdyZXMgYm91d2Jsb2sxETAPBgNVBAMM\n\n[root@gurus-pgsdb-server1 ~]# sed 's/^/ /' /data/postgres/data/certs/root.crt\n\n-----BEGIN CERTIFICATE-----\nMIIGRTCCBC2gAwIBAgIBAjANBgkqhkiG9w0BAQsFADCB1zELMAkGA1UEBhMCTkwx\nEDAOBgNVBBEMBzM3MjEgTUExEDAOBgNVBAgMB1V0cmVjaHQxEjAQBgNVBAcMCUJp\nbHRob3ZlbjEiMCAGA1UECQwZQW50b25pZSB2IExlZXV3ZW5ob2VrbG4gOTE9MDsG\nA1UECgw0Umlqa3NpbnN0aXR1dXQgdm9vciBWb2xrc2dlem9uZGhlaWQgZW4gTWls\naWV1IChSSVZNKTEaMBgGA1UECwwRUG9zdGdyZXMgYm91d2Jsb2sxETAPBgNVBAMM\nCHBvc3RncmVzMB4XDTIyMDgxODE2NDgyOVoXDTI5MTExOTE2NDgyOVowgYsxCzAJ\n\nThe certificates can simply be added to the current inventory:\n\nENV=poc\nvim environments/$ENV/group_vars/all/certs.yml\n\n# Add the client certificate to `certs.client.chain` (right before placing the new certificate above it).\n\n# Add the server certificate to certs.server.chain (just before placing the new certificate on top).\n</code></pre> <p>Note</p> <p>By not including this change in the merge request (MR), we can easily roll it back later.</p> <p>3: Deliver the bundle of old and new chains to the Application Administrators and ask them to adjust the application configuration and load the new root certificates.</p> <p>Actually, this is what was placed at the previous step at certs.server.chain.</p> <p>4: Reconfigure Postgres and all related tools from the building block:</p> <pre><code>ENV=poc\ncd ~/git/ansible-postgres\nexport ANSIBLE_VAULT_PASSWORD_FILE=~/git/ansible-postgres/bin/gpgvault\nansible-playbook -i environments/$ENV rollout_new_certs.yml\n</code></pre> <p>5: Deliver the entire bundle (only new <code>root.crt</code>, and the client certificates, configuration readme, etc.) to the application administrator.</p> <p>The procedure The procedure for 'Antwoord aanvrager can be followed.</p> <p>Request application administrators to register with these new client certificates (configuration adjustment and reload/restart).</p> <p>6: Roll back the changes, push to GitLab, and run Ansible again.</p> <pre><code>ENV=poc\ncd ~/git/ansible-postgres\ngit reset environments/$ENV/group_vars/all/certs{,vault}.yml\nexport ANSIBLE_VAULT_PASSWORD_FILE=~/git/ansible-postgres/bin/gpgvault\nansible-playbook -i environments/$ENV rollout_new_certs.yml --tags stolon\n</code></pre> <p>7: Completion</p> <p>With this final rollout, the environment is now fully migrated to the new certificates and the procedure is complete.</p>"},{"location":"administrators-guide/troubleshooting/connectivity/","title":"Connectivity","text":"<p>To properly analyze PostgreSQL connectivity issues, it is important to understand how connections are routed and where things can fail along the path.</p> <p>This documentation describes the connection paths and provides guidance on what to investigate when troubleshooting connectivity problems.</p>"},{"location":"administrators-guide/troubleshooting/connectivity/#postgres-read-write-connections-via-the-router","title":"Postgres Read-Write Connections via the Router","text":""},{"location":"administrators-guide/troubleshooting/connectivity/#how-to-recognize","title":"How to recognize","text":"<p>Read-write connections through the router are initiated by the client using the VIP as the endpoint and port 5432 as the destination port.</p> <p>Examples:</p> <pre><code># A pg_service file:\n[myapp]\nuser=usr\npassword=pwd\nhost=acme-dvppg1pr-v01p.acme.corp.com\nport=5432\nsslmode=verify-full\ndbname=myappdb\n\n# Then a connection with service=myapp\n\n# libpq connection string:\n'user=usr password=pwd host=acme-dvppg1pr-v01p.acme.corp.com port=5432 sslmode=verify-full dbname=myappdb'\n\n# JDBC connection URL as:\npostgresql://usr:pwd@acme-dvppg1pr-v01p.acme.corp.com:5432/myappdb\n</code></pre> <p>RW router connections can be recognized by:</p> <ul> <li>Port is 5432</li> <li>Hostname contains pr-v01</li> </ul>"},{"location":"administrators-guide/troubleshooting/connectivity/#paths","title":"Paths","text":"<p>The connections follow these paths:</p> <ol> <li> <p>Starts at application level and then (OpenShift?) routing, network firewall, etc., all the way to the gateway.</p> </li> <li> <p>To the Virtual IP on port 5432</p> </li> <li> <p>The VirtualIP is linked by KeepaliveD.</p> <ul> <li>Is KeepaliveD OK?</li> <li>Is the VIP attached to one server?</li> <li>Does the network address of the VIP match the network of the interface it's connected to?</li> </ul> </li> <li> <p>Is the firewall active?</p> <ul> <li>What are the firewall rules? <code>sudo iptables -L</code>?</li> <li>Are the app servers included for port 5432?</li> </ul> </li> <li> <p>Arrives at HAProxy</p> </li> <li> <p>Is HAProxy running?</p> </li> <li>What does the haproxy stat command say?<ul> <li>Is there an active PostgresReadWrite-backend? There should be one...</li> <li>Does it match expectations? See HAProxy documentation...</li> </ul> </li> <li>Is PgRoute66 running?</li> <li>Does the logging match expectations? Run with debugging if necessary. See PgRoute66 documentation...</li> <li> <p>Are the PgRoute66 certificates OK? <code>sudo openssl x509 -text -noout -in ~pgroute66/.postgresql/postgresql.crt</code></p> <ul> <li>Check that <code>validity - Not After</code> has not expired</li> <li>Check that <code>X509v3 Key Usage: Digital Signature, Key Encipherment, Data Encipherment</code> is set</li> <li>Ensure the subject ends with <code>CN = pgroute66</code></li> </ul> </li> <li> <p>HAProxy routes to the Postgres primary port 25432</p> </li> <li> <p>Is the firewall active?</p> </li> <li>What are the firewall rules? <code>sudo iptables -L</code></li> <li> <p>Are the router nodes included for port 25432?</p> </li> <li> <p>Port 25432 is stolon-proxy:</p> </li> <li> <p>Is stolon-proxy running on the primary node?</p> </li> <li>What does the avchecker service say?<ul> <li>Problem with avchecker@stolon is an issue with Postgres on the primary node</li> <li>Problem with avchecker@proxy is an issue with stolon-proxy</li> <li>Problem with avchecker@routerrw is an issue with the router</li> </ul> </li> <li> <p>Does local connection work via stolon-proxy? <code>sudo -iu postgres bash -c 'psql service=proxy'</code></p> </li> <li> <p>Stolon-proxy routes to the primary Postgres</p> </li> <li>Is there a primary?<ul> <li>Log in to one of the database servers and switch to user postgres. Check the output of stolonctl status which identifies the primary</li> <li>Log in to the primary, become postgres (sudo -iu postgres) and check the status with <code>psql -c 'select pg_is_in_recovery()'</code> (f is expected)</li> </ul> </li> <li>Is the user allowed from the pg_hba config?<ul> <li>Note that the source IP of the traffic are the proxy nodes!!!</li> </ul> </li> <li>What does logging on the primary say? Are there login errors?</li> </ol>"},{"location":"administrators-guide/troubleshooting/connectivity/#postgresql-read-only-ro-connections-via-the-router","title":"PostgreSQL Read-Only (RO) Connections via the Router","text":""},{"location":"administrators-guide/troubleshooting/connectivity/#how-to-recognize_1","title":"How to recognize","text":"<p>Routed connections through the router are initiated by the client with the VIP as the endpoint and port 5433 as the destination port.</p> <p>Examples: <pre><code># A pg_service file with the following service:\n[myapp]\nuser=usr\npassword=pwd\nhost=acme-dvppg1pr-v01p.acme.corp.com\nport=5433\nsslmode=verify-full\ndbname=myappdb\n\n# Then a connection with service=myapp\n\n# libpq connection string:\n'user=usr password=pwd host=acme-dvppg1pr-v01p.acme.corp.com port=5433 sslmode=verify-full dbname=myappdb'\n\n# JDBC connection URL:\npostgres://usr:pwd@acme-dvppg1pr-v01p.acme.corp.com:5433/myappdb\n</code></pre></p> <p>RO connections can be recognized by:</p> <ul> <li>Port is 5433</li> <li>Hostname contains pr-v01</li> </ul>"},{"location":"administrators-guide/troubleshooting/connectivity/#paths_1","title":"Paths","text":"<p>The connections follow these paths:</p> <ol> <li> <p>Begins at the application and then routing, network firewall, etc., up to the gateway.</p> </li> <li> <p>Towards the Virtual IP on port 5433</p> <ul> <li>The VirtualIP is attached by KeepaliveD.</li> <li>Is KeepaliveD OK?</li> <li>Is the VIP attached to one server?</li> <li>Does the network address of the VIP match the network of the interface it's connected to?</li> </ul> </li> <li> <p>Is the firewall active?</p> <ul> <li>What are the firewall rules? <code>sudo iptables -L</code>?</li> <li>Are the application servers included for port 5433?</li> </ul> </li> <li> <p>Arrives at HAProxy</p> </li> <li> <p>Is HAProxy running?</p> </li> <li>What does the haproxy stat command show?<ul> <li>Is there an active PostgresReadOnly-backend? There should be one...</li> <li>Does it match expectations? See HAProxy documentation...</li> </ul> </li> <li>Is PgRoute66 running?</li> <li>Does the logging match expectations? Run with debugging temporarily if necessary. See PgRoute66 documentation...</li> <li> <p>Are the PgRoute66 certificates OK? <code>sudo openssl x509 -text -noout -in ~pgroute66/.postgresql/postgresql.crt</code></p> <ul> <li>Check that <code>validity - Not After</code> has not expired</li> <li>Ensure that <code>X509v3 Key Usage: Digital Signature, Key Encipherment, Data Encipherment</code> is set</li> <li>Verify that the subject ends with <code>CN = pgroute66</code></li> </ul> </li> <li> <p>HAProxy routes to the Postgres standbys (not primary) on port 5432</p> </li> <li>Is the firewall active?<ul> <li>What are the firewall rules? <code>sudo iptables -L</code></li> <li>Are the router nodes included for port 5432?</li> </ul> </li> <li>Is the user allowed from the pg_hba config?<ul> <li>Note that the source IP of the traffic is from the proxy nodes!!!</li> </ul> </li> <li>What does the logging on the standby servers say? Are there login errors? Are there one or more standbys?</li> <li>Log in to one of the database servers as user postgres. Check the output of stolonctl status for which are the standbys</li> <li>Log into the standbys, become postgres (sudo -iu postgres) and check the status with <code>psql -c 'select pg_is_in_recovery()'</code> (it is expected)</li> </ol>"},{"location":"administrators-guide/troubleshooting/connectivity/#stolon-proxy-postgresql-read-write-connections","title":"Stolon Proxy PostgreSQL Read-Write Connections","text":""},{"location":"administrators-guide/troubleshooting/connectivity/#how-to-recognize_2","title":"How to recognize","text":"<p>Stolon-proxy read-write connections are initiated by the client with the database servers as the endpoint and port 25432 as the destination port.</p> <p>Examples:</p> <pre><code># A `pg_service` file with the following service:\n[myapp]\nuser=usr\npassword=pwd\nhost=acme-dvppg1db-server1.acme.corp.com, acme-dvppg1db-server2.acme.corp.com, acme-dvppg1db-server3.acme.corp.com\nport=25432\nsslmode=verify-full\ndbname=myappdb\n\n# and then a connection with service=myapp\n\n# libpq connection string:\n'user=usr password=pwd host=acme-dvppg1db-server1.acme.corp.com,acme-dvppg1db-server2.acme.corp.com,acme-dvppg1db-server3.acme.corp.com port=25432 sslmode=verify-full dbname=myappdb'\n\n# JDBC connection URL:\npostgres://usr:pwd@acme-dvppg1db-server1.acme.corp.com,acme-dvppg1db-server2.acme.corp.com,acme-dvppg1db-server3.acme.corp.com:25432/myappdb\n</code></pre> <p>RW router connections can be recognized by:</p> <ul> <li>Port is 25432</li> <li>Hostnames contain db-l0</li> </ul>"},{"location":"administrators-guide/troubleshooting/connectivity/#paths_2","title":"Paths","text":"<p>The connections follow these paths:</p> <ol> <li>Starts with the application, followed by (OpenShift?) routing, network firewall, etc., up to the gateway.</li> <li>To the Database server on port 25432</li> <li>Is the firewall active?</li> <li>What are the firewall rules? <code>sudo iptables -L</code>?</li> <li>Are the app servers included for port 25432?</li> <li>Port 25432 is stolon-proxy:</li> <li>Is stolon-proxy running?</li> <li>What does the avchecker service say?<ul> <li>Issue with avchecker@stolon\u00a0is an issue with Postgres on the primary node</li> <li>Issue with avchecker@proxy is an issue with stolon-proxy</li> </ul> </li> <li>Does local connection work via stolon-proxy? <code>sudo -iu postgres bash -c 'psql service=proxy'</code></li> <li>Stolon-proxy routes to the primary postgres</li> <li>Is there a primary?<ul> <li>Log into one of the database servers as user postgres. Check the output of <code>stolonctl status</code> which shows the primary.</li> <li>Log into the primary, become postgres (sudo -iu postgres) and check the status with <code>psql -c 'select pg_is_in_recovery()'</code> (f is expected)</li> </ul> </li> <li>Is the user allowed from the pg_hba config?<ul> <li>Note that the source IP of the traffic are the proxy nodes!!!</li> </ul> </li> <li>What does the logging on the primary say? Are there login errors?</li> </ol>"},{"location":"administrators-guide/troubleshooting/connectivity/#direct-postgresql-read-write-connections","title":"Direct PostgreSQL Read-Write Connections","text":""},{"location":"administrators-guide/troubleshooting/connectivity/#how-to-recognize_3","title":"How to recognize","text":"<p>Direct RW connections are initiated by the client targeting the DB servers directly on port 5432.</p> <p>Examples:</p> <pre><code># A `pg_service` file with the following service:\n[myapp]\nuser=usr\npassword=pwd\nhost=acme-dvppg1db-server1.acme.corp.com, acme-dvppg1db-server2.acme.corp.com, acme-dvppg1db-server3.acme.corp.com\nport=5432\nsslmode=verify-full\ndbname=myappdb\ntarget_session_attrs=read-write\n\n# Then a connection with service=myapp\n\n# A libpq connection string like:\n'user=usr password=pwd host=acme-dvppg1db-server1.acme.corp.com,acme-dvppg1db-server2.acme.corp.com,acme-dvppg1db-server3.acme.corp.com port=5432 sslmode=verify-full dbname=myappdb target_session_attrs=read-write'\n\n# JDBC connection URL:\npostgres://usr:pwd@acme-dvppg1db-server1.acme.corp.com,acme-dvppg1db-server2.acme.corp.com,acme-dvppg1db-server3.acme.corp.com:5432/myappdb?targetServerType=master\n</code></pre> <p>Direct RW connections are recognizable by:</p> <ul> <li>Port 5432</li> <li>libpq: <code>target_session_attrs=read-write</code></li> <li>JDBC: <code>targetServerType=primary</code></li> <li>Hostnames contain db-l0</li> </ul>"},{"location":"administrators-guide/troubleshooting/connectivity/#paths_3","title":"Paths","text":"<p>The connections follow these paths:</p> <ol> <li> <p>Begins with the application and then (OpenShift?,) routing, network firewall, etc., up to the gateway</p> </li> <li> <p>Is OpenShift routing correctly configured?</p> </li> <li>Does the traffic indeed come from the intended IP address?</li> <li> <p>Is the firewall open for traffic from the AppServer IP to the database nodes?</p> </li> <li> <p>To the Database server on port 5432</p> </li> <li> <p>Is the firewall active?</p> </li> <li>What are the firewall rules? <code>sudo iptables -L</code></li> <li> <p>Are the app server IPs included for port 5432?</p> </li> <li> <p>On port 5432, PostgreSQL is running</p> </li> <li>Is there a primary?<ul> <li>Log in to one of the database servers as user postgres. Check the output of stolonctl status to determine which is the primary.</li> <li>Log in to the primary as postgres (sudo -iu postgres) and check the status with <code>psql -c 'select pg_is_in_recovery()'</code> (f is expected)</li> </ul> </li> <li>Is the user allowed from the pg_hba config?<ul> <li>Note that the source IPs of the traffic are the app servers!!!</li> </ul> </li> <li>What does the logging on the primary say? Are there login errors?</li> </ol>"},{"location":"administrators-guide/troubleshooting/connectivity/#direct-postgresql-connections-read-only","title":"Direct PostgreSQL Connections (Read-Only)","text":""},{"location":"administrators-guide/troubleshooting/connectivity/#how-to-recognize_4","title":"How to recognize","text":"<p>Direct RO connections are established by the client with the database hosts as endpoints and port 5432 as the destination port.</p> <p>Examples:</p> <pre><code># A `pg_service` file with the following service:\n[myapp]\nuser=usr\npassword=pwd\nhost=acme-dvppg1db-server1.acme.corp.com, acme-dvppg1db-server2.acme.corp.com, acme-dvppg1db-server3.acme.corp.com, acme-dvppg1db-server4.acme.corp.com\nport=5432\nsslmode=verify-full\ndbname=myappdb\n\n# with the v14+ driver\ntarget_session_attrs=read-only # or standby or prefer-standby\n\n# Then a connection with service=myapp\n\n# libpq connection string:\n'user=usr password=pwd host=acme-dvppg1db-server1.acme.corp.com,acme-dvppg1db-server2.acme.corp.com,acme-dvppg1db-server3.acme.corp.com,acme-dvppg1db-server4.acme.corp.com port=5432 sslmode=verify-full dbname=myappdb target_session_attrs=read-only'\n\n# JDBC connection URL as:\npostgres://usr:pwd@acme-dvppg1db-server1.acme.corp.com,acme-dvppg1db-server2.acme.corp.com,acme-dvppg1db-server3.acme.corp.com,acme-dvppg1db-server4.acme.corp.com:5432/myappdb?targetServerType=secondary\n</code></pre> <p>RO connections can be recognized by:</p> <ul> <li>Port 5432</li> <li>Hostnames contain db-l0</li> <li>libpq: <code>target_session_attrs=read-only</code> (or <code>standby</code>, <code>prefer-standby</code>)</li> <li>JDBC: <code>targetServerType=secondary</code> (or <code>slave</code>, <code>preferSlave</code>, <code>preferSecondary</code>)</li> </ul>"},{"location":"administrators-guide/troubleshooting/connectivity/#paths_4","title":"Paths","text":"<p>The connections follow these paths:</p> <ol> <li>Start at the application level and then proceed to routing (OpenShift?), network firewall, etc., until reaching the gateway.</li> <li>To the Database hostname on port 5432</li> <li>Is the firewall active?</li> <li>What are the firewall rules? <code>sudo iptables -L</code></li> <li>Are the app servers included for port 5432?</li> <li>Is the user allowed from the pg_hba configuration?<ul> <li>Note that the source IP of the traffic is the app server IP!!!</li> </ul> </li> <li>What do the logs on the standby servers say? Are there login errors? Are there one or more standbys?</li> <li>Log in to one of the database servers as user postgres. Check the output of <code>stolonctl status</code> which shows the standbys.</li> <li>Log into the standbys as postgres (sudo -iu postgres) and check the status with <code>psql -c 'select pg_is_in_recovery()'</code> (it is expected).</li> </ol>"},{"location":"administrators-guide/troubleshooting/openssl/","title":"OpenSSL","text":"<p>The <code>openssl</code> Linux command can be used for performing all certificate-related tasks, including:</p> <ul> <li>Generate new private keys, certificate signing requests, certificates, etc. (chainsmith uses openssl)</li> <li>Query information about certificates, including lifetime/expiry, subject/CommonName, x509 extensions, etc.</li> <li>Verification of the combination of a certificate and private key</li> <li>Verification of a certificate chain</li> <li>Conversion of private keys</li> </ul> <p>This documentation describes all OpenSSL commands, what they can be used for, and how certain things can be done.</p>"},{"location":"administrators-guide/troubleshooting/openssl/#commands","title":"Commands","text":""},{"location":"administrators-guide/troubleshooting/openssl/#verify-if-a-private-key-and-certificate-belong-together","title":"Verify if a private key and certificate belong together","text":"<p>Compare the MD5 hash of the modulus:</p> <pre><code>openssl rsa -modulus -noout -in \"/data/postgres/data/certs/server.key\" | openssl md5\n\nopenssl x509 -modulus -noout -in \"/data/postgres/data/certs/server.crt\" | openssl md5\n\n# To read the certificate from a .crt file:\nopenssl x509 -text -noout -in /data/postgres/data/certs/server.crt\n\n# To request a certificate from a web server:\nopenssl s_client -showcerts -servername acme-vbepr-v11a.acme.corp.com -connect acme-vbepr-v11a.acme.corp.com:443 &lt; /dev/null\n\n# To request a certificate from a PostgreSQL server:\n\nOpen a connection to initiate TLS with the PostgreSQL server on `acme-dvppg1db-server1.acme.corp.com` at port `5432`, and display the certificates using OpenSSL.\n\n# To view the certificate chain that Postgres provides:\nopenssl crl2pkcs7 -nocrl -certfile /data/postgres/data/certs/server.crt | openssl pkcs7 -print_certs\n\n# To check whether a certificate/chain is trusted by another certificate\nopenssl verify -CAfile ~postgres/.postgresql/root.crt /data/postgres/data/certs/server.crt\n\n# To check if a certificate is trusted by an intermediate and root (both separately, not in PG SBB)\nopenssl verify -CAfile ~/postgres/.postgresql/root.crt -untrusted ~/postgres/.postgresql/intermediate.crt /data/postgres/data/certs/server.crt\n</code></pre>"},{"location":"administrators-guide/troubleshooting/openssl/#troubleshooting","title":"Troubleshooting","text":""},{"location":"administrators-guide/troubleshooting/openssl/#certificate-expired","title":"Certificate expired","text":"<p>Since work is being done with a single mutual TLS (mTLS) chain where all client and server certificates have the same expiry date, the expiry can be checked on one certificate to validate the entire chain.</p> <p>To check expiration:</p> <pre><code>[postgres@acme-dvppg1db-server2 ~]$ openssl x509 -text -noout -in /data/postgres/data/certs/server.crt | grep -A2 Validity\n\n# Validity\nNotBefore: Oct 10 04:48:08 2022 GMT\nNotAfter: Oct 10 04:48:08 2023 GMT\n\nIn this example, the certificates expire on October 11, 2023 (4:48 GMT is 6:48 CEST)...\n</code></pre>"},{"location":"administrators-guide/troubleshooting/openssl/#check-if-certificate-expires-within-the-next-7-days","title":"Check if certificate expires within the next 7 days","text":"<p><pre><code>[me@acme-dvppg1db-server1 ~]$ sudo openssl x509 -checkend $(60 * 60 * 24 * 7) -noout -in ~postgres/.postgresql/postgresql.crt\n</code></pre> Certificate will not expire</p> <p>This command looks at expiration in the coming week (3600 sec \u00d7 24 hours \u00d7 7 days)</p> <p>If the certificates have expired, a new chain must be created and distributed through this procedure: Generate and Roll Out New Certificates</p>"},{"location":"administrators-guide/troubleshooting/openssl/#certificate-is-not-accepted","title":"Certificate is not accepted","text":"<p>To be able to accept connections, the software must trust the certificate, which works through the chain of trust:</p> <ul> <li>The certificate must be trusted by the intermediate certificate</li> <li>The intermediate certificate must be trusted by the root certificate</li> <li>There can be multiple intermediates applied, but for the PostgreSQL building block it is not applicable.</li> <li>The root certificate must be trusted by the software</li> </ul> <p>Examples:</p> <ul> <li>To establish an SSL connection to PostgreSQL, the client must be able to accept the server certificate.</li> <li>Postgres uses a server certificate for this purpose, which identifies itself: <code>/data/postgres/data/certs/server.crt</code></li> <li>The client (for example, <code>psql</code>) has the chain to validate the server certificate: <code>~postgres/.postgresql/root.crt</code></li> </ul> <p>!!! Note:</p> <pre><code>Actually, the intermediate should be associated with the server certificate and not with the root certificate.\n</code></pre> <ul> <li>To accept a client connection:</li> <li>The client identifies itself with a certificate: <code>~postgres/.postgresql/postgresql.crt</code></li> <li>This is validated by PostgreSQL using a chain: <code>/data/postgres/data/certs/root.crt</code></li> </ul>"},{"location":"administrators-guide/troubleshooting/openssl/#the-chain-can-be-validated-as-follows","title":"The chain can be validated as follows:","text":"<p><pre><code># Verifying the server certificate:\n[postgres@acme-dvppg1db-server2 ~]$\nopenssl verify -CAfile ~/postgres/.postgresql/root.crt /data/postgres/data/certs/server.crt\n/data/postgres/data/certs/server.crt: OK\n\n# Verifying the client certificate:\n[postgres@acme-dvppg1db-server2 ~] $ openssl verify -CAfile /data/postgres/data/certs/root.crt ~/.postgresql/postgresql.crt\n/var/lib/pgsql/.postgresql/postgresql.crt: OK\n</code></pre> In order to validate other certificates, the certificates must therefore be brought together.</p> <p>Copy anything if necessary into the <code>gurus-dbabh-server1</code> in a temporary folder.</p> <p>P.S. certificates are public data and not secret.</p> <p>So copying them to temporary folders is also not a security issue.</p>"},{"location":"administrators-guide/troubleshooting/openssl/#subject-and-common-name","title":"Subject and Common Name","text":"<p>Every certificate has a subject and a Common Name which must match:</p> <ul> <li>For server certificates: the Fully Qualified Domain Name (FQDN)</li> <li>For client certificates: the PostgreSQL username</li> </ul> <p>The subject (and the Common Name) can be easily verified using an <code>openssl</code> command:</p> <pre><code>openssl x509 -in certificate.crt -noout -subject\n\n# Example Server Certificate\nSubject: C=NL, ST=Utrecht, L=Blaricum, O=Nibble IT, OU=PgVillage, CN=localhost\nSubject: C=NL, postalCode=1261 WZ, ST=Utrecht, L=Blaricum, street=Binnendelta 1-u 2, O=Nibble IT, OU=PgVillage, CN=server1.nibble-it.local\n\n# Example Client Certificate\nSubject: CN=pgfga, O=Nibble-IT, OU=Chainmsith, L=Blaricum, ST=Utrecht, C=NL\nSubject: C=NL, postalCode=1261 WZ, ST=Utrecht, L=Blaricum, street=Binnendelta 1-u 2, O=Nibble-IT, OU=PgVillage, CN=pgfga\n</code></pre> <p>In the example, it can be seen that:</p> <ul> <li>the server certificate with Common Name\u00a0(CN) acme-dvppg1db-server2.acme.corp.com is accepted.</li> <li>The certificate will therefore be accepted when clients connect to this host.</li> <li>the client certificate with Common Name (CN) postgres</li> <li>This certificate will therefore be accepted when the client attempts to log in as the postgres user.</li> </ul>"},{"location":"administrators-guide/troubleshooting/openssl/#x509-extensions-and-san","title":"x509 Extensions and SAN","text":"<p>x509 is a certificate standard and it has defined Extensions.</p> <p>There are 2 important extensions:</p>"},{"location":"administrators-guide/troubleshooting/openssl/#key-usage","title":"Key Usage","text":"<p>JDBC sets very high requirements on client certificates, particularly that the following extensions are enabled: - Digital Signature - Key Encipherment - Data Encryption</p> <p>Chainsmith enables these extensions.</p> <p>Verify:</p> <pre><code>[postgres@acme-dvppg1db-server2 ~]$ openssl x509 -text -noout -in data/postgres/data/certs/server.crt | grep -A1 'X509v3 Key Usage:'\n\nX509v3KeyUsage:\n\nDigital Signature, Key Encipherment, Data Encipherment\n</code></pre>"},{"location":"administrators-guide/troubleshooting/openssl/#subject-alternative-name","title":"Subject Alternative Name","text":"<p>Since TCP proxies (such as stolon-proxy and HAProxy) are also used in the PostgreSQL architecture, it is possible that a client connects to an FQDN different from that of the server they are actually connecting to.</p> <p>For example, he connects to the VIP (acme-dvppg1 pr-v 01p.acme.corp.com) and accesses the primary database server, such as\u00a0acme-dvppg1 pr-v 02p.acme.corp.com, via HAProxy and stolon-proxy.</p> <p>X.509 has an additional extension called Subject Alternative Names (SAN), which allows for configuring extra hostnames.</p> <p>These can be requested via:</p> <pre><code>[postgres@acme-dvppg1db-server2 ~]$ openssl x509 -text -noout -in /data/postgres/data/certs/server.crt | grep -A1 'X509v3 Subject Alternative Name:'\n</code></pre> <p>!!! Note:</p> <pre><code>The translation task does not apply to shell commands or file paths.\n</code></pre> <pre><code>X509v3 Subject Alternative Name:\n\nDNS: acme-dvppg1db-server2.acme.corp.com, IP Address: 10.0.4.43  \nDNS: acme-dvppg1pr-v01p.acme.corp.com, IP Address: 10.0.4.28  \nDNS: acme-dvppg1pr-server1.acme.corp.com, IP Address: 10.0.4.26  \nDNS: acme-dvppg1pr-server2.acme.corp.com, IP Address: 10.0.4.27  \nDNS: acme-dvppg1db-server1.acme.corp.com, IP Address: 10.0.4.42  \nDNS: acme-dvppg1db-server3.acme.corp.com, IP Address: 10.0.4.44  \nDNS: acme-dvppg1db-server4.acme.corp.com, IP Address: 10.0.4.45\n</code></pre> <p>!!! Note:</p> <pre><code>In addition to DNS entries, this SAN also has IP entries, but it seems the PostgreSQL client does not handle them well.\n</code></pre>"},{"location":"administrators-guide/troubleshooting/openssl/#private-key-conversion","title":"Private Key Conversion","text":"<p>JDBC requires the key in either PKCS12 or PKCS8 DER format. DBeaver specifically requires PKCS8 DER (no password).</p> <pre><code>#PKCS#8 PEM Format\nopenssl pkcs8 -topk8 -inform PEM -outform PEM -in cims_rw.key -out cims_rw.pk8.pem -nocrypt\n\n# PKCS8 DER Format (Suitable for JDBC including DBeaver)\nopenssl pkcs8 -topk8 -inform PEM -outform DER -in cims_rw.key -out cims_rw.pk8 -nocrypt\n\n# PKCS12 Format (Also Suitable for JDBC, Always with Password)\nopenssl pkcs12 -export -nocerts -inkey cims_rw.key -out cims_rw.p12\n\n# In some cases, a password must be set for the keys:\n\n#PKCS#8 PEM Format\nopenssl pkcs8 -topk8 -inform PEM -outform PEM -in cims_rw.key -out cims_rw.pk8.pem\n\n# PKCS8 DER Format (Suitable for JDBC Including DBeaver)\nopenssl pkcs8 -topk8 -inform PEM -outform DER -in cims_rw.key -out cims_rw.pk8\n\n# PKCS12 Format (Suitable for JDBC, Always with Password So Same Command)\nopenssl pkcs12 -export -nokeys -in cims_rw.pem -out cims_rw.p12\n</code></pre> <p>Chainsmith generates automatically the standard PEM and PKCS8 format keys.</p> <p>These can be found in the temporary folder (or the GPG archive).</p> <p>Example for client certificates for the Postgres user:</p> <ul> <li><code>./tls/int_client/private/postgres.key.pem</code></li> </ul> <p>PEM format</p> <ul> <li><code>./tls/int_client/private/postgres.key.pk8</code></li> </ul> <p>Note: The file path appears to be in English.</p> <p>PKCS#8 format (ASCII)</p> <ul> <li><code>./tls/int_client/private/postgres.key.der</code></li> </ul> <p>PK8 format (DATA)</p>"},{"location":"administrators-guide/troubleshooting/quick_fix/","title":"On-Call Troubleshooting Guide","text":"<p>This page helps you during on-call duty to quickly analyze issues and determine the correct resolution path.</p> <p>Start by identifying the type of problem you are dealing with, then follow the referenced documentation to analyze and resolve it correctly.</p>"},{"location":"administrators-guide/troubleshooting/quick_fix/#application-does-not-have-access-to-postgres","title":"Application does not have access to Postgres","text":"<p>An application requires three key components to successfully access PostgreSQL:</p> <ol> <li>An available PostgreSQL instance</li> <li>Network connectivity</li> <li>A valid configuration</li> </ol> <p>Therefore, the first step in troubleshooting is to identify which of these areas is causing the issue.</p>"},{"location":"administrators-guide/troubleshooting/quick_fix/#1-check-if-postgres-itself-is-available","title":"1. Check if Postgres itself is available:","text":"<ul> <li>Use the checks described in the avchecker documentation.</li> <li>Resolve all issues so that avchecker reports again that Postgres is available.</li> </ul>"},{"location":"administrators-guide/troubleshooting/quick_fix/#2-check-if-postgres-is-available-for-the-application","title":"2. Check if Postgres is available for the application:","text":"<ul> <li>Network problems are outside the scope of the DBA.</li> </ul> <p>In principle, these kinds of issues are always resolved by network management, or Container Hosting (CHP).</p> <p>Conduct the direction yourself, stay engaged in the process and provide clear information on what works (availability within the Postgres architecture) and what does not work (connectivity of the application to the VIP or to Postgres).</p> <ul> <li>For more information, see the documentation on Connections and Connection Paths</li> </ul>"},{"location":"administrators-guide/troubleshooting/quick_fix/#3-check-if-the-client-is-correctly-configured","title":"3. Check if the client is correctly configured:","text":"<p>Note</p> <p>Issues caused by incorrect configuration usually result from recent changes and are not part of service availability work.</p> <p>Ensure that the client configuration includes:</p> <ul> <li>Host: VIP, or a list of PostgreSQL hosts (comma-separated)</li> <li>Port:</li> <li>5432 (RW on VIP)</li> <li>5433 (RO on VIP)</li> <li>25432 (via stolon-proxy)</li> <li>5432 (direct connection)</li> <li>Username and database name</li> <li>Authentication: client certificates (preferred) or password</li> <li>Session targeting: <ul> <li><code>target_session_attrs</code> (libpq)  </li> <li><code>targetServerType</code> (JDBC)</li> </ul> </li> <li>SSL mode: <code>sslmode=verify-full</code></li> </ul> <p>Additionally:</p> <ul> <li>Verify the PostgreSQL pg_hba.conf configuration.</li> <li>Review application logs with the app administrator.</li> <li>Check for PostgreSQL log errors.</li> <li>For more information, see the documentation on client configuration and about mTLS.</li> </ul>"},{"location":"administrators-guide/troubleshooting/quick_fix/#recovery-emergency-restore","title":"Recovery / Emergency Restore","text":"<p>It may happen that an application administrator requests a point-in-time restore to be performed, for example because too much data has been deleted or to roll back database changes from an application update.</p> <p>It is also possible that due to a disaster scenario all replica instances are no longer available and can only be restored using a Restore (latest point in time).</p> <p>In both situations, this can be resolved by referring to the Point in Time Restore documentation.</p> <p>Note</p> <p>In almost all cases, the reason for a point-in-time restore is not due to an error in the Postgres architecture or by the DBA.</p> <p>Therefore, in almost all cases, a point-in-time restore does not result in service downtime. Take the time to perform a proper point-in-time restore.</p>"},{"location":"administrators-guide/troubleshooting/verify/","title":"Component: PostgreSQL Building Block","text":"<p>The PostgreSQL Building Block consists of the following components:</p> <ul> <li>PostgreSQL</li> <li>Stolon</li> <li>Etcd</li> <li>wal-g</li> </ul>"},{"location":"administrators-guide/troubleshooting/verify/#requirements-and-dependencies","title":"Requirements and Dependencies","text":"<p>PostgreSQL</p> <ol> <li><code>sudo su - postgres</code></li> <li><code>psql</code></li> <li><code>create database pgbench;</code></li> <li><code>\\q</code></li> <li><code>/usr/pgsql-12/bin/pgbench -i -s10000 pgbench</code> # filling</li> <li>Test: <code>/usr/pgsql-12/bin/pgbench -c 10 -l -j 4 -P 120 -T 600 pgbench</code></li> </ol>"},{"location":"administrators-guide/troubleshooting/verify/#poc-results","title":"POC Results","text":"<pre><code>starting vacuum...end.\n\nprogress: 120.0 s, 210.3 tps, lat 47.515 ms stddev 29.338\nprogress: 240.0 s, 275.6 tps, lat 36.283 ms stddev 21.895\nprogress: 360.0 s, 415.2 tps, lat 24.077 ms stddev 49.606\nprogress: 480.0 s, 497.7 tps, lat 20.087 ms stddev 18.762\nprogress: 600.0 s, 522.0 tps, lat 19.151 ms stddev 16.870\n\nTransaction Type: &lt;builtin: TPC-B&gt;\nScaling factor: 10000\nquery mode: simple\nnumber of clients: 10\nnumber of threads: 4\nduration: 600 s\n\nnumber of transactions actually processed: 230507\naverage latency = 26.023 ms\nlatency stddev = 30.854 ms\n\ntps = 384.125073 (including connection establishments)\ntps = 384.128736 (excluding connection establishments)\n</code></pre>"},{"location":"administrators-guide/troubleshooting/verify/#etcd","title":"Etcd","text":"<p>As postgres: <code>etcdctl check performance</code> This command can be run on any of the three nodes.</p>"},{"location":"administrators-guide/troubleshooting/verify/#stolon","title":"Stolon","text":"<pre><code>On the master:\n/home/postgres/bin/demote.sh\n\nOn a standby:\n/home/postgres/bin/reinstate.sh\n\nOn a keeper (master or standby):\nreboot\nkill -9 stolon-keeper\n\nAs the PostgreSQL user:\nstolonctl status\n\n== Active Sentinels ==\n\nID           LEADER\n81cfd599     false\nd4c10d79     true\nf71041f5     false\n\n=== Active Proxies ===\n\nID\n6b72f506\nb9e1cb00\ncde31aea\n\n=== Keepers ===\n\nUID                   HEALTHY  PG LISTEN ADDRESS   PG HEALTHY  PG WANTED GEN  PG CUR GEN\ngurus_pgsdb_server1   true     10.0.5.66:5432      true        2              2\ngurus_pgsdb_server2   true     10.0.5.67:5432      true        5              5\ngurus_pgsdb_server3   true     10.0.5.68:5432      true        2              2\n\n=== Cluster Information ===\n\nMaster Keeper: gurus_pgsdb_server2\n\n== Keepers/Database Tree ==\n\ngurus_pgsdb_server2 (master)\n\u251c\u2500 gurus_pgsdb_server3\n\u2514\u2500 gurus_pgsdb_server1\n\n# Run 24/5/2022\n[postgres@gurus-pgsdb-server2 ~]$ /usr/pgsql-12/bin/pgbench -c 10 -l -j 4 -P 120 -T 600 pgbench\n\nstarting vacuum...end.\nprogress: 120.0 s, 365.6 tps, lat 27.336 ms stddev 15.355\nprogress: 240.0 s, 361.2 tps, lat 27.673 ms stddev 16.090\nprogress: 360.0 s, 289.1 tps, lat 34.586 ms stddev 40.564\nprogress: 480.0 s, 343.4 tps, lat 29.108 ms stddev 17.820\nprogress: 600.0 s, 354.7 tps, lat 28.194 ms stddev 17.010\n\n# Additional info:\n\ntransaction type: &lt;builtin: TPC-B&gt;\nscaling factor: 10000\nquery mode: simple\nnumber of clients: 10\nnumber of threads: 4\nduration: 600 s\n\n\nNumber of transactions processed: 205685\naverage latency = 29.163 ms\nlatency stddev = 22.633 ms\ntps = 342.771509 (including connection establishments)\ntps = 342.778836 (excluding connection establishments)\n</code></pre>"},{"location":"administrators-guide/troubleshooting/verify/#execution","title":"Execution","text":""},{"location":"architecture/mtls/","title":"mTLS","text":"<p>The building block uses certificates for encryption of network traffic (server certificates) and for authentication (client certificates). For situations where one chain is comprised of both client and server certificates, this is called mTLS.</p>"},{"location":"architecture/mtls/#chainsmith","title":"chainsmith","text":"<p>The default option is to use chainsmith to create a single chain for every new cluster. The chain contains</p> <ul> <li>a freshly created root certificate</li> <li>2 intermediates, server and client, both signed by the root</li> <li>a server certificate for every server that the cluster is comprised of, all of them signed by the server intermediate</li> <li>client certificates for every service requiring certificate authentication, all of them signed by the client intermediate</li> </ul>"},{"location":"architecture/mtls/#bring-your-own","title":"Bring your own","text":"<p>As an alternative you have the option to generate your own certificates from your own intermediate.</p> <p>Note</p> <p>Signing client certificates by your root certificate brings down security, and is not advised</p>"},{"location":"architecture/mtls/#background-information","title":"Background information","text":"<ul> <li>Background information about the tool:\u00a0chainsmith</li> </ul>"},{"location":"architecture/overview/","title":"Overview","text":""},{"location":"architecture/overview/#what-pgvillage-is-not","title":"What PgVillage is NOT","text":"<p>Imagine a large (capital?) city. So much traffic, so much noise, so much going on,too much to follow. There is zero trustworthyness, but Everybody has access, nobody has oversight or control, and everybody seems annonymous, untrustworthy, and to be there for their own benefit. The streets are filthy, the river enters the city heavilly poluted, and leaves the city even more poluted.</p> <p>Note</p> <p>We love cities, just trying to paint a picture ;)</p> <p>Now imagine your database solutions to be resembled by this city, the data being resembled by the data. And you know what PgVillage is not. PgVillage is the very opposite.</p>"},{"location":"architecture/overview/#what-pgvillage-is","title":"What PgVillage IS","text":"<p>With PgVillage, everything is calm, the data that enters should be clean, or at least it stands out as being dirty because of the input, not because of the infrastructure itself. There has been put a lot of effort into authorization:</p> <ul> <li>Peer (your neighbour) if you can</li> <li>cert (thrusted by a thrustworthy 3rd party, with all papers in place) for everything your neighbor cannot help</li> <li>LDAP, Kerberos, oAuth for maintenance guys, bt only those that should have access. And they are still someone your probably know on a personal level</li> </ul> <p>PgVillage offers a robust architecture, with state of the art infrastructure, such as</p> <ul> <li>Stolon relying on etcd for High Availability</li> <li>WAL-g and minio for backup</li> <li>Prometheus / Grafana for metrics, alerts and dashboards</li> <li>Properly connect to PostgreSQL</li> <li>with Client Connection Failover if you can, or</li> <li>with stolon-proxy which always points at the current pimary, or</li> <li>use a proxy with pgroute66, haproxy and optionally keepalived for VIP management if you want.</li> <li>PgQuartz for PostgreSQL miantenance</li> <li>Implement federated authorization (ldap sync), and manage the resources such as databases, extensions, users, roles, etc. with pgfga</li> <li>With PgVillage you are in control of how you leverage the power of PostgreSQL.   Run your favorite distribution, deploay and maintain with ease, use the Power of PostgreSQL with Foreign Data Wrappers such as db2_fdw</li> </ul> <p>With PgVillage you keep your data in a safe, trustworthy place. No Vendor Lockin, no corporate suits chasing you for money, just support when you need it and for what you need it. With PgVillage you are relaxed, healthy, in control, and happy.</p>"},{"location":"architecture/overview/#design","title":"Design","text":""},{"location":"architecture/pg_service/","title":"postres services","text":"<p>Connecting to Postgres may require a lot of configuration, including user, database, multiple hosts, connection parameters, etc.</p> <p>Note</p> <p>Please don;t configure passwords clear text in configuration files!!!</p> <p>PostgreSQL clients can be used with a pg_service.conf file to configure services, where every service represents a connection with different setting.</p> <p>PgVillage creates a ~postgres/pg_service.conf file whichh conatins all types of connections that the linux user for the postgres service could require.</p> <p>Example</p> <p>[local] host=/tmp port=5432</p> <p>[proxy] host=127.0.01 port=25432 sslmode=verify-full</p> <p>[master] host=host1.mydomain.org,host2.mydomain.org,host3.mydomain.org port=5432 target_session_attrs=read-write sslmode=verify-full</p> <p>[standby] host=host1.mydomain.org,host2.mydomain.org,host3.mydomain.org port=5432 target_session_attrs=read-write</p>"},{"location":"architecture/pg_service/#background","title":"Background","text":"<p>For more information, please refer to PostgreSQL docs.</p>"},{"location":"archive/openssl_client_certs/","title":"Openssl client certs","text":"<p>Note: For automation and reproducibility, ChainSmith is used. and Documentation Generate and Roll Out New Certificates</p>"},{"location":"archive/openssl_client_certs/#introduction","title":"Introduction","text":"<p>Binnen deze bouwsteen worden client certificaten gebruikt voor authenticatie van postgres users.</p> <p>Client certificaat authenticatie heeft als voordeel dat het certificaat en de key niet op de PostgreSQL database servers bekend hoeft te zijn.</p> <p>PostgreSQL gebruikt een root certificaat om de betrouwbaarheid van het client certificaat te controleren.</p> <p>Dat kan (als het intermediate met de sleutel bewaard is) ook achteraf nieuwe certificaten worden uitgegeven.</p> <p>Further, each client certificate is issued for a specific user, making it harder to remember and copy, and requiring server TLS.</p> <p>Het is dus een veiligere oplossing. Het vereist echter ook een hoger kennis niveau, met name in beheer.</p>"},{"location":"archive/openssl_client_certs/#benodigdheden","title":"Benodigdheden","text":"<p>Om met client certificaten te kunnen authentiseren is het volgende nodig:</p> <ul> <li>A TLS connection. That means:</li> <li><code>hostssl</code> in the HBA file</li> <li><code>ssl=on</code> within PostgreSQL</li> <li>a valid server certificate<ul> <li>not expired</li> <li>corresponding to the server FQDN</li> </ul> </li> <li>a root certificate on the client that trusts the server certificate</li> <li>A valid client certificate</li> <li>not expired</li> <li>corresponding to the user who is logging in</li> <li>A root certificate on the PostgreSQL database server that trusts the client certificate</li> <li>A rule in the PostgreSQL HBA file that prescribes certificates as an authentication method for this communication</li> <li>Managed with Ansible: <code>environments/[ENVIRONMENT]/group_vars/all/generic.yml</code></li> <li>Example:</li> </ul> <p>hostssl cmdbvm cmdb 10.0.6.188/32 cert</p> <p>Note: Both <code>hostssl</code> and <code>cert</code> are required.</p> <ul> <li>The rule must be the first one that applies (not under another rule with a different method).</li> </ul>"},{"location":"archive/openssl_client_certs/#reference-material","title":"Reference material","text":"<p>Note: This is reference documentation. Use Chainsmith and the generate and roll out new certificates documentation</p> <p>Voor het genereren van client certificaten is alleen een recente versie van openssl nodig.</p> <p>Verder beschrijft deze WI alle stappen die nodig zijn voor het genereren van een nieuwe chain.</p> <p>The WI is inspired by, among other things:</p> <pre><code>[https://www.makethenmakeinstall.com/2014/05/ssl-client-authentication-step-by-step/](https://www.makethenmakeinstall.com/2014/05/ssl-client-authentication-step-by-step/)\n</code></pre>"},{"location":"archive/openssl_client_certs/#aanmaken-ca","title":"Aanmaken CA","text":"<p>certificaten worden altijd ondertekend door een ander certificaat.</p> <p>Voor server certificaten kan dit extern signed worden met een lange chain van intermediate certs.</p> <p>En in andere situaties kan een certificaat self signed zijn.</p> <p>Voor client certificaten is het nodig dat er een CA is.</p> <p>And for our application, an intermediate actually has little added value.</p> <p>We gaan daarom een CA maken die direct client certificaten kan ondertekenen.</p> <p>On the system with OpenSSL:</p> <p># Go to a new folder. Here come the temporary certificates and private keys:</p> <p>cd$(mktemp -d)</p>"},{"location":"archive/openssl_client_certs/#copy-etcpkitlsopensslcnf-into-this-folder","title":"Copy <code>/etc/pki/tls/openssl.cnf</code> into this folder:","text":"<p>cp /etc/pki/tls/openssl.cnf ./ca.cnf</p> <p>Then adjust the following in the locally copied <code>ca.cnf</code>:</p> <pre><code>[Your specific changes here]\n</code></pre> <ul> <li>In the chapter [ req ], it must:</li> <li>remove an existing option: <code>attributes = req_attributes</code></li> <li>add a new option: <code>prompt = no</code></li> <li>The entire existing content in the chapter [ req_distinguished_name ] can be replaced with:</li> </ul> <pre><code>CN = **[REPLACED BY VIP HOSTNAME FOR API OR POSTGRES]**\n</code></pre> <p>C = NL</p> <p>UT = Utrecht</p> <pre><code>L = Bilthoven\n</code></pre> <pre><code>= \u00a0National Institute for Public Health and the Environment (Acme)\n- In the chapter `[usr_cert]`, `nsCertType = client, email` must be set\n</code></pre> <p>Example diff:</p> <p>diff /etc/pki/tls/openssl.cnf ca.cnf</p> <p>110d109</p> <p>&lt; attributes \u00a0= req_attributes</p> <p>111a111</p> <pre><code>&gt; prompt = no\n</code></pre> <p>129,156c129,133</p> <pre><code>countryName = Landnaam (2 letters code)\n</code></pre> <pre><code>&lt;countryName_default = XX&gt;\n</code></pre> <pre><code>&lt; countryName_min = 2 &gt;\n</code></pre> <pre><code>&lt; countryName_max \u00a0 \u00a0 = 2 &gt;\n</code></pre> <p>&lt;</p> <pre><code>&lt;stateOrProvinceName&gt; = State or Province Name (full name)\n</code></pre> <pre><code># stateOrProvinceName_default = Default Province\n</code></pre> <p>&lt;</p> <p>&lt; localityName \u00a0\u00a0= Locality Name (eg, city)</p> <pre><code>&lt;localityName_default = Default City&gt;\n</code></pre> <p>&lt;</p> <pre><code>&lt; 0.organizationName = Organization Name (e.g., company)\n</code></pre> <pre><code>&lt; 0.organizationName_default = Default Company Ltd&gt;\n</code></pre> <p>&lt;</p> <pre><code># We can do this, but it's not usually necessary :-)\n</code></pre> <pre><code>#1.organizationName = Second Organization Name (e.g., company)\n</code></pre> <pre><code>#1.organizationName_default = World Wide Web Pty Ltd\n</code></pre> <p>&lt;</p> <pre><code>organizationalUnitName = Organizational Unit Name (e.g., section)\n</code></pre> <pre><code># organizationalUnitName_default =\n</code></pre> <p>&lt;</p> <pre><code>commonName = Common Name (e.g., your name or your server's hostname)\n</code></pre> <pre><code>&lt; commonName_max \u00a0\u00a0= 64 &gt;\n</code></pre> <p>&lt;</p> <pre><code>emailAddress = Email Address\n</code></pre> <pre><code>emailAddress_max = 64\n</code></pre> <p>&lt;</p> <pre><code># SET-ex3 = SET extension number 3\n</code></pre> <p>---</p> <pre><code>&gt; CN = acme-vbepr-v01a.acme.corp.com\n</code></pre> <pre><code>&gt; C = NL\n</code></pre> <pre><code>ST = Utrecht\n</code></pre> <p>L = Bilthoven</p> <p>O = National Institute for Public Health and the Environment (Acme)</p> <p>184c161</p> <pre><code># nsCertType = client, email\n</code></pre> <p>---</p> <pre><code>nsCertType = client, email\n</code></pre> <p>Then the root certificate can be created:</p> <p>openssl req -newkey rsa:4096 -nodes -keyform PEM -keyout ca.pem -x509 -days 3650 -outform PEM -out ca.cer -config ca.cnf</p> <p>Generating a 4096 bit RSA private key</p> <p>.........................................................++</p> <p>.................................++</p> <pre><code>generating a new private key and saving it to 'ca.pem'\n</code></pre> <p>-----</p>"},{"location":"archive/openssl_client_certs/#aanmaken-van-een-client-certificaat","title":"Aanmaken van een client certificaat","text":"<p>Take care:</p> <pre><code>- For Postgres: Create a client certificate for every user who logs in with client certificates. Use the username as CN in the config file:\n  - postgres\n  - avchecker\n  - pgquartz\n  - pgfga\n</code></pre> <p>For each client, a separate request must be made. Example for <code>pgrep_user</code>:</p>"},{"location":"archive/openssl_client_certs/#copy-the-config-and-adjust-cn-to-the-username","title":"Copy the config and adjust CN = ... to the username","text":"<pre><code>sed 's/CN \\*=.* /CN = pgquartz/ ' ca.cnf &gt; pgquartz.cnf\n</code></pre> <p># Generate a new private key:</p> <p>openssl genrsa -out pgquartz.key 4096</p>"},{"location":"archive/openssl_client_certs/#generate-a-new-csr","title":"Generate a New CSR","text":"<pre><code>openssl req -new -key pgquartz.key -out pgquartz.req -config pgquartz.cnf\n</code></pre> <p># Sign the certificate (fill in your own made-up password from step 1)</p> <p>openssl x509 -req -in pgquartz.req -CA ca.cer -CAkey ca.pem -set_serial 101 -extensions client -days 365 -outform PEM -out pgquartz.crt</p>"},{"location":"archive/openssl_client_certs/#distribution","title":"Distribution","text":"<p>Use the documentation for Generate and Roll Out New Certificates</p>"},{"location":"archive/openssl_server_certs/","title":"Openssl server certs","text":"<p>Note: For automation and reproducibility, ChainSmith is used.</p>"},{"location":"archive/openssl_server_certs/#introduction","title":"Introduction","text":"<p>Binnen deze bouwsteen worden server certificaten gebruikt voor identificatie van postgres database servers.</p> <p>Dit heeft als voordeel dat de client met abslute zekerheid weet dat de communicatie direct et de database server plaats vindt.</p> <p>Furthermore, the traffic is encrypted in such a way that only the client and server know about the communication (TLS in transit).</p> <p>The PostgreSQL client uses a root certificate to verify the trustworthiness of the PostgreSQL database server.</p>"},{"location":"archive/openssl_server_certs/#requirements","title":"Requirements","text":"<p>To be able to authenticate with client certificates, the following is needed:</p> <ul> <li>hostssl in the hba file</li> <li>ssl=on within postgres</li> <li>a valid server certificate</li> <li>not yet expired</li> <li>matching the server FQDN</li> <li>a root certificate on the client that trusts the server certificate</li> </ul>"},{"location":"archive/openssl_server_certs/#how-it-works","title":"How it works","text":""},{"location":"archive/openssl_server_certs/#why-san-certificates","title":"Why SAN Certificates?","text":"<p>For the FrontEnd, a SAN certificate is not required, but it's still nice to have one.</p> <p>Het primaire verkeer komt binnen op de VIP en derhalve moet de CN van het certificaat overeenkomen met de CN van de VIP.</p> <p>The VIP can be linked to two hosts and on both hosts, HAProxy ensures that the traffic is balanced across two other hosts.</p> <p>Since the API can be called via the VIP, but also on the hostnames of the 4 hosts, the certificate should actually have all 4 hosts as alternate names.</p> <p>For Postgres certificates, there are more types of traffic and also traffic directed towards hosts. In fact:</p> <ul> <li>Most of the traffic comes in on the VIP.</li> <li>However, there is also replication traffic to the master and to the cascading standby.</li> <li>There is also traffic from PgBouncer (on the proxies) to the master database server.</li> <li>Connecting to the VIP means that you are transparently connected at the TCP level to:</li> <li>Postgres on the RW part (5432)</li> <li>Postgres on the RO part (5433)</li> <li>PgBouncer on the RO part (6433)</li> </ul> <p>This means, for example, that a connection on VIP:5432 sees the same certificate as a replication connection on MASTER:5432 (and those are different hostnames).</p> <p>It is possible to make (part of) the traffic accept a certificate with an incorrect CN, but it's better to use a SAN certificate. <pre><code>### For which hosts?\n\nThe following certificates need to be applied for:\n\n---\n\n1. A certificate for PostgreSQL traffic (backend) for the following CN/Alternatives:\n</code></pre></p> <p>De huidige chainsmith configuratie genereert aparte certificaten voor iedere host.</p> <p>This can be with Ansible inventory adjustments 1 certificate    - VIP (CN)    - Backend Proxy 1 and 2 (Alternative)    - All postgres servers (Alternative)  </p> <ol> <li>A certificate for the backup server. This traffic is actually completely separated, but the existing automation for certificate chains is reused to also create this chain.</li> </ol>"},{"location":"archive/openssl_server_certs/#hoe-de-csr-aanmaken","title":"Hoe de CSR aanmaken","text":"<p>Note: This is reference documentation. Use Documentatie uitrollen nieuwe certificaten</p> <p>As a basis, the procedure used is found on this link.</p> <pre><code>1: Create a configuration file for OpenSSL. Save this as req.conf.\n</code></pre> <p>Voorbeelden in het volgende hoofdstuk</p> <p>2: Generate the CSR and the private key:</p> <p># Generate</p> <pre><code>openssl req -new -out company_san.csr -newkey rsa:4096 -nodes -sha256 -keyout company_san.key.temp -config req.conf\n</code></pre>"},{"location":"archive/openssl_server_certs/#convert-the-key-into-pkcs1","title":"Convert the key into PKCS#1","text":"<pre><code>openssl rsa -in company_san.key.temp -out company_san.key\n</code></pre> <p># Zet de CSR in leesbaar formaat in een file ernaast</p> <pre><code>openssl req -text -noout -verify -in company_san.csr &gt; company_san.csr.txt\n</code></pre> <p>Or a simple little script:</p> <pre><code># In the Folder with the Configuration Files:\n</code></pre> <pre><code># cat generate.sh\n</code></pre>"},{"location":"archive/openssl_server_certs/#binbash","title":"!/bin/bash","text":"<pre><code>ENDPOINT=${1:-unknown}\n</code></pre> <pre><code>`-f ${ENDPOINT}.conf || echo \"${ENDPOINT}.conf does not exist\"; exit 1`\n</code></pre> <pre><code>openssl req -new -out ${ENDPOINT}.csr -newkey rsa:4096 -nodes -sha256 -keyout ${ENDPOINT}.key.temp -config ${ENDPOINT}.conf\n</code></pre> <pre><code>openssl req -text -noout -verify -in ${ENDPOINT}.csr &gt; ${ENDPOINT}.csr.txt\n</code></pre> <pre><code>openssl rsa -in ${ENDPOINT}.key.temp -out ${ENDPOINT}.pem\n</code></pre> <pre><code>sed 's/^/        /'${ENDPOINT}.pem\n</code></pre> <p># Call:</p> <p>./generate.sh abackend</p> <p>./generate.sh afrontend</p> <p>./generate.sh awitness</p> <pre><code>3: Store the private key in Ansible Vault\n</code></pre> <ul> <li>Located in <code>vault/certs/</code></li> <li>Most are symlinks.</li> <li>You need the <code>acme-vbepr-v*.acme.corp.com.yml</code> and the <code>acme-vberm-l01*.acme.corp.com.yml</code>.</li> <li>Open with <code>ansible-vault edit [file]</code></li> <li>Pay attention to the alignment (2 spaces) under <code>private_ssl_key</code>.</li> </ul> <pre><code>4: Create a JIRA ticket with the details:\n</code></pre> <ul> <li>With label BI-CIF-SERVICES</li> <li>With the CSR (company_san.csr and company_san.csr.txt)</li> <li>With a description of what the CSR is for</li> </ul>"},{"location":"archive/openssl_server_certs/#deploy-certificates","title":"Deploy Certificates","text":"<pre><code>Note: This is reference documentation. Use [Chainsmith](../../../../../../../../../pages/xwiki/Infrastructuur/Team%3A+DBA/Werkinstrukties/Postgres/Bouwsteen/Chainsmith/WebHome.html)...\n</code></pre> <pre><code>1. Save the certificates in Ansible Vault as well  \n   - Certificates themselves are located in `vault/certs/`  \n   - Most files there are symlinks.  \n   - You need the `acme-vbepr-v*.acme.corp.com.yml` and `acme-vberm-l01*.acme.corp.com.yml`  \n   - Open with `ansible-vault edit [file]`  \n   - Pay attention to the alignment (2 spaces) under `public_ssl_key`.  \n   - Enter the certificate, followed by the certificates from the chain. The root certificate at the bottom can be omitted.  \n\n2. If necessary, also update the root certificate  \n   - Found in `\"environments/000_cross_env_vars\"` as key `postgres_root_cert`  \n   - It is placed correctly for all PostgreSQL clients  \n   - This should contain the root certificate of the chain. This would be the lowest one from the chain (omitted at point 1).  \n\n3. Deploy via Ansible. Steps: pgbouncer, postgres, and nginx.  \n   - Everything should be automatically reloaded if done correctly, but manual reloading may still be necessary.\n</code></pre>"},{"location":"archive/openssl_server_certs/#tips-and-tricks","title":"Tips and Tricks","text":"<p>See OpenSSL for commands for verification.</p>"},{"location":"archive/openssl_server_certs/#examples-recconf","title":"Examples <code>rec.conf</code>","text":"<p><pre><code>Warning: This is reference documentation. Use [Chainsmith](../../../../../../../../../pages/xwiki/Infrastructuur/Team%3A+DBA/Werkinstrukties/Postgres/Bouwsteen/Chainsmith/WebHome.html)...\n</code></pre> <pre><code>#### BackEnd in A\n\n\\[req\\]\n\n`distinguished_name = req_distinguished_name`\n\n---\n\n```markdown\nreq_extensions=v3_req\n</code></pre></p> <p>no prompt</p> <pre><code>[request distinguished name]\n</code></pre> <p>C=NL</p> <p>UTRECHT</p> <p><code>L = Bilthoven</code></p> <p>= National Institute for Public Health and the Environment (Acme)</p> <pre><code>CN=acme-vbepr-v01a.acme.corp.com\n</code></pre> <p><code>[v3_req]</code></p> <pre><code>keyUsage = keyEncipherment, dataEncipherment\n</code></pre> <pre><code>extendedKeyUsage=serverAuth\n</code></pre> <pre><code>subjectAltName = @alt_names\n</code></pre> <p>[alt_names]</p> <pre><code>DNS.1=acme-vbepr-server1.acme.corp.com\n</code></pre> <p>DNS.2=acme-vbepr-server2.acme.corp.com</p> <pre><code>DNS.3=acme-vbedb-server1.rivv.corp.com\n</code></pre> <pre><code>DNS.4=acme-vbedb-server2.acme.corp.com\n</code></pre> <p>DNS.5=acme-vbedb-server3.acme.corp.com</p> <pre><code>DNS.6=acme-vbedb-server4.acme.corp.com\n</code></pre> <pre><code>DNS.7= acme-vbedb-server5.acme.corp.com\n</code></pre> <pre><code>DNS.8=acme-vbedb-server6.rivv.corp.com\n</code></pre>"},{"location":"developers-guide/developing/","title":"Introduction","text":"<p>Deze documentatie beschrijft hoe evenuele nieuwe features kunnen worden toegevoegd.</p>"},{"location":"developers-guide/developing/#dependencies","title":"Dependencies","text":"<ul> <li>For expansion of the SBB, consider looking at the community project PgVillage, where developments are continuing.</li> <li>For personal expansions (from Acme), adjustments to the code should be made through Ansible Development.</li> <li>For the personal code (from Acme), see:\u00a0https://gitlab.int.corp.com/gurus-db-team/ansible-postgres/-/tree/dev/</li> </ul>"},{"location":"developers-guide/developing/#development","title":"Development","text":"<p>This works through the following steps:</p> <ol> <li> <p>Product Ownership</p> </li> <li> <p>Why is the new feature needed</p> </li> <li>How much can it cost in development and operation</li> <li> <p>What is expected of it (availability, Open Source, support options, etc.)</p> </li> <li> <p>Solution Design</p> </li> <li> <p>What solutions are there</p> </li> <li> <p>Which has preference</p> </li> <li> <p>Proof of Concept (POC)</p> </li> <li> <p>Build it in the POC environment</p> </li> <li> <p>Automation</p> </li> <li> <p>Ansible role</p> </li> <li>Ansible environment adjustments</li> <li> <p>Roll out where desired</p> </li> <li> <p>Management</p> </li> <li> <p>Documentation</p> </li> <li>Transfer to the management team</li> <li> <p>Availability service</p> </li> <li> <p>Support</p> </li> <li>Investigate support options, support wishes and acquire it</li> </ol>"},{"location":"developers-guide/vagrant/","title":"Vagrant","text":""},{"location":"developers-guide/vagrant/#part-of-a-component","title":"Part of a component","text":"<p>Bouwblok Postgresql</p>"},{"location":"developers-guide/vagrant/#introduction","title":"## Introduction","text":"<p>Om PostgreSQL te installeren wordt gebruik gemaakt van Ansible.</p> <p>For developing playbooks and roles, it's handy to use a local environment.</p> <p>dit kan middels Virtuele machines op eigen laptop.</p> <p>VirtualBox is a popular tool for running virtual machines (VMs).</p> <p>Vagrant is a tool that provides a command-line interface (CLI) for interacting with VirtualBox and virtual machines (VMs).</p>"},{"location":"developers-guide/vagrant/#markdown","title":"```markdown","text":"<p>Requirements and Dependencies <pre><code>For using Vagrant and VirtualBox, these programs must first be installed on a local laptop.\n\nOm Ansible playbooks te runnen is Ansible nodig.\n\nSo:\n\n- Ansible\n\n- VirtualBox\n\n- Vagrant\n\n\\- Git\n\n## Uitvoering\n\nTo roll out a few VMs using Vagrant, a `Vagrantfile` is needed.\n\nLogging into these VMs using SSH keys requires an SSH key pair.\n\nCreate this and then place it next to the Vagrantfile in the same directory:\n\ncreate directory: `mkdir ~/Virtualmachines`\n\ncreate a key pair:\n\n```shell\nssh-keygen -t rsa -b &lt;size&gt; -f ~/Virtualmachines/id_rsa\n</code></pre></p> <pre><code>maak Vagrantfile aan met onderstaande vulling\u00a0 ( deze maakt 7 VM's aan, pas eventueel aan naar behoefte ):\n\n```ruby\nVagrant.configure(\"2\") do |config|\n</code></pre>"},{"location":"developers-guide/vagrant/#base-vm-os-configuration","title":"Base VM OS configuration.","text":"<pre><code># config.vm.box = \"generic/rhel8\"\n</code></pre> <pre><code># config.vm.box = \"bento/rockylinux-8\"\n</code></pre> <pre><code>config.vm.box = \"bento/ubuntu-20.04\"\n</code></pre> <pre><code>config.vm.synced_folder '.', '/vagrant', disabled: true\n</code></pre> <pre><code>config.ssh.insert_key = false\n</code></pre> <pre><code>config.vm.provider :virtualbox do |v|\n</code></pre> <p>v.memory = 2048</p> <p><code>vcpus = 4</code></p> <pre><code># v.linked_clone = true\n</code></pre> <p>end</p>"},{"location":"developers-guide/vagrant/#define-three-vms-with-static-private-ip-addresses","title":"Define three VMs with static private IP addresses.","text":"<p>boxes = [</p> <p>{ :name =&gt; \"server1.example.com\", :ip =&gt; \"192.168.56.11\" },</p> <p>{ :name =&gt; \"server2.example.com\", :ip =&gt; \"192.168.56.12\" },</p> <pre><code>{ name: \"server3.example.com\", ip: \"192.168.56.13\" },\n</code></pre> <pre><code>{ name: \"server4.example.com\", ip: \"192.168.56.14\" },\n</code></pre> <pre><code>{ \"name\": \"server5.example.com\", \"ip\": \"192.168.56.15\" },\n</code></pre> <pre><code>{:name =&gt; \"server6.example.com\", :ip =&gt; \"192.168.56.16\"},\n</code></pre> <pre><code>{ name: \"server7.example.com\", ip: \"192.168.56.17\" }\n</code></pre> <p>]</p> <pre><code>if Vagrant.has_plugin?('vagrant-registration')\n</code></pre> <pre><code>config.registration.username = 'mailaddress@hcs-company.com'\n</code></pre> <pre><code>config.registration.password = 'password_redhat.com'\n</code></pre> <p>end</p>"},{"location":"developers-guide/vagrant/#allocate-resources-for-each-of-the-virtual-machines","title":"Allocate resources for each of the virtual machines.","text":"<pre><code>boxes.each do |opts|\n</code></pre> <pre><code>config.vm.define opts[:name] do |config|\n</code></pre> <pre><code>config.vm.hostname = opts[:name]\n</code></pre> <pre><code>config.vm.network :private_network, ip: opts[:ip]\n</code></pre> <pre><code>config.vm.provision \"file\", source: \"id_rsa\", destination: \"/home/vagrant/.ssh/id_rsa\"\n</code></pre> <pre><code>public_key = File.read(\"id_rsa.pub\")\n</code></pre> <pre><code>config.vm.provision :shell, inline: \"\n</code></pre> <pre><code>echo 'Copying Ansible VM public SSH keys to the VM'\n</code></pre> <pre><code>mkdir -p /home/vagrant/.ssh\n</code></pre> <p>chmod 700 /home/vagrant/.ssh</p> <pre><code>echo '#{public_key}' &gt;&gt; /home/vagrant/.ssh/authorized_keys\n</code></pre> <pre><code>chmod -R 600 /home/vagrant/.ssh/authorized_keys\n</code></pre> <pre><code>echo 'Host 192.168.*.*' &gt;&gt; /home/vagrant/.ssh/config\n</code></pre> <pre><code>echo 'StrictHostKeyChecking no' &gt;&gt; /home/vagrant/.ssh/config\n</code></pre> <pre><code>echo 'UserKnownHostsFile /dev/null' &gt;&gt; /home/vagrant/.ssh/config\n</code></pre> <p>chmod -R 600 /home/vagrant/.ssh/config</p> <p><code>, privileged: false</code></p> <p>end</p> <p>end</p> <p>end</p> <p>then run the following command:</p> <pre><code>vagrant up\n</code></pre>"},{"location":"tools/avchecker/","title":"Avchecker","text":"<p>Avchecker is a tool that monitors the availability of PostgreSQL.</p>"},{"location":"tools/avchecker/#how-it-works","title":"How It Works","text":"<ol> <li>It\u2019s a Python script.</li> <li>It connects to a database.</li> <li>It creates (if necessary) a table with one row.</li> <li>Then it endlessly repeats:</li> <li>Reads back the last value.</li> <li>Writes a new value.</li> <li>Checks if the time difference between the previous and new value is longer than 7.5 seconds (adjustable).</li> <li>If the difference exceeds this threshold, it reports to stdout.</li> </ol>"},{"location":"tools/avchecker/#architecture-overview","title":"Architecture Overview","text":"<p>Avchecker runs on all database servers as a service and monitors different connection endpoints:</p> <ul> <li>stolon \u2192 Direct connection to the master.</li> <li>proxy \u2192 Connection to the master through stolon-proxy.</li> <li>router \u2192 Connection via HAProxy on port 5432.</li> </ul>"},{"location":"tools/avchecker/#requirements-and-dependencies","title":"Requirements and Dependencies","text":"<p>Avchecker runs as a service with multiple instances per database server.</p>"},{"location":"tools/avchecker/#components","title":"Components","text":""},{"location":"tools/avchecker/#1-service-files","title":"1. Service Files","text":"<ul> <li>Location: <code>/etc/systemd/system/avchecker@.service</code> (Ansible managed)</li> <li>Instances:</li> <li><code>avchecker@stolon.service</code></li> <li><code>avchecker@proxy.service</code></li> <li><code>avchecker@routerro.service</code></li> <li><code>avchecker@routerrw.service</code></li> </ul>"},{"location":"tools/avchecker/#2-script","title":"2. Script","text":"<ul> <li>Path: <code>/opt/avchecker/avchecker.py</code> (Ansible managed)</li> <li>Python version: <code>3.6.8</code></li> </ul>"},{"location":"tools/avchecker/#3-linux-user","title":"3. Linux User","text":"<ul> <li>User: <code>avchecker</code> (Ansible managed)</li> <li>Authentication: Client certificates (Ansible managed)</li> </ul>"},{"location":"tools/avchecker/#4-table","title":"4. Table","text":"<ul> <li>Database: <code>postgres</code></li> </ul> <pre><code>CREATE TABLE public.avchecker (last timestamptz);\n</code></pre> <p>(Managed by Python script)</p>"},{"location":"tools/avchecker/#5-configuration-files","title":"5. Configuration Files","text":"<ul> <li><code>/etc/default/avchecker_proxy</code>(configuration for connections via stolon-proxy)</li> <li><code>/etc/default/avchecker_stolon</code>(configurations for direct connections to the master)</li> <li><code>/etc/default/avchecker_routerro</code>(configurations for connections via haproxy port 5433)</li> <li><code>/etc/default/avchecker_routerrw</code>(configurations for connections via haproxy port 5432)</li> </ul>"},{"location":"tools/avchecker/#6-connections","title":"6. Connections:","text":"<ul> <li>Each database server creates a connection for every Avchecker instance.</li> <li>A cluster with 4 nodes and a router results in 16 total connections, of which 12 connect to the master database.</li> </ul>"},{"location":"tools/avchecker/#7-endpoints","title":"7. Endpoints","text":"<ul> <li>stolon: direct connection to the master database on one of the nodes.</li> <li>proxy: direct connection to stolon-proxy, which forwards to the master database.</li> <li>routerrw:</li> <li>connection to HAProxy on port 5432</li> <li>from HAProxy to stolon-proxy on the master</li> <li>from stolon-proxy to the master</li> <li>routerro:</li> <li>connection to HAProxy on port 5433</li> <li>from HAProxy to one of the standby instances</li> </ul>"},{"location":"tools/avchecker/#usage","title":"Usage","text":"<p>The purpose of Avchecker is to monitor the connectivity and availability of the PostgreSQL service across different endpoints.</p>"},{"location":"tools/avchecker/#example-scenarios","title":"Example Scenarios","text":"<ul> <li> <p>If the router does not function properly or behaves inconsistently:</p> </li> <li> <p><code>avchecker@proxy</code> and <code>avchecker@stolon</code> do not provide notifications.</p> </li> <li> <p><code>avchecker@routerro</code> and <code>avchecker@routerrw</code> do provide notifications.</p> </li> <li> <p>If the application experiences issues but <code>avchecker@routerro</code> and <code>avchecker@routerrw</code> do not:</p> </li> <li> <p>The problem is most likely in the application, routing, or firewalling up to the router VIP.</p> </li> </ul> <p>This makes Avchecker an excellent diagnostic tool to determine at which level a connectivity issue exists.</p> <p>Avchecker runs continuously as a systemd service and reports any issues in the systemd journal.</p> <p>There is a small difference between the <code>routerro</code> service and the other services.</p>"},{"location":"tools/avchecker/#commands","title":"Commands","text":"<p>Status control works best via <code>journalctl</code> commands.</p> <pre><code>[root@acme-dvppg1db-server2 ~]# journalctl -efu avchecker@routerro | head\n</code></pre> <p>Example logs:</p> <pre><code>-- Logs begin at Sun 2022-10-16 02:26:36 CEST. --\n\nOct 16 00:38:06 acme-dvppg1db-server2 avchecker.py[629825]: cannot execute UPDATE in a read-only transaction\n\nOct 16 03:38:06 acme-dvppg1db-server2 avchecker.py[629825]: cannot execute UPDATE in a read-only transaction\n\nOct 16 03:38:06 acme-dvppg1db-server2 avchecker.py[629825]: cannot execute UPDATE in a read-only transaction\n\nOct 16 03:06:38 acme-dvppg1db-server2 avchecker.py[629825]: cannot execute UPDATE in a read-only transaction\n\nOct 16 02:38:06 acme-dvppg1db-server2 avchecker.py[629825]: cannot execute UPDATE in a read-only transaction\n\nOct 16 02:52:16 acme-dvppg1db-server2 avchecker.py[629825]: cannot execute UPDATE in a read-only transaction\n\nOct 1602:52:16 acme-dvppg1db-server2 avchecker.py[629825]: cannot execute UPDATE in a read-only transaction\n\nOct 16 05:02:16 acme-dvppg1db-server2 avchecker.py[629825]: cannot execute UPDATE in a read-only transaction\n\nOct 1602:52:16 acme-dvppg1db-server2 avchecker.py[629825]: cannot execute UPDATE in a read-only transaction\n</code></pre>"},{"location":"tools/avchecker/#proxy-service-example","title":"Proxy Service Example","text":"<pre><code>[root@acme-dvppg1db-server2 ~]# journalctl -efu avchecker@proxy\n\n#Logs begin at Sun 2022-10-16 02:26:36 CEST.\n\nOct 16 20:25:28 acme-dvppg1db-server2 avchecker.py[629455]: 0:00:08.314879\n\n[root@acme-dvppg1db-server2 ~]# journalctl -efu avchecker@stolon\n\n-- Logs begin at Sun 2022-10-16 02:26:36 CEST.\n\n[root@acme-dvppg1db-server2 ~]# journalctl -efu avchecker@routerrw\n\n-- Logs begin at Sun 2022-10-16 02:26:36 CEST.\n</code></pre> <p>At the first router, we observe a large number of notifications.</p> <p>This is expected behavior because updates fail on a standby instance \u2014 which is logical for <code>routerro</code>.</p> <p>It was decided to include these notifications in the output since they confirm that we are working against a standby instance.</p> <p>The downside is that there are more messages per second on stdout, causing the logs to fill up quickly. Hence the use of <code>| head 20</code>.</p> <p>Systemd handles this efficiently and is not significantly impacted.</p> <p>As an alternative, it can be decided, for example, to give a notification every n times and skip the rest.</p> <p>This feature is (for now) not implemented.</p> <p>For the proxy service, you see a notification:</p> <pre><code>Oct 16 20:25:28 acme-dvppg1db-server2 avchecker.py[629455]: 0:00:08.314879\n</code></pre> <p>This indicates that stolon-proxy took 8.31 seconds to reconnect and rewrite data. Possible causes:</p> <ul> <li>The stolon-proxy was restarted</li> <li>The query took longer than expected due to locking or load delays.</li> </ul> <p>All other services show no notifications.</p>"},{"location":"tools/avchecker/#controls","title":"Controls","text":"<p>If it\u2019s important to check availability, do the following steps.</p> <ul> <li>Log in to one of the database servers (preferably not the master)</li> <li>Check the output of <code>journalctl</code> for all available services</li> </ul>"},{"location":"tools/avchecker/#everything","title":"Everything","text":"<p>Run the following commands:</p> <pre><code>cat /var/log/syslog | grep 'avchecker\\|proxy'\njournalctl -efu avchecker@stolon\njournalctl -efu avchecker@routerrw\n</code></pre>"},{"location":"tools/avchecker/#verify-the-following","title":"Verify the Following","text":"<ul> <li>The services have few interruptions</li> <li>Only <code>routerro</code> should have many and recent rules, typically showing <code>cannot execute UPDATE in a read-only transaction</code>.</li> <li>All other interruptions are points to look at</li> <li>Interruptions from different services don't coincide</li> <li>If interruptions for all avchecker@ services coincide, there was likely an issue that the application also experienced</li> <li>An interruption for a single service probably indicates longer transaction times (postgres was busy) and is likely not noticed by the application</li> <li>The endpoint used by the application works properly</li> <li>Systems with a router config are probably routerw and routerro</li> <li>Systems without a router are likely stolon (direct to master)</li> <li>The routerro service has recent rules</li> <li>and only gives notifications for <code>cannot execute UPDATE in a read-oavchecker@routerronly transaction</code></li> <li>if there are no recent rules, it could be that the service is down or that the entire environment has been down for a long time. First check the avcheker@stolon service and all related controls.</li> </ul> <p>Note</p> <p>For a quick end-to-end check of PostgreSQL with a router, check the <code>@routerrw</code> and <code>@routerro</code> services.</p> <p>See at @routerrw and @routerro what to look out for\u2026</p>"},{"location":"tools/avchecker/#stolon","title":"@stolon","text":"<pre><code>journalctl -efu avchecker@stolon\n</code></pre> <p>If you encounter issues with <code>avchecker@stolon</code>, investigate and resolve them before proceeding</p> <p>Check whether there is a master available, if it can be reached from the current server, and ensure that the client certificates are functioning correctly (for example, they have not expired):</p> <pre><code>[postgres@acme-dvppg1db-server1 ~]$ psql service=master\n</code></pre>"},{"location":"tools/avchecker/#postgresql-1211","title":"PostgreSQL (12.11)","text":"<pre><code>SSL connection (protocol: TLSv1.3, cipher: TLS_AES_256_GCM_SHA384, bits: 256, compression: off)\n\nType \"help\" for help.\n\npostgres=# \\q\n\n[postgres@acme-dvppg1db-server1 ~]$\n</code></pre> <p>Connecting with <code>service=master</code> does the following:</p> <ul> <li>Connects one-to-one to all nodes and</li> <li>Performs a full SSL check (<code>sslmode=verify-full</code>)</li> <li>Authenticates with client certificates (for user <code>postgres</code>)</li> <li>Checks if it is a master instance</li> <li>Goes on to the next node if necessary</li> </ul> <p>If this works then you already know a lot:</p> <ul> <li>The server operates with a server certificate that can be verified with the root certificate (~postgres/.postgresql/root.crt).</li> <li>Client certificates are functioning correctly, can be verified by the server, and have not expired.</li> <li>NOTE: The entire chain expires simultaneously.</li> <li>NOTE: There is certificate monitoring for this certificate!</li> <li>A master instance is available.</li> </ul>"},{"location":"tools/avchecker/#proxy","title":"@proxy","text":"<p>The next step is the stolon-proxy layer. Check it with</p> <pre><code>journalctl -efu avchecker@proxy\n</code></pre> <p>If you notice issues with <code>avchecker@proxy</code>, resolve them before investigating <code>stolon-rw</code>.</p> <p>HAProxy connects through stolon-proxy, so issues with stOLON-PROXY also have consequences.</p> <p>If <code>avchecker@stolon</code> works properly, the best explanation is that the stolon-proxy service did not start correctly (or at all).</p> <p>Other issues may be related to firewalling (but the AvChecker also connects locally, and the stolon proxy connects to the master in the same way as the service=<code>stolon</code> check).</p> <p>Eventueel kan stolon-proxy ook gecontroleerd worden met psql:</p> <pre><code>[postgres@acme-dvppg1db-server1 ~]$ psql service=proxy\n</code></pre> <p>PostgreSQL (12.11)</p> <pre><code>SSL connection (protocol: TLSv1.3, cipher: TLS_AES_256_GCM_SHA384, bits: 256, compression: off)\n</code></pre>"},{"location":"tools/avchecker/#type-help-for-help","title":"Type \"help\" for help.","text":"<pre><code>postgres=# \\q\n\n[postgres@acme-dvppg1db-server1 ~]$\n</code></pre> <p>Connecting with <code>service=proxy</code> does the following:</p> <ul> <li>Connect locally to the stolon-proxy port</li> <li>Stolon-proxy forwards traffic to the master</li> <li>Performs full SSL verification (<code>sslmode=verify-full</code>)</li> <li>Authenticates with client certificates (for user <code>postgres</code>)</li> </ul> <p>If this works out then you already know a lot:</p> <ul> <li>The server operates with a server certificate that can be verified using the root certificate (~postgres/.postgresql/root.crt).</li> <li>The client certificates are functioning properly, can be verified by the server, and have not expired.</li> <li>NOTE: The entire chain expires simultaneously.</li> <li>NOTE: There is certificate monitoring for this certificate!!!</li> <li>A master instance is available.</li> <li>The local stolon-proxy can successfully connect to it.</li> </ul> <p>Read more information in the documentation of stolon.</p>"},{"location":"tools/avchecker/#routerrw","title":"@routerrw","text":"<p>Used when HAProxy + Keepalived + PgRoute66 are deployed.</p> <p>If <code>@stolon</code> and <code>@proxy</code> are healthy but <code>@routerrw</code> fails:</p> <ul> <li>The issue lies in router configuration (HAProxy, Keepalived, or PgRoute66).</li> </ul> <p>Refer to internal documentation for:</p> <ul> <li>HAProxy</li> <li>KeepaliveD</li> <li>PgRoute66</li> </ul> <p>The <code>@routerrw</code> service can also be very well used for an end-to-end check.</p> <p>The best tool for end-to-end control is the <code>@routerro</code> service, but it does not verify if the router also forwards to the primary database server.</p> <p>Therefore, @routerro should also be checked along with @routerrw:</p> <pre><code>journalctl -efu avchecker@routerrw\n</code></pre> <p>Check the output, ensuring that there are not (or very few) lines reporting a timeout.</p> <p>If you don't see any issues but find that insufficient, then check with the HAProxy documentation the output of the <code>show stat</code> command.</p>"},{"location":"tools/avchecker/#routerro","title":"@routerro","text":"<p>When a router configuration (using HAProxy, KeepaliveD and PgRoute66) is used, it is also monitored with the avchecker@routerrw service.</p> <p>If there are issues with <code>@stolon</code> and <code>@proxy</code>, it's best to resolve these first.</p> <p>If there are no issues from @stolon and @proxy, then the problem must be found in the router configuration.</p> <p>This can best be investigated and resolved with the documentation of keepalived, HAProxy and pgRoute66.</p> <p>The @routerro service can also be very well used for an end-to-end check.</p> <p>The main advantage of the <code>@routerro</code> service is that it continuously outputs status and covers all components.</p> <p>The result of this check immediately provides a lot of useful information.</p> <pre><code>journalctl -efu avchecker@routerro | head\n</code></pre> <p>Check the output, ensuring primarily that the router service provides recent rules (such as \"cannot execute UPDATE in a read-only transaction\") and nothing else.</p> <p>This tells you that:</p> <ul> <li>The router is still working properly.</li> <li>At least one of the other services is still updating the table.</li> <li>Streaming replication is still functioning (the other service updates the master and the changes reach this standby).</li> <li>The VIP is still linked to the server with a healthy HAProxy and pgrouter66 (i.e., the primary router is still working well).</li> <li>Server and client certificates are still working properly.</li> </ul>"},{"location":"tools/chainsmith/","title":"Chainsmith","text":"<p>Chainsmith is a community tool that can be configured with a simple YAML file to generate certificate chains.</p> <p>For more information, please refer to:</p> <ul> <li>mTLS</li> <li>Community repository: https://github.com/pgvillage-tools/chainsmith</li> </ul>"},{"location":"tools/chainsmith/#requirements-and-dependencies","title":"Requirements and Dependencies","text":"<p>Within the PostgreSQL deployment, we use Chainsmith as follows:</p> <ul> <li>We install chainsmith on the management server (and updates).</li> <li>We generate and distribute the certificates.</li> <li>When monitoring indicates that the certificates are about to expire, we regenerate and redistribute a new set of certificates.</li> </ul>"},{"location":"tools/chainsmith/#use","title":"Use","text":"<p>The PgVillage chainsmith tole takes care if installation, configuration, running and distributing the certificates.</p>"},{"location":"tools/etcd/","title":"Etcd","text":"<p>Etcd is a key-value store.</p> <p>Etcd consists of:</p> <ul> <li>an etcd service</li> <li>an etcdctl tool to read data from etcd via the command line</li> <li>an API that can be used by other tools (such as Stolon and PGQuartz) to read and use configuration</li> </ul>"},{"location":"tools/etcd/#prerequisites-and-dependencies","title":"Prerequisites and Dependencies","text":"<p>Within the PostgreSQL building block, etcd is used as the consensus mechanism for the cluster layer.</p> <p>Stolon uses etcd to store and distribute cluster-wide configuration, including:</p> <ul> <li><code>pg_hba.conf</code> configuration</li> <li><code>postgreSQL.conf</code> settings</li> <li>Cluster topology \u2014 which database is primary and which are standbys</li> </ul> <p>This configuration is consistently distributed across the entire cluster by etcd, which means:</p> <ul> <li>all nodes see the same configuration, or</li> <li>one or more nodes see that etcd is not available</li> </ul> <p>The stolon ensures that Postgres is available only when the configuration (consistent with consensus) is available for the stolon instance.</p> <p>In addition to stolon, both pgquart and WAL-G (<code>/opt/wal-g/scripts/backup_locked.sh</code>) also directly use etcd.</p>"},{"location":"tools/etcd/#operational-background-information","title":"Operational background information","text":""},{"location":"tools/etcd/#etcd-database-size","title":"Etcd database size","text":"<p>Etcd has its own internal database and retains old information. By default, etcd keeps the size of this database (with retention) to about 2.1 GB.</p> <p>This value can be adjusted in the etcd configuration. However, a larger database can impact etcd performance and, consequently, the availability of Stolon and PostgreSQL.</p> <p>We keep the default configuration.</p> <p>Note</p> <p>In the past, there were issues related to etcd database size. Since then, <code>ETCD_AUTO_COMPACTION_RETENTION</code> has been configured, and the setup has been stable. The following instructions are retained for historical reference and can be used for manual intervention if needed.</p> <p>If issues arise, the database can be manually reduced using compact and defragment commands.</p> <ol> <li>Check Etcd Service Status    If problems occur, check the status of the etcd service as follows:</li> </ol> <p>Example</p> <pre><code>[etcd@gurus-pgsdb-server1 ~]$ systemctl status etcd\n\u25cf etcd.service - Etcd Server\nLoaded: loaded (/etc/systemd/system/etcd.service; enabled; vendor preset: disabled)\nActive: active (running) since Tue 2022-01-19 14:58:39 CET; 2 months 21 days ago\nMain PID: 2142547 (etcd)\nTasks: 10 (limit: 49457)\nMemory: 344.0 MB\nCGroup: /system.slice/etcd.service\n\u2514\u25002142547 /usr/local/bin/etcd\n\nOct 10 10:07:16 gurus-pgsdb-server1 bash[2142547]: {\"level\":\"warn\",\"ts\":\"2022-07-26T11:07:49.311+0200\",\"caller\":\"clientv3/retry_interceptor.go:62\",\"msg\":\"retrying of unary invoker failed\",\"target\":\"endpoint://client-02e576d1-d16f-4610-8fee-0586f7dbe4c1/127.0.0.1:2379\",\"attempt\":0,\"error\":\"rpc error: code = ResourceExhausted desc = etcdserver: mvcc: database space exceeded\"}\n</code></pre> <ol> <li>Check the Database Size and Alarm Status</li> </ol> <p>The size of the database can be queried as follows:</p> <p>Example</p> <pre><code># Requesting Status Endpoints:\netcdctl --write-out=table endpoint status\n\n# Request Alarm Status\netcdctl alarm list\n</code></pre> <ol> <li>Manual Compaction and Defragmentation</li> </ol> <p>This issue can be resolved by manually executing the following commands on all cluster members.</p> <p>Note</p> <p>This should not be executed on all members at the same time, as it affects the availability of etcd and, consequently, also that of PostgreSQL.</p> <p>Example</p> <pre><code># 1) Request an audit\netcdctl get mykey -w=json\n{\"header\":{\"cluster_id\":4788661511241613818,\"member_id\":336793577597500103,\"revision\":700518,\"raft_term\":26}}\n\n# 2) Compact Revision\n[etcd@gurus-pgsdb-server1 ~]$ etcdctl compact 700518\ncompacted revision 700518\n\n# 3) Defragment database\n[etcd@gurus-pgsdb-server1 ~]$ etcdctl defrag\n\n# 4) Remove All Alarms\n[etcd@gurus-pgsdb-server1 ~]$ etcdctl alarm disarm\n</code></pre>"},{"location":"tools/haproxy/","title":"HAProxy","text":"<p>The PostgreSQL component can optionally be deployed with a PostgreSQL router.</p> <p>The router consists of a high-availability (HA) setup with two servers, a virtual IP address, and HAProxy together with PgRoute66 running on both nodes.</p> <p>HAProxy is used to route TCP traffic to the appropriate PostgreSQL server(s), and PgRoute66 is used to direct HAProxy.</p>"},{"location":"tools/haproxy/#requirements","title":"Requirements","text":"<p>On the PostgreSQL router setup, the following components are required for HAProxy to function properly:</p> <ul> <li>The binary is rolled out using the (standard) haproxy rpm (from Satellite)</li> <li>The haproxyconfig can be found in <code>/etc/haproxy/haproxy.cfg</code> and is rolled out and managed via Ansible</li> <li>The haproxy config requires some hardcoded config in the Ansible inventory:<ul> <li><code>haproxy_rw_backends</code></li> <li><code>haproxy_ro_backends</code>   located in <code>environments/{ENV}/group_vars/all/generic.yml</code></li> </ul> </li> <li>HAProxy depends on a properly working PgRoute66</li> <li>The integration between HAProxy and PgRoute66 depends on the following scripts:</li> <li><code>/usr/local/bin/checkpgprimary.sh</code> (Ansible managed)</li> <li><code>/usr/local/bin/checkpgstandby.sh</code> (Ansible managed)</li> </ul>"},{"location":"tools/haproxy/#troubleshooting","title":"Troubleshooting","text":"<p>In principle, no management is required for HAProxy and PgRoute66.</p> <p>For troubleshooting, you can inspect active HAProxy connections using the following command:</p> <pre><code>[me@acme-dvppg1pr-server2 ~]$ echo \"show stat\" | sudo nc -U /var/lib/haproxy/stats | cut -d \",\" -f 1,2,5,6,18,37 | column -s, -t\n\npxname                    svname                                 scur  smax  status   check_status\nhaproxy-stat              FRONTEND                              0     0     OPEN     -\nPostgresReadWrite-frontend FRONTEND                              5     135   OPEN     -\nPostgresReadOnly-frontend  FRONTEND                              4     8     OPEN     -\nPostgresReadWrite-backend  acme-dvppg1db-server1.acme.corp.com   0     0     DOWN     PROCERR\nPostgresReadWrite-backend  acme-dvppg1db-server2.acme.corp.com   5     39    UP       PROCOK\nPostgresReadWrite-backend  acme-dvppg1db-server3.acme.corp.com   1     1     DOWN     PROCERR\nPostgresReadWrite-backend  acme-dvppg1db-server4.acme.corp.com   0     6     DOWN     PROCERR\nPostgresReadWrite-backend  BACKEND                              5     135   UP       -\nPostgresReadOnly-backend   acme-dvppg1db-server1.acme.corp.com   1     3     UP       PROCOK\nPostgresReadOnly-backend   acme-dvppg1db-server2.acme.corp.com   0     4     DOWN     PROCERR\nPostgresReadOnly-backend   acme-dvppg1db-server3.acme.corp.com   2     4     UP       PROCOK\nPostgresReadOnly-backend   acme-dvppg1db-server4.acme.corp.com   1     5     UP       PROCOK\nPostgresReadOnly-backend   BACKEND                              4     8     UP       -\n</code></pre> <p>From this output, you can conclude that:</p> <ul> <li>The primary database server <code>acme-dvppg1db-server2.acme.corp.com</code> is (UP and PROCOK for PostgresReadWrite-backend)</li> <li>Standby databases <code>acme-dvppg1db-server1.acme.corp.com</code>, <code>acme-dvppg1db-server3.acme.corp.com</code>, and <code>acme-dvppg1db-server4.acme.corp.com</code> are (UP and PROCOK for PostgresReadOnly-backend)</li> <li>There is not much traffic</li> <li>currently 5/4 connections for RW/RO</li> <li>maximum 135/8 for RW/RO</li> </ul>"},{"location":"tools/haproxy/#all-done","title":"All Done","text":"<p>Currently, all traffic is routed only to the primary node, via stolon-proxy on port 25432.</p> <ul> <li>Technically, this is convenient (no dual hop to a standby and then on to the primary)</li> <li>During switchover/failover, this means that the traffic will always come out at the primary</li> <li>However, this makes stolon-proxy on the primary node into a Single Point of Failure</li> </ul>"},{"location":"tools/keepalived/","title":"Keepalived","text":"<p>The PostgreSQL component can optionally be deployed with a PostgreSQL router.</p> <p>The router consists of a high-availability (HA) setup of two servers with a virtual IP address (VIP).</p> <p>This VIP is managed using Keepalived.</p>"},{"location":"tools/keepalived/#requirements","title":"Requirements","text":"<p>On the PostgreSQL router setup, the following components are required for Keepalived to function properly:</p> <ul> <li>The binary is rolled out using the (standard) Keepalived RPM (from Satellite).</li> <li>The Keepalived configuration can be found in <code>/etc/keepalived/keepalived.conf</code> and is deployed and managed via Ansible.</li> <li>The value for <code>virtual_router_id</code> should be unique per Keepalived cluster within a segment. This value is pseudo-randomly generated by Ansible.</li> </ul> <p>Important: Ensure that the router nodes are patched one at a time. After patching the first node, verify that it has fully recovered and the VIP is active before proceeding with the second node.</p>"},{"location":"tools/keepalived/#use","title":"Use","text":"<p>Keepalived is configured in balanced mode in this setup.</p> <p>This means both instances have the same priority, and no fallback is triggered when both nodes are available again.</p> <p>Under normal circumstances, both nodes will be available and one of the nodes will have been connected to the VIP (it is undetermined which one).</p> <p>Which VIP has been connected can be checked using the following command:</p>"},{"location":"tools/keepalived/#node-1","title":"Node 1","text":"<pre><code>me@gurus-dbabh-server1 ~/g/ansible-postgres (tmp)&gt; ssh acme-dvppg1pr-server1.acme.corp.com\n[me@acme-dvppg1pr-server1 ~]$ ip a\n\n1: lo: &lt;LOOPBACK, UP, LOWER_UP&gt; MTU 65536 qdisc noqueue state UNKNOWN group default qlen 1000\n  link/loopback 00:00:00:00:00:00 brd 00:00:00:00:00:00\n  inet 127.0.0.1/8 scope host lo\n    valid_lft forever preferred_lft forever\n\n2: ens192: &lt;BROADCAST,MULTICAST,UP,LOWER_UP&gt; mtu 1500 qdisc fq_codel state UP group default qlen 1000\n  Link/Ethernet: 00:50:56:9d:54:47\n  Broadcast (brd): ff:ff:ff:ff:ff:ff\n  inet 10.0.4.*26/23 brd 10.0.4.455 scope global noprefixroute ens192\n    valid_lft forever\n    preferred_lft forever\n\n[me@acme-dvppg1pr-server1 ~]$ logout\nConnection to acme-dvppg1pr-server1.acme.corp.com closed.\n</code></pre>"},{"location":"tools/keepalived/#node-2","title":"Node 2","text":"<pre><code>me@gurus-dbabh-server1 ~/g/ansible-postgres (tmp) &gt; ssh acme-dvppg1pr-server2.acme.corp.com\n[me@acme-dvppg1pr-server2 ~\\]$ ip a\n\n1: lo: &lt;LOOPBACK,UP,LOWER_UP&gt; mtu 65536 qdisc noqueue state UNKNOWN group default qlen 1000\n    link/loopback 00:00:00:00:00:00 brd 00:00:00:00:00:00\n    inet 127.0.0.1/8 scope host lo\n      valid_lft forever preferred_lft forever\n\n2: ens192: &lt;BROADCAST,MULTICAST,UP,LOWER_UP&gt; mtu 1500 qdisc fq_codel state UP group default qlen 1000\n    link/ether 00:50:56:9d:79:5e brd ff:ff:ff:ff:ff:ff\n    inet 10.0.4.*27/23 brd 10.0.4.455 scope global noprefixroute ens192\n      valid_lft forever preferred_lft forever\n    inet 10.0.4.*28/23 scope global secondary ens192\n      valid_lft forever preferred_lft forever\n</code></pre> <p>In this example, node 2 has the extra VIP connected (2 IP addresses: 10.0.4.27 and 10.0.4.28) while node 1 does not (1 IP address: 10.0.4.*26).</p>"},{"location":"tools/keepalived/#todo","title":"ToDo","text":"<p>Keepalived could be provided with a script that checks if HAProxy, PgRoute66, and potentially even PostgreSQL behind HAProxy are accessible.</p> <p>This could perhaps further increase availability.</p> <p>In practice, however, an issue has never occurred that could have been prevented by this.</p>"},{"location":"tools/minio/","title":"MinIO","text":"<p>The standard building block makes backups using WAL-G and WAL-G stores the backups in Cloud Storage (buckets).</p> <p>Within Acme, however, there is no Cloud Storage available and therefore MinIO is deployed so that WAL-G can subsequently transport the backups to MinIO so that they:</p> <p>Note</p> <p>Note that using native Object storage from your Cloud provider, backup solution or storage solution is preferred over a VM withh Minio. This solution is only meant to provide an option whhen no other Object STorage solution is available.</p> <ul> <li>Stored outside the database server.</li> <li>All cluster nodes use a shared backup medium.</li> <li>The file system behind MinIO can be easily included in CommVault via VM Backup.</li> </ul> <p>This means that MinIO can be replaced by a future Cloud Storage (bucket) system once it becomes available within Acme.</p>"},{"location":"tools/minio/#requirements","title":"Requirements","text":"<p>For MinIO, the following components are required:</p> <ul> <li>MinIO and mcli are both run on a separate backup server (e.g., acme-dvppg1 bc-server1)</li> </ul>"},{"location":"tools/minio/#server","title":"Server:","text":"<ul> <li>The binary is deployed to /usr/local/bin/ using the RPMs</li> <li>Source for the minio RPM: https://dl.min.io/server/minio/release/linux-amd64</li> <li>The service file <code>/etc/systemd/system/minio.service</code> is created and maintained by Ansible</li> <li>The configuration <code>/etc/default/minio</code> is deployed and maintained by Ansible</li> <li>TLS certificates (<code>/etc/pki/tls/minio/*</code>) are deployed and maintained by Ansible</li> <li>The root certificate is made available to wal-g by Ansible</li> <li>Runs on port 9091 (default)</li> <li>Stores data in <code>/data/postgres/backup/</code></li> </ul>"},{"location":"tools/minio/#client-mcli","title":"Client (mcli):","text":"<ul> <li>Used by Ansible to create the bucket.</li> <li>Also configured by Ansible (under the <code>minio-server</code> user).</li> <li>The binary is deployed to <code>/usr/local/bin/</code> using the RPMs.</li> <li>Source for the mcli RPM: https://dl.min.io/client/mc/release/linux-amd64</li> </ul>"},{"location":"tools/minio/#usage","title":"Usage","text":"<p>In principle, everything is deployed and maintained through Ansible.</p> <p>If more insight is desired, two things can be done:</p> <p>1: Ensure routing to the management console (SSH proxy, opening ports in the firewall, etc.).</p> <p>You can then use a browser to connect to this port and browse through the bucket (MinIO).</p> <p>2: (Advice) Use <code>mcli</code> on the backup server under the <code>minio-user</code> account.</p> <pre><code>me@gurus-dbabh-server1 ~/g/ansible-postgres (tmp)&gt; ssh acme-dvppg1bc-server1.acme.corp.com\n\n#Last login: Thu Oct 13 21:12:47 2022 from 10.0.6.100\n\n[me@acme-dvppg1bc-server1 ~] $ sudo -i uminio-user\n\n[minio-user@acme-dvppg1bc-server1 ~]$ /usr/local/bin/mcli ls minio/backup/basebackups_005/\n\n[2022-10-11 16:54:12 CEST] 404B STANDARD base_000000010000000C0000000E_backup_stop_sentinel.json\n\n[2022-10-12 09:08:48 CEST]  530B STANDARD base_000000010000000E00000069_D_000000010000000C0000000E_backup_stop_sentinel.json\n\n[2022-10-12 09:29:37 CEST] 404B STANDARD base_000000010000000E0000006B_backup_stop_sentinel.json\n\n[2022-10-12 20:02:06 CEST]   528B STANDARD base_000000010000000E0000006E_D_000000010000000E0000006B_backup_stop_sentinel.json\n\n[2022-10-13 20:02:17 CEST]   559B STANDARD base_000000010000001000000046_D_000000010000000E0000006E_backup_stop_sentinel.json\n\n[2022-10-13 21:31:56 CEST]    0B base_000000010000000C0000000E/\n\n[2022-10-13 21:31:56 CEST] \u00a0\u00a0\u00a0\u00a00B base_000000010000000E00000069_D_000000010000000C0000000E/\n\n[2022-10-13 21:31:56 CEST]    0B base_000000010000000E0000006B/\n\n[2022-10-13 19:31:56 UTC]    0B base_000000010000000E0000006E_D_000000010000000E0000006B/\n\n[2022-10-13 19:31:56 UTC] \u00a0\u00a0\u00a0\u00a00B base_000000010000001000000046_D_000000010000000E0000006E/\n</code></pre>"},{"location":"tools/minio/#tips-tricks","title":"Tips &amp; tricks","text":"<p>When files are deleted, they are temporarily stored in a temporary folder for a certain period of time.</p> <p>This temporary folder is cleaned up every 24 hours, removing all data that is 24 hours or older.</p> <p>In theory, temporary data can therefore be cleaned up 47 hours later (if delays occur).</p> <p>While the storage behind MinIO is typically sized sufficiently, if you need to free up space (e.g., by cleaning old backups), you can manually clean temporary data by restarting the MinIO service:</p> <pre><code>[me@acme-dvppg1bc-server1 ~]$ sudo systemctl restart minio.service\n</code></pre> <p>This only affects a backup and restore (they need to be restarted).</p> <p>Recovery (wal-fetch) and archiving (wal-push) are simply replayed and are not impacted by a MinIO restart.</p>"},{"location":"tools/nagios/","title":"Nagios","text":"<p>For the standard PostgreSQL building block, monitoring has been implemented using Nagios.</p> <p>This documentation describes how it has been implemented, as well as things that can be better.</p>"},{"location":"tools/nagios/#requirements-and-dependencies","title":"Requirements and Dependencies","text":"<ul> <li>check_postgres</li> <li>ansible-postgres role for nagios</li> <li> <p>nagios server (ansible managed with role):</p> </li> <li> <p><code>gurus-nagios-server1.int.corp.com:/opt/nagios/etc/host.cfg.d/{dbserver fqdn}.cfg</code></p> </li> <li> <p><code>gurus-nagios-server1.int.corp.com:/opt/nagios/etc/host.cfg.d/{dbserver fqdn}-custom.cfg</code></p> </li> <li> <p>db servers (ansible managed with role):</p> </li> <li><code>/etc/nrpe.d/check_certs.cfg</code></li> <li><code>/etc/nrpe.d/check_postgres.cfg</code></li> <li><code>/etc/nrpe.d/multi_check.cfg</code></li> <li><code>/etc/nrpe.d/proces_check.cfg</code></li> <li><code>/etc/nrpe.d/service_check.cfg</code></li> <li><code>/opt/gurus/nrpe/pg_multi_db_checks.sh</code></li> <li><code>/opt/gurus/nrpe/check_postgres_*</code></li> <li><code>/opt/gurus/nrpe/check_certs.sh</code></li> </ul>"},{"location":"tools/nagios/#additional-information","title":"Additional information","text":"<p>The configuration is managed in the Ansible inventory at <code>environments/[ENV]/group_vars/hacluster/nagios.yml</code>: <code>nagios_checks</code></p> <p>For the POC environment, the configuration on Nagios itself is disabled:</p> <pre><code># File: environments/poc/group_vars/hacluster/nagios.yml\nnagios_servers[1].enabled: false\n</code></pre>"},{"location":"tools/nagios/#todo","title":"Todo","text":"<p>The following things can be better:</p> <ul> <li>Nagios monitoring on certificates</li> <li>Currently all regularly monitored for <code>~postgres/.postgresql/postgresql.crt</code></li> <li>Requires a sudo rule for the nrpe user</li> <li>Redesign of the solution would be immensely helpful</li> <li>https://github.com/Vonng/pg_exporter</li> <li>Another solution</li> <li>Newer and likely better than check_postgres</li> <li>Requires some scripting (but much simpler from the check_postgres pl script)</li> <li>Monitoring on wal-g</li> <li>Custom to build</li> <li>How do we get the retention?</li> </ul> <p>2-12-2022: A manual action for now, to ensure the proper functioning of the certificate check:</p> <p>Add the file <code>10_sudo_rule_nrpe_postgres</code> to <code>/etc/sudoers.d/</code> with the following content:</p> <pre><code>nrpe ALL=(postgres) NOPASSWD: ALL\n</code></pre>"},{"location":"tools/pgquartz/","title":"PgQuartz","text":"<p>PgQuartz is a community tool used to configure and execute scheduled jobs in PostgreSQL environments.</p> <p>It reads and runs job configurations defined in YAML files.</p> <p>A job definition can include:</p> <pre><code># Example structure\n\nsteps: # What actions need to be performed\nchecks: # How to verify that everything is working correctly\nconnections: # Definitions of connections to the PostgreSQL environment\netcd_config: # pgquartz can wait for the same job on other servers via etcd\ngeneral_config: # General settings (debug mode, log file path, parallel execution, etc.)\n</code></pre>"},{"location":"tools/pgquartz/#requirements-and-dependencies","title":"Requirements and Dependencies","text":"<p>PgQuartz is installed by default with the PostgreSQL SBB, but it is only used for vaccination certificates.</p> <p>More information:</p> <ul> <li>Inventory</li> <li>PgQuartz</li> </ul>"},{"location":"tools/pgroute66/","title":"PgRoute66","text":"<p>The PostgreSQL building block can optionally be executed with a PostgreSQL router.</p> <p>The router consists of a HA setup with 2 servers, each having a Virtual IP address and running HAProxy and PgRoute66 on both nodes.</p> <p>HAProxy is used to route TCP traffic to the correct PostgreSQL server(s), and PgRoute66 is used to control HAProxy. HAProxy.</p> <p>Pgroute66 is an open-source project and is maintained by the community.</p> <ul> <li>rpmbuilder Releases on GitHub</li> <li>pgvillage Repository</li> </ul>"},{"location":"tools/pgroute66/#requirements","title":"Requirements","text":"<p>For the PostgreSQL router setup, the following components are required for PgRoute66 to function correctly:</p> <ul> <li>The binary is deployed via the depgroute66 RPM (from Satellite).</li> <li>The PgRoute66 configuration file is located at: <code>/etc/pgroute66/config.yaml</code>   and is deployed and managed through Ansible.</li> <li>The PgRoute66 service file can be found at: <code>/etc/systemd/system/pgroute66.service</code>   and is also deployed and managed through Ansible.</li> <li>PgRoute66 runs under the pgroute66 Linux user, which is deployed and managed through Ansible.</li> <li>PgRoute66 uses mTLS for PostgreSQL connections through:</li> <li>a client certificate and key (deployed and managed through Ansible)</li> <li>a root certificate to verify the server certificate (deployed and managed through Ansible)</li> <li>all three located in <code>~pgroute66/.postgresql/</code></li> </ul>"},{"location":"tools/pgroute66/#use","title":"Use","text":"<p>PgRoute66 runs as a service and does not require any manual actions.</p> <p>For troubleshooting, check the service logs:</p> <pre><code>[me@pgv-dvppg1pr-server2 ~] $ sudo journalctl -efu pgroute66.service\n\n-- Logs begin at Thu 2022-10-13 02:09:58 CEST.\nOct 13 22:29:19 pgv-dvppg1pr-server2 pgroute66[1608249]: [GIN] 2022/10/13 - 22:29:19 | 200 | 1.780668ms | 127.0.0.1 | GET \"/v1/standbys\"\nOct 13 22:29:19 pgv-dvppg1pr-server2 pgroute66[1608249]: [GIN] 2022/10/13 - 22:29:19 | 200 | 1.951384ms | 127.0.0.1 | GET \"/v1/primary\"\nOct 13 22:29:21 pgv-dvppg1pr-server2 pgroute66[1608249]: [GIN] 2022/10/13 - 22:29:21 | 200 | 1.812061ms | 127.0.0.1 | GET \"/v1/primary\"\nOct 13 22:29:19 pgv-dvppg1pr-server2 pgroute66[1608249]: [GIN] 2022/10/13 - 22:29:19 | 200 | 1.986874ms | 127.0.0.1 | GET \"/v1/standbys\"\nOct 13 22:29:20 pgv-dvppg1pr-server2 pgroute66[1608249]: [GIN]2022/10/13 - 22:29:20 | 200 | 1.917752ms | \u00a0 127.0.0.1 | GET \"/v1/standbys\"\nOct 13 22:29:20 pgv-dvppg1pr-server2 pgroute66[1608249]: [GIN]2022/10/13 - 22:29:20 | 200 | 1.69516ms |     127.0.0.1 | GET \"/v1/primary\"\nOct 13 22:29:21 pgv-dvppg1pr-server2 pgroute66[1608249]: [GIN]2022/10/13 - 22:29:21 | 200 | 1.957809ms |   127.0.0.1 | GET \"/v1/standbys\"\nOct 13 22:29:21 pgv-dvppg1pr-server2 pgroute66[1608249]: [GIN]2022/10/13 - 22:29:21 | 200 | 1.812061ms | \u00a0 127.0.0.1 | GET\u00a0 \"/v1/primary\"\nOct 13 22:29:21 pgv-dvppg1pr-server2 pgroute66[1608249]: [GIN]2022/10/13 - 22:29:21 | 200 | 1.606772ms |  127.0.0.1 | GET \"/v1/standbys\"\n</code></pre> <p>If necessary, the logging can also be temporarily increased by raising the log level to debug in the config file:</p> <pre><code>[me@pgv-dvppg1pr-server2 ~]$ vim /etc/pgroute66/config.yaml\n\n[me@pgv-dvppg1pr-server2 ~]$ grep loglevel /etc/pgroute66/config.yaml\n\nloglevel: develop\n\n[me@pgv-dvppg1pr-server2 ~]$ systemctl restart pgroute66.service\n</code></pre> <p>Note</p> <p>HAProxy and PgRoute66 are loosely coupled, and a successful restart of the pgroute66 has no impact on the availability of the service.</p>"},{"location":"tools/pgroute66/#todo","title":"ToDo","text":"<p>PgRoute66 could be enhanced to detect the primary database based on Stolon configuration in etcd.</p> <p>In that case, PgRoute66 can run on the database server along with it and HAProxy can be used without the <code>external-check</code> and <code>insecure-fork-wanted</code> options.</p>"},{"location":"tools/stolon/","title":"Stolon","text":"<p>Stolon is a PostgreSQL High Availability (HA) tool that uses etcd (or another key/value store) for consensus.</p> <p>This means etcd ensures the entire cluster shares the same configuration, and Stolon uses that configuration to create and manage an HA PostgreSQL cluster.</p> <p>Stolon provides, among other things:</p> <ul> <li>One-time initialization of the cluster</li> <li>Cloning the master to the standbys</li> <li>Management of replication</li> <li>Management of High Availability</li> <li>Routing 25432 to 5432 on the master\u00a0(stolon-proxy)</li> <li>Configuration management (pg_hba.conf and postgresql.conf)</li> </ul> <p>Stolon is an open-source project maintained by the community.</p> <ul> <li>rpmbuilder Releases on GitHub</li> <li>pgvillage Repository</li> </ul> <p>Intent: The goal is to get these two pull requests merged upstream so separate builds are no longer required.</p>"},{"location":"tools/stolon/#requirements","title":"Requirements","text":"<p>For a stolon, the following components are needed:</p> <ul> <li> <p>Stolon binaries   Installed in <code>/usr/local/bin/</code> via the RPM:</p> </li> <li> <p><code>stolonctl</code> \u2013 CLI management tool</p> </li> <li><code>stolon-keeper</code> \u2013 PostgreSQL manager</li> <li><code>stolon-proxy</code> \u2013 TCP proxy for forwarding traffic to the master</li> <li> <p><code>stolon-sentinel</code> \u2013 Cluster manager</p> </li> <li> <p>Systemd files   Deployed by Ansible to <code>/etc/systemd/system/</code>:</p> </li> <li> <p><code>stolon-keeper.service</code></p> </li> <li><code>stolon-proxy.service</code></li> <li> <p><code>stolon-sentinel.service</code></p> </li> <li> <p>The stolon config files   Deployed by Ansible to <code>/etc/sysconfig/</code>:</p> </li> <li> <p><code>stolon-stkeeper</code></p> </li> <li><code>stolon-stproxy</code></li> <li> <p><code>stolon-stsentinel</code></p> </li> <li> <p>a working etcd and configuration to access it</p> </li> <li>We use the standard ports, but if not, then the custom port must be configured</li> <li>We don't yet use etcd with tls / client certificate, but if so, then the certificates need to be available and configured</li> <li>stolon takes care of all postgres matters, including initialization, cloning, and starting Postgres</li> <li>stolon needs the paths to the correct postgres binaries, datadir, and waldir</li> <li>is located in\u00a0<code>/etc/sysconfig/stolon-stkeeper</code></li> </ul>"},{"location":"tools/stolon/#usage","title":"Usage","text":"<p>Configuration and management of Stolon are fully implemented in PgVillage.</p> <p>Furthermore, it is important to be able to use <code>stolonctl</code> in particular to query information and (if necessary) make adjustments.</p> <p><code>stolonctl</code> requires a few configuration parameters so that it knows how to connect to etcd and which cluster it operates on (we use only one, but the configuration is still necessary).</p> <p>With these parameters set, stolonctl can be used under any user (connect to etcd via the API).</p> <p>A few examples:</p>"},{"location":"tools/stolon/#check-status","title":"Check Status","text":""},{"location":"tools/stolon/#setting-up-environment-variables","title":"Setting up Environment Variables","text":"<pre><code>export STOLONCTL_CLUSTER_NAME=stolon-cluster\nexport STOLON_CTL_STORE_BACKEND=etcdv3\n</code></pre>"},{"location":"tools/stolon/#check-cluster-status","title":"Check Cluster Status","text":"<pre><code>[root@acme-dvppg1db-server1 sysconfig]#/usr/local/bin/stolonctl status\n</code></pre> <p>=== Active Sentinels === ID LEADER 3b05c06e true 6b467314 false 7ae581ff false 902f6b4d false</p> <p>=== Active Proxies === ID 09605cac 308fcad1 4a1634ea 534074d4</p> <p>=== Keepers ===</p> <p>UID HEALTHY PG_LISTEN_ADDRESS PG_HEALTHY PG_WANTED_GEN PG_CURRENT_GEN acme_dvppg1db_server1 true 10.0.4.42:5432 true 33 33 acme_dvppg1db_server2 true 10.0.4.43:5432 true 55 55 acme_dvppg1db_server3 true 10.0.4.44:5432 true 33 33 acme_dvppg1db_server4 true 10.0.4.45:5432 true 33 33</p> <p>===Cluster Info=== MasterKeeper: acme_dvppg1db_server2</p> <p>== Keepers/DB Tree == acme_dvppg1db_server2 (master) \u251c\u2500 acme_dvppg1db_server4 \u251c\u2500 acme_dvppg1db_server3 \u2514\u2500 acme_dvppg1db_server1</p>"},{"location":"tools/stolon/#query-the-current-configuration","title":"Query the current configuration","text":"<pre><code>#set up config required for stolonctl\n\nexport STOLONCTL_CLUSTER_NAME=stolen-cluster\n\nexport STOLONCTL_STORE_BACKEND=etcdv3\n\n# Request cluster spec\n\n[root@acme-dvppg1db-server1 sysconfig]# /usr/local/bin/stolonctl spec\n\nOutcome:\n\n{\n\n\"initMode\": \"new\",\n\"defaultSUReplAccessMode\": \"strict\",\n\"pgParameters\": {\n\"archive_command\": \"/opt/wal-g/scripts/archive.sh %p\",\n\"archive_mode\": \"on\",\n\"datestyle\": \"ISO, MDY\",\n\"default_text_search_config\": \"pg_catalog.english\",\n\"dynamic_shared_memory_type\": \"posix\",\n\"effective_cache_size\": \"5822MB\",\n\"idle_in_transaction_session_timeout\": \"60 minutes\",\n\"lc\\_messages\": \"en\\_US.UTF-8\",\n\"LC_MONETARY\": \"en_US.UTF-8\"\n\"lc_numeric\": \"C.UTF-8\",\n\"lc_time\": \"en_US.UTF-8\",\n\"listen_addresses\": \"['*']\",\n\"log_connections\": \"on\",\n\"log_destination\": \"csvlog\",\n\"log_directory\": \"/var/log/postgresql\",\n\"log_disconnections\": \"on\",\n\"log_error_verbosity\": \"verbose\",\n\"log_file_mode\": \"0600\",\n\"log_filename\": \"postgresql-%Y%m%d.log\",\n\"log_line_prefix\": \"%m [%p%: [%l-1] db=%d,user=%u,app=%a,client=%h \",\n\"log_min_duration_statement\": \"5000\",\n\"log_min_error_statement\": \"error\",\n\"log_min_messages\": \"warning\",\n\"log_rotation_age\": \"1d\",\n\"log_rotation_size\": \"1GB\",\n\"log_statement\": \"ddl\",\n\"log_timezone\": \"Europe/Amsterdam\",\n\"log_truncate_on_rotation\": \"on\",\n\"logging_collector\": \"on\",\n\"max_connections\": \"100\",\n\"max_parallel_workers\": \"8\",\n\"max_parallel_workers_per_gather\": \"2\",\n\"max_wal_senders\": \"3\",\n\"max_wal_size\": \"76762MB\",\n\"max_worker_processes\": \"8\",\n\"min_wal_size\": \"25587MB\",\n\"restore_command\": \"/opt/wal-g/scripts/archive_restore.sh %f %p\",\n\"shared_buffers\": \"1940MB\",\n\"ssl\": \"true\",\n\"ssl_ca_file\": \"/data/postgres/data/certs/root.crt\",\n\"ssl_cert_file\": \"/data/postgres/data/certs/server.crt\",\n\"ssl_key_file\": \"/data/postgres/data/certs/server.key\",\n\"statement_timeout\": \"60min\",\n\"timezone\": \"Europe/Amsterdam\",\n\"wal_level\": \"archive\",\n\"work_mem\": \"29813kB\"\n\n},\n\n\"pgHBA\": [\n\n[local] all all ident\n\n\"hostssl postgres avchecker samenet cert\",\n\nhostssl vcbe_db cims_rw sanet scram-sha-256,\n\n\"hostssl all all samenet cert\"\n\n\\]\n</code></pre>"},{"location":"tools/stolon/#update-cluster-config","title":"Update cluster config","text":"<pre><code>set up configuration required for stolonctl\n\nexport STOLONCTL_CLUSTER_NAME=stolon-cluster\n\nexport STOLONCTL_STORE_BACKEND=etcdv3\n\n# adjust cluster spec\n\n/usr/local/bin/stolonctl update -f /data/postgres/data/stolon\\_custom\\_config.yml --patch\n\nThe command should give no output (if it works correctly).\n\n# Patch\n\nIncidentally, the patch offers options to adjust configurations, but this configuration \"comes alongside.\"\n\nSettings that do not receive a value from the custom_config file retain their current configuration.\n\nThe entire configuration can also be completely adjusted by first setting it with the `spec` option in a file, then making adjustments, and finally reloading without the `--patch` option using `stolonctl update`.\n\nset up configuration required for stolonctl\n\nexport STOLONCTL_CLUSTER_NAME=stolon-cluster\n\nexport STOLONCTL_STORE_BACKEND=etcdv3\n\n\n# dumping\n/usr/local/bin/stolonctl spec &gt; /tmp/stolon_custom_config.yml\n\n# adjust\nedit /tmp/stolon_custom_config.yml\n\n\n# Check if it's still JSON.\ncat /tmp/stolon\\_custom\\_config.yml \\| python -m json.tool\n\n#read cluster specification\n/usr/local/bin/stolonctl update -f /data/postgres/data/stolon_custom_config.yml\n</code></pre>"},{"location":"tools/stolon/#help","title":"Help","text":"<p>To list all options of <code>stolonctl</code>, you can run the command with the <code>-h</code> option:</p> <pre><code>stolonctl -h\nOr use the `--help` option:\nstolonctl --help\n\n[root@acme-dvppg1db-server1 sysconfig]# /usr/local/bin/stolonctl\n\nstolon command line client\n\nUsage:\n\nstolonctl [Flags]\nstolonctl [command]\n</code></pre> <p>Available Commands: manage cluster data Manage current cluster data</p> <p>Failkeeper - Force keeper as \"temporarily\" failed. The sentinel will compute new cluster data, considering it as failed, and then restore its state to the actual one.</p> <p>help \u00a0\u00a0Help about any command</p> <pre><code>| Command | Description |\n|----------|-------------|\n| `initialize` | Initialize a new cluster |\n| `promote` | Promote a standby to primary |\n| `register`\u00a0|\u00a0Register stolon keepers for service discovery |\n| `removekeeper` | Removes keeper from cluster data |\n| `spec` | Retrieve current cluster specification |\n| `status` | Display the current cluster status |\n| `update` | Update cluster configuration |\n| `version` | Show stolonctl version |\n\n# Flags:\n\n--cluster-name string            cluster name\n-h, --help                       help for stolonctl\n--kube-context string             name of the kubeconfig context to use\n--kube-namespace string           name of the Kubernetes namespace to use\n\n--kube-resource-kind string       the Kubernetes resource kind to be used to store Stolon cluster data\n                                  and perform sentinel leader election (currently, only \"configmap\" is supported).\n\n--kubeconfig string               path to kubeconfig file. Overrides $KUBECONFIG\n--log-level string                debug, info (default), warn or error (default \"info\")\n--metrics-listen-address string   metrics listen address, e.g., \"0.0.0.0:8080\" (disabled by default)\n--store-backend string            store backend type (etcdv2/etcd, etcdv3, consul, or kubernetes)\n--store-ca-file string            verify certificates of HTTPS-enabled store servers using this CA bundle\n--store-cert-file string          certificate file for client identification to the store\n--store-endpoints string          a comma-delimited list of store endpoints\n                                  (use https scheme for TLS communication)\n                                  (defaults: http://127.0.0.1:2379 for etcd, http://127.0.0.1:8500 for consul)\n--store-key string                private key file for client identification to the store\n--store-prefix string             the store base prefix (default \"stolon/cluster\")\n--store-skip-tls-verify           skip store certificate verification (insecure!!!)\n--store-timeout duration          store request timeout (default 5s)\n--version                         version for stolonctl\n</code></pre> <p>Use <code>stolonctl [command] --help</code> for more information about a specific command.</p>"},{"location":"tools/stolon/#when-nothing-else-works","title":"When Nothing Else Works","text":"<p>In some cases, Stolon may fail to recover automatically.</p> <p>The situation was as follows:</p> <ul> <li>The 3rd node was (according to Stolon) the master.</li> <li>The 3rd node had issues with the datadir and refused to start again.</li> <li>The other nodes were also not okay anymore.</li> </ul> <p>This has been resolved by reinitializing the cluster:</p> <pre><code># Set up configuration required for `stolonctl`.\n\nexport STOLONCTL_CLUSTER_NAME=stolon-cluster\nexport STOLONCTL_STORE_BACKEND=etcdv3\n\n# reinitialize\n/usr/local/bin/stolonctl init\n</code></pre> <p>Note</p> <p>Preferably perform a point-in-time restore!!!</p> <p>There is a \"are you sure\" prompt and then the cluster information is cleared afterwards.</p> <p>Once confirmed, the existing cluster metadata will be cleared.</p> <p>After that, the backup was discarded and restored.</p> <p>This approach is not recommended for production use.</p> <p>It is better to use the Point in time restore procedure. It also executes an init but with the PITR option so that the latest backup is restored from WAL-G.</p>"},{"location":"tools/wal-g/","title":"WAL-G","text":"<p>For backup and restore, we use Wal-G and MinIO.</p> <p>WAL-G is an open-source project maintained by the community.</p> <ul> <li>rpmbuilder Releases on GitHub</li> <li>pgvillage Repository</li> </ul> <p>At the time of this writing, an adapted RPM is being used, which is based on:</p> <ul> <li>The latest version of https://github.com/wal-g/wal-g/tags</li> <li>This change: https://github.com/wal-g/wal-g/pull/1269 (for restoring delta backups of Stolon managed databases)</li> </ul> <p>The intention is to get this pull request merged so that separate builds are no longer needed.</p>"},{"location":"tools/wal-g/#requirements","title":"Requirements","text":"<p>For making wal-g, the following components are needed:</p>"},{"location":"tools/wal-g/#1-wal-g-binary","title":"1. WAL-G Binary","text":"<ul> <li>Installed via RPM to <code>/usr/local/bin/</code></li> </ul>"},{"location":"tools/wal-g/#2-scripts","title":"2. Scripts","text":"<p>Deployed by Ansible to <code>/opt/wal-g/scripts/</code>:</p> <ul> <li><code>archive_restore.sh</code> \u2013 used for catch-up when a standby lags behind or during recovery</li> <li><code>archive.sh</code> \u2013 sends WAL files to MinIO using WAL-G</li> <li><code>backup_locked.sh</code> \u2013 wrapper script using <code>etcdctl lock</code> to ensure backups run on only one server</li> <li><code>backup.sh</code> \u2013 creates a new backup if there isn\u2019t a recent one</li> <li><code>delete.sh</code> \u2013 cleans up old backups</li> <li><code>log_cleanup.sh</code> \u2013 maintains WAL-G log files</li> <li><code>maintenance.sh</code> \u2013 wrapper for <code>log_cleanup.sh</code> and <code>delete.sh</code></li> <li><code>restore.sh</code> \u2013 restores a WAL-G backup (see Point in time restore for more information)</li> </ul>"},{"location":"tools/wal-g/#3-cron-scheduling","title":"3. Cron Scheduling","text":"<ul> <li>Managed by Ansible</li> <li>Cron file: <code>/etc/cron.d/wal-g</code></li> </ul>"},{"location":"tools/wal-g/#4-configuration-file","title":"4. Configuration File","text":"<ul> <li>File: <code>/etc/default/wal-g</code> (maintained by Ansible)</li> <li>Contains:</li> <li>Retention policies</li> <li>Number of delta backups</li> <li>Backup skip intervals</li> </ul>"},{"location":"tools/wal-g/#5-minio-configuration","title":"5. MinIO Configuration","text":"<ul> <li>MinIO runs on the backup server</li> <li>Access configuration is included in <code>/etc/default/wal-g</code></li> <li>Root certificate located in <code>~postgres/.wal-g/certs/</code> (for TLS verification)</li> </ul>"},{"location":"tools/wal-g/#use","title":"Use","text":"<p>Essentially, everything is automated using Ansible, Bash scripts, and cron.</p> <p>You can even perform a Point-in-Time Restore using this procedure: Point in time restore.</p> <p>However, if desired, wal-g can also be invoked as a command-line tool.</p> <p>That works as follows:</p>"},{"location":"tools/wal-g/#configuring-wal-g-for-use","title":"Configuring WAL-G for Use","text":"<p>We do this by sourcing the environment variables using the following command:</p> <p>Read the configuration from <code>/etc/default/wal-g</code></p> <pre><code># Skip lines with a #\n# Export them as variables for subcommands\neval \"$(sed '/#/d;s/^/export /' /etc/default/wal-g)\"\n</code></pre> <p>Then wal-g can be called directly.</p> <p>A few examples:</p> <p>Checking which backups are available</p> <pre><code>[postgres@acme-dvppg1db-server2 ~] $ /usr/local/bin/wal-g-pg backup-list\n...\nname                                                       modified             wal_segment_backup_start\nbase_000000010000000C0000000E                              2022-10-11T14:54:12Z 000000010000000C0000000E\nbase_000000010000000E00000069_D_000000010000000C0000000E   2022-10-12T07:08:48Z 000000010000000E00000069\nbase_000000010000000E0000006B                              2022-10-12T07:29:37Z 000000010000000E0000006B\nbase_000000010000000E0000006N_D_000000010000000E0000006B   2022-10-12T18:02:06Z 000000010000000E0000006N\nbase_000000010000001000000046_D_000000010000000E0000006E   2022-10-13T18:02:17Z 000000010000001000000046\n</code></pre>"},{"location":"tools/wal-g/#backups-can-be-deleted","title":"Backups can be deleted.","text":"<p>For example, you can delete all backups, or (as in the example above) the backups up to <code>base_000000010000000E0000006B</code>:</p>"},{"location":"tools/wal-g/#remove-all-backups-only-report","title":"Remove all backups (only report)","text":"<p>/usr/local/bin/wal-g-pg delete everything</p> <p>Delete all backups up to base_000000010000000E0000006B (only report)</p> <p>/usr/local/bin/wal-g-pg delete before <code>base_000000010000000E0000006B</code></p> <p>Without the <code>--confirm</code> option, WAL-G only provides a report. With <code>--confirm</code>, it actually performs the deletion.</p>"},{"location":"tools/wal-g/#delete-all-backups-report-and-actually-remove","title":"Delete all backups (report and actually remove)","text":"<p>/usr/local/bin/wal-g-pg delete everything --confirm</p> <p>Delete all backups up to base_000000010000000E0000006B (report and actually remove)</p> <p>/delete wal-g-pg postgres delete before base_000000010000000E0000006B --confirm</p>"},{"location":"tools/wal-g/#help","title":"Help","text":"<p>You can view available commands using:</p> <pre><code>[postgres@acme-dvppg1db-server2 ~]$ /usr/local/bin/wal-g-pg\nPostgreSQL backup tool\n\nUsage:\n\nwal-g [command]\n\nAvailable Commands:\n\nbackup-fetch    Fetch a backup from storage\nbackup-list List available backups\nbackup-mark Mark a backup as permanent or impermanent\nbackup-push Create a backup and upload it to storage\n\n# catchup-fetch\n\nFetches an incremental backup from storage\n\n# Catch-up List\n\nPrints available incremental backups.\ncatchup-push: Creates an incremental backup from LSN\nCompletion: Generate shell completion code for the specified shell.\nduplicate: Duplicate specific or all backups\nDelete: Clears old backups and WALs\n\n# flags\nDisplay the list of available global flags for all wal-g commands\nhelp Help about any command\n\npgbackrest\nInteract with pgBackRest backups (beta)\nst (DANGEROUS) Storage tools\n\nwal-fetch Fetches a WAL file from storage\nwal-push    Uploads a WAL file to storage\nwal-receive \u00a0\u00a0Receive WAL stream with postgres Streaming Replication Protocol and push to storage\nwal-restore \u00a0\u00a0Restores WAL segments from storage.\nwal-show     Show information about storage WAL segments, grouped by timelines.\nwal-verify Verify WAL storage folder. Available checks: integrity, timeline.\n\n# Flags:\n--config string   config file (default is $HOME/.walg.json)\n-h, --help          help for wal-g\n--turbo        Ignore all kinds of throttling defined in config\n- `-v`, `--version`\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0 version voor `wal-g`\n\nTo obtain a complete list of all global flags, execute: `wal-g flags`\n\nUse `wal-g [command] --help` for more information about a command.\n\n### Help for the `delete` Command\n\n[postgres@acme-dvppg1db-server2 ~]$ /usr/local/bin/wal-g-pg delete --help\nClears old backups and WALs\n\nUsage:\nwal-g delete [command]\n\nAvailable Commands:\nbefore\neverything\ngarbage\nretain\ntarget\n\nFlags:\n--confirm \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 Confirms backup deletion\n-h, --help  help for delete\n--use-sentinel-time Use backup creation time from sentinel for backups ordering.\n\nGlobal Flags:\n--config string    config file (default is $HOME/.walg.json)\n--turbo     Ignore all kinds of throttling defined in config\n</code></pre> <p>To get the complete list of all global flags, run: <code>wal-g flags</code></p> <p>Use \"wal-g delete [command] --help\"for more information about a command.</p>"},{"location":"tuning/Parallellization_tuning/","title":"PostgreSQL Parallelization Tuning Guide","text":"<p>Since PostgreSQL 10, there is support for parallel query.  </p> <p>The main idea is that the planner takes parallel plans into account. - Parallel plans are more efficient, as parts run in parallel. - Plans are also less efficient, as there is extra setup cost.  </p> <p>As such, the executor will calculate and compare total cost of plans with and without parallelization, and choose the most optimal one.  </p> <p>This page describes the most important options and how they affect the planning or execution phase.</p>"},{"location":"tuning/Parallellization_tuning/#parameters","title":"Parameters","text":""},{"location":"tuning/Parallellization_tuning/#1-parallel_setup_cost","title":"1. <code>parallel_setup_cost</code>","text":"<p>This parameter sets the extra cost to take into account when using parallel query.</p>"},{"location":"tuning/Parallellization_tuning/#2-parallel_tuple_cost","title":"2. <code>parallel_tuple_cost</code>","text":"<p>This parameter sets the extra cost to take into account for every tuple that is passed to the coordinator when using parallel query.</p>"},{"location":"tuning/Parallellization_tuning/#3-max_parallel_workers","title":"3. <code>max_parallel_workers</code>","text":"<p>This parameter limits the number of workers that can be used for parallel query overall.  </p> <p>Example: If one query is using 4 workers, only 2 remain available for another parallel query, and all other parallel queries need to wait for the running queries to finish.</p>"},{"location":"tuning/Parallellization_tuning/#4-max_parallel_workers_per_gather","title":"4. <code>max_parallel_workers_per_gather</code>","text":"<p>This parameter limits the number of workers that can be used for a single running parallel query.</p>"},{"location":"tuning/Parallellization_tuning/#5-min_parallel__size","title":"5. <code>min_parallel_*_size</code>","text":"<p>These settings can be used to limit parallel query only to objects that are larger than the setting.</p> Parameter Description <code>min_parallel_index_scan_size</code> Limits by size of an index <code>min_parallel_relation_size</code> Limits by size of a relation (e.g., table) <code>min_parallel_table_scan_size</code> Limits the size of a table that needs to be scanned (e.g., when indices are used to filter parts of the table)"},{"location":"tuning/Parallellization_tuning/#advice-tuning","title":"Advice &amp; Tuning","text":"<p>Parallel query can have unexpected behavior, and therefore the default settings are very relaxed.  </p> <p>If you want to experiment with parallel query on an active system, you could test with more aggressive settings, by:  </p> <ul> <li>Increasing <code>parallel_setup_cost</code> (so that the planner more quickly considers parallel query)  </li> <li>Increasing <code>max_parallel_workers</code> (so that more processing power is available for parallel queries)  </li> <li>Increasing <code>max_parallel_workers_per_gather</code> (so that more processing power is available for a single parallel query)</li> </ul> <p>When tuning, consider the following: 1. Monitor and actively track the overall runtime of queries. 2. When overall query runtime increases, parallel processing power might be too much at the expense of normal queries. 3. Check effect on complex queries with long duration interactively:    - Start a session    - Set parameters in the session    - Run query with <code>EXPLAIN ANALYZE</code>    - Check the effect on the plan for different values of the settings</p>"},{"location":"tuning/Parallellization_tuning/#partitioning","title":"Partitioning","text":"<p>By splitting a table into multiple partitions, operations on the table can be split into multiple operations (one per partition) and as such run in parallel. Using native partitioning helps with parallel query.</p>"},{"location":"tuning/disk_tuning/","title":"PostgreSQL Disk Tuning Guide","text":"<p>This page describes PostgreSQL configuration options that have an immediate impact on disk utilization and consumption, and how to tune them.</p>"},{"location":"tuning/disk_tuning/#parameters","title":"Parameters","text":""},{"location":"tuning/disk_tuning/#1-min_wal_size","title":"1. <code>min_wal_size</code>","text":"<p>When PostgreSQL WAL files take less space than this setting, the existing WAL files are reused instead of removed and recreated, which is a cheaper operating system operation.  </p> <p>Recommendation: - Always use a separate filesystem for WAL files. - Set <code>min_wal_size</code> to 25% of the filesystem size.</p>"},{"location":"tuning/disk_tuning/#2-max_wal_size","title":"2. <code>max_wal_size</code>","text":"<p>When PostgreSQL WAL files take more space than this setting, PostgreSQL tries to archive as much as possible to free up WAL space.  </p> <p>Recommendation: - Always use a separate filesystem for WAL files. - Set <code>max_wal_size</code> to 75% of the filesystem size.</p>"},{"location":"tuning/disk_tuning/#3-checkpoint_timeout","title":"3. <code>checkpoint_timeout</code>","text":"<p>Checkpoints automatically occur as needed, but if no checkpoint occurs within this duration, PostgreSQL forces a checkpoint after the timeout.  </p> <p>This ensures: 1. WAL files are still archived to the archive location, reducing RPO (Recovery Point Objective) during a disaster.    - This is less critical in some setups. 2. Checkpoints have less work to do when they occur.</p> <p>Recommendation: Align this timeout with the maximum RPO target during a disaster (e.g., when the primary DC fails and the standby in DR is not online).</p>"},{"location":"tuning/disk_tuning/#4-hot_standby_feedback","title":"4. <code>hot_standby_feedback</code>","text":"<p>When this setting is off, standby servers do not send feedback to the primary about active queries. This can lead to the primary cleaning up transaction info that is still required by queries on the standby, resulting in a \u201csnapshot too old\u201d failure.</p> <p>When switched on, standby servers report query info back to the primary.  </p> <p>Behavior: 1. Ensures that snapshot info for standby queries remains preserved. 2. Introduces risk \u2014 snapshot info may not be cleaned, causing WAL file buildup and possible WAL location flooding.</p>"},{"location":"tuning/forbidden_parameters/","title":"PostgreSQL Forbidden Parameters","text":"<p>This is a list of parameters that should never be changed, along with explanations why.</p>"},{"location":"tuning/forbidden_parameters/#parameters","title":"Parameters","text":""},{"location":"tuning/forbidden_parameters/#1-fsync","title":"1. <code>fsync</code>","text":"<p>The <code>fsync</code> parameter controls whether fsync is enabled or disabled. Fsync guarantees disk writes, and failed fsyncs are retried \u2014 commits are halted until successful writes occur. As such, fsync is a crucial part of PostgreSQL\u2019s durability mechanism.</p> <p>Important</p> <ul> <li><code>fsync</code> should never be disabled.  </li> <li>It is considered a debug parameter for developers who know exactly what they are doing.  </li> <li>Disabling it (<code>fsync = false</code>) breaks community and vendor support.  </li> </ul> <p>If you ever run into a PostgreSQL cluster with fsync switched to on, immediately 1. Dump the data. 2. Recreate the instance. 3. Load the data. </p> <p>The instance might have inconsistencies that need to be manually checked and corrected by functional application management.</p>"},{"location":"tuning/forbidden_parameters/#2-autovacuum","title":"2. <code>autovacuum</code>","text":"<p>The <code>autovacuum</code> parameter controls whether autovacuum is enabled or disabled.  </p> <p>Disabling autovacuum introduces many risks, including:  </p> <ul> <li>Statistics are not refreshed, leading to poor query performance.  </li> <li>Excessive I/O operations.  </li> <li>Increased memory pressure.  </li> <li>Frequent flushing and overwriting of pages in <code>shared_buffers</code>, resulting in poor caching.  </li> <li>High CPU usage.  </li> </ul> <p>Additionally, autovacuum handles transaction ID wraparound. If disabled, transaction IDs can exhaust, leading to a system-wide halt.</p> <p>Never disable autovacuum. - Manual vacuuming may provide some control but offers limited benefits. - Autovacuum generally runs exactly when needed. - If manual vacuum automation is introduced, it should only reduce how often autovacuum triggers \u2014 not replace it.</p> <p>If you find a PostgreSQL system where <code>autovacuum</code> is switched off, you should: 1. Turn on autovacuum immediately. 2. Manually vacuum all databases. 3. Wait at least one week before reporting any issues to the community or vendor.</p> <p><code>autovacuum</code> is one of the parameters that, when set to <code>false</code>, voids support from both the PostgreSQL community and commercial vendors.</p>"},{"location":"tuning/hpages/","title":"PostgreSQL HugePages Configuration Guide","text":"<p>This guide provides recommended HugePages settings for PostgreSQL based on the server's available memory (RAM).  </p>"},{"location":"tuning/hpages/#recommended-hugepages-settings","title":"Recommended HugePages Settings","text":"Server RAM Recommended HugePages 8 GB 1150 16 GB 2190 24 GB 3241 32 GB 4289"},{"location":"tuning/hpages/#running-postgresql-with-max_connections-1000","title":"Running PostgreSQL with <code>max_connections = 1000</code>","text":"<p>Before testing HugePages configuration, stop the PostgreSQL service:</p> <pre><code>sudo systemctl stop postgres\n</code></pre>"},{"location":"tuning/hpages/#8-gb-ram-server","title":"8 GB RAM Server","text":"<p>Calculation: 25% Shared_buffers = 2GB = 1024 HP_blocks + 126 blocks overhead</p> <p>Command: <pre><code>/usr/pgsql-16/bin/postgres --shared-buffers=2GB -D /pgdata/fenrir_dev -C shared_memory_size_in_huge_pages\n\n# Output:\n1150\n</code></pre></p>"},{"location":"tuning/hpages/#16-gb-ram-server","title":"16 GB RAM Server","text":"<p>Calculation: 25% Shared_buffers = 4GB = 2048 HP_blocks + 142 blocks overhead</p> <p>Command: <pre><code>/usr/pgsql-16/bin/postgres --shared-buffers=4GB -D /pgdata/fenrir_dev -C shared_memory_size_in_huge_pages\n\n# Output:\n2190\n</code></pre></p>"},{"location":"tuning/hpages/#24-gb-ram-server","title":"24 GB RAM Server","text":"<p>Calculation: 25% Shared_buffers = 6GB = 3072 HP_blocks</p> <p>Command: <pre><code>/usr/pgsql-16/bin/postgres --shared-buffers=6GB -D /pgdata/fenrir_dev -C shared_memory_size_in_huge_pages\n\n# Output:\n3241\n</code></pre></p>"},{"location":"tuning/hpages/#32-gb-ram-server","title":"32 GB RAM Server","text":"<p>Calculation: 25% Shared_buffers = 8GB = 4096 HP_blocks</p> <p>Command: <pre><code>/usr/pgsql-16/bin/postgres --shared-buffers=8GB -D /pgdata/fenrir_dev -C shared_memory_size_in_huge_pages\n\n# Output:\n4289\n</code></pre></p>"},{"location":"tuning/logging_tuning/","title":"PostgreSQL Logging Tuning Guide","text":""},{"location":"tuning/logging_tuning/#parameters","title":"Parameters","text":""},{"location":"tuning/logging_tuning/#1-log_min_messages","title":"1. <code>log_min_messages</code>","text":"<p>The PostgreSQL process can log extra information. Default is to log warning and worse (<code>error</code>, <code>fatal</code>, <code>panic</code>), which is the recommended value. This value is for generic purposes. For statement processing, see <code>log_min_error_statement</code>.</p>"},{"location":"tuning/logging_tuning/#2-log_min_error_statement","title":"2. <code>log_min_error_statement</code>","text":"<p>The PostgreSQL process can, while processing a statement, log extra information. Default is to log error and worse (<code>fatal</code>, <code>panic</code>), which is the recommended value.</p>"},{"location":"tuning/logging_tuning/#3-log_min_duration_statement","title":"3. <code>log_min_duration_statement</code>","text":"<p>PostgreSQL can log statements that take longer than a certain amount of time.  </p> <ul> <li>Default is disabled (-1).  </li> <li>A value of 0 always logs all statements.  </li> <li>Recommended value: </li> <li>Set as per business requirement (defined by the database requestor).  </li> <li>Use 10 seconds when no business requirement exists.</li> </ul>"},{"location":"tuning/logging_tuning/#4-log_autovacuum_min_duration","title":"4. <code>log_autovacuum_min_duration</code>","text":"<p>PostgreSQL can log vacuums initiated by autovacuum that take longer than a certain amount of time. Recommendation: Set this value to 10 seconds, so that longer vacuums stand out.</p>"},{"location":"tuning/logging_tuning/#5-log_checkpoints","title":"5. <code>log_checkpoints</code>","text":"<p>PostgreSQL can track checkpoints. Recommendation: - Set to on and monitor checkpoint frequency. - Once or twice per minute (or less) is acceptable. - More frequent checkpoints (dozens per minute) may indicate excessive WAL usage or that <code>max_wal_size</code> is too small and needs immediate attention.</p>"},{"location":"tuning/logging_tuning/#6-log_connections-log_disconnections","title":"6. <code>log_connections</code>, <code>log_disconnections</code>","text":"<p>PostgreSQL can track connections and disconnections. However, when PEM agents are used (logging every second), enabling these settings floods the logs.  </p> <p>Recommendation: Keep these settings off unless: - There are audit requirements. - You are debugging connection issues.</p>"},{"location":"tuning/logging_tuning/#7-log_duration","title":"7. <code>log_duration</code>","text":"<p>PostgreSQL can log the duration of long-running statements. Recommendation: Enable this feature (very small overhead).</p>"},{"location":"tuning/logging_tuning/#8-log_error_verbosity","title":"8. <code>log_error_verbosity</code>","text":"<p>PostgreSQL can log more or less detail.  </p> <ul> <li>Default value: default (recommended).  </li> <li>Setting this parameter to verbose can be helpful when investigating an issue \u2014 only temporarily.  </li> <li>Setting to terse is not required.</li> </ul>"},{"location":"tuning/logging_tuning/#9-log_statement","title":"9. <code>log_statement</code>","text":"<p>PostgreSQL can be configured to monitor specific statements. Default is none. Recommendation: Set to ddl to ensure schema changes are logged.</p>"},{"location":"tuning/logging_tuning/#10-log_temp_files","title":"10. <code>log_temp_files</code>","text":"<p>PostgreSQL can log temp files that are larger than a certain size. Recommendation: Log temp files when they are larger than <code>work_mem</code>, as this indicates that increasing <code>work_mem</code> could be beneficial.</p>"},{"location":"tuning/memory_tuning/","title":"PostgreSQL Memory Tuning Guide","text":"<p>This page describes PostgreSQL configuration options that have immediate impact on memory utilization and consumption, and how to tune them.</p>"},{"location":"tuning/memory_tuning/#parameters","title":"Parameters","text":""},{"location":"tuning/memory_tuning/#1-shared_buffers","title":"1. <code>shared_buffers</code>","text":"<p>When Postgres starts, the initial process (called postmaster) allocates a block of shared memory. After that, every background process and every connection is forked from postmaster and has access to this segment.  </p> <p>This shared block of memory that every fork has access to is called Shared Buffers, and its size is defined by the <code>shared_buffers</code> parameter. The block contains some small buffers, like <code>wal_buffers</code>, but most of shared buffers are used for page buffers.</p> <p>Recommendations: - As a starting point, <code>shared_buffers</code> can be set to 25% of total server memory, which is also the default that TPA-exec sets it to. - Setting it too large brings risks      - to background Linux processes (e.g., cron) being starved for memory      - Out Of Memory killer that must free memory for the OS to remain functional. - Setting it too small brings risk for double buffering, where only a few pages are kept in shared buffers and everything is cached in the page cache. Double buffering has a small performance impact but is otherwise harmless. - Tune it together with <code>max_connections</code> and <code>work_mem</code>.</p>"},{"location":"tuning/memory_tuning/#2-work_mem","title":"2. <code>work_mem</code>","text":"<p>Every query runs one or more operations (sort, merge, index lookup, etc.). Each operation can request a block of memory, and the size of that block is limited by the <code>work_mem</code> parameter.  </p> <p>Thus, <code>work_mem</code> is one of the most important parameters for limiting private memory, the other being <code>max_connections</code>.</p> <ul> <li>Too large values for <code>work_mem</code> will make the server starve for memory, trigger Out Of Memory killer, or (in worst cases) down the system.  </li> <li>Too small values for <code>work_mem</code> will result in PostgreSQL using temp files more frequently (its form of swapping).  </li> </ul> <p>Tuning <code>work_mem</code>: 1. Start with a sane value. 2. Theoretical lower bound could be 25% of server memory divided by max_connections, complexity and parallelization. 3. Usually you can select a practical value which is 2-4 times bigger 4. Round to a power of (e.a. 16MB). 5. Monitor for memory utilization (how much caching is applied) and if there is little OS caching (0-20%) decrease work_mem (or shared_buffers, or increase server memory) by halving the value 6. Monitor PostgreSQL logs for temp files; if many appear, increase <code>work_mem</code>. 7. Adjust <code>log_temp_files</code> when changing <code>work_mem</code> globally.</p>"},{"location":"tuning/memory_tuning/#3-max_connections","title":"3. <code>max_connections</code>","text":"<p><code>max_connections</code> defines the maximum number of concurrent connections accepted by PostgreSQL. When setting and/or tuning <code>max_connections</code>, the following needs to be taken in consideration  </p> <p>Each connection is a separate process, which adds overhead for the Linux kernel, memory usage, and increases risk of locking.</p> <p>Considerations when tuning: - There\u2019s an optimal \u201chot zone\u201d where you have enough, but not excessive, parallelization. - It depends on storage performance. - It depends on cpu / memory performance.  - It depends on the number of CPU\u02bcs. - Applications with connection poolers can be adjusted to run enough and no too much  parallel connections, alternatively PgBouncer can be used instead.</p> <p>Recommendations: - Keep it below 2000 connections, or use a connection pooler such as PgBouncer. - Ensure enough (virtual) CPUs and memory. - Memory formula guideline: <pre><code>(max_connections * work_mem * query_complexity * parallelization_factor)\n+ shared_buffers + os_memory + caching\n</code></pre> - If needed, lower <code>shared_buffers</code> (when possible) or increase total server memory.</p>"},{"location":"tuning/memory_tuning/#4-effective_cache_size","title":"4. <code>effective_cache_size</code>","text":"<p><code>Effective_cache_size</code> is a parameter with very limited impact. The Postgres query planner uses this parameter as an estimate of cache size, which in turn is used to see see if a block that is not in shared_buffers is expected to be in the filesystem cache (which is faster) or would result in an actual IO. Best is to set it to the theoretical max cache size (which would be something like 75% of OSmemory).</p>"},{"location":"tuning/memory_tuning/#background-tuning-memory-parameters","title":"Background \u2013 Tuning Memory Parameters","text":"<p>To tune memory parameters effectively, follow this structured approach:</p>"},{"location":"tuning/memory_tuning/#step-1-consider-server-memory-allocation","title":"Step 1: Consider server memory allocation","text":"Component Recommended Share Description <code>shared_buffers</code> ~25% Shared memory for pages Private memory ~25% Memory used by connections OS + Kernel processes 10\u201325% Includes <code>ssh</code>, <code>cron</code>, <code>systemd</code>, etc. Cache / Overflow Remainder Used for filesystem caching"},{"location":"tuning/memory_tuning/#step-2-consider-query-complexity","title":"Step 2: Consider query complexity","text":"<ul> <li>Estimate the average number of complex operations (e.g., sorts, merges).  </li> <li>Complex operations consume more memory.  </li> <li>Transactional systems: ~1  </li> <li>Analytical systems: 8\u201316  </li> </ul>"},{"location":"tuning/memory_tuning/#step-3-consider-parallelization","title":"Step 3: Consider parallelization","text":"<ul> <li>Determine how many queries are run in parallel and the number of parallel workers.  </li> <li>Typical values:  </li> <li>1 \u2192 no parallelization  </li> <li>max_parallel_workers_per_gather for 100% parallelization </li> </ul>"},{"location":"tuning/memory_tuning/#step-4-compute-total-memory-allocation","title":"Step 4: Compute total memory allocation","text":"<ul> <li>Consider max_connections</li> </ul>"},{"location":"tuning/postgresql_tuning_intro/","title":"Tuning Guide: PostgreSQL","text":""},{"location":"tuning/postgresql_tuning_intro/#introduction","title":"Introduction","text":"<p>This is a list of the most important PostgreSQL configuration options and how to tune them.</p> <p>You will find: - A list of the most important options and how to tune them - Common issues and config changes that would resolve them - A forbidden section (parameters that should always remain the default values) - Background information for personal reference</p>"},{"location":"tuning/postgresql_tuning_intro/#background","title":"Background","text":"<p>For an intuitive overview of configuration options and their background info, see: PostgresqlCO.NF: PostgreSQL configuration</p>"},{"location":"tuning/tune_parameters/","title":"PostgreSQL Must Tune Parameters","text":"<p>This page describes all parameters that should always be tuned.</p>"},{"location":"tuning/tune_parameters/#parameters","title":"Parameters","text":""},{"location":"tuning/tune_parameters/#1-shared_buffers","title":"1. <code>shared_buffers</code>","text":"<p>Set to 25% of server memory by default. For more tuning, see: PostgreSQL Memory Tuning Guide \u2013 shared_buffers</p>"},{"location":"tuning/tune_parameters/#2-effective_cache_size","title":"2. <code>effective_cache_size</code>","text":"<p>Set to 75% of server memory by default. For more info, see: PostgreSQL Memory Tuning Guide \u2013 effective_cache_size</p>"},{"location":"tuning/tune_parameters/#3-min_wal_size","title":"3. <code>min_wal_size</code>","text":"<p>Set to 25% of the size of the WAL disk. For more info, see: PostgreSQL Disk Tuning Guide \u2013 min_wal_size</p>"},{"location":"tuning/tune_parameters/#4-max_wal_size","title":"4. <code>max_wal_size</code>","text":"<p>Set to 75% of WAL disk. For more info, see: PostgreSQL Disk Tuning Guide \u2013 max_wal_size</p>"},{"location":"tuning/tune_parameters/#5-autovacuum_vacuum_scale_factor","title":"5. <code>autovacuum_vacuum_scale_factor</code>","text":"<p>For large tables (&gt;100G), set <code>autovacuum_vacuum_scale_factor</code> to 0.01, so that vacuum is triggered when 1% (1G) consists of dead tuples. For more info, see: PostgreSQL Vacuum Tuning Guide \u2013 autovacuum_vacuum_threshold and autovacuum_vacuum_scale_factor</p> <p>Example: <pre><code>ALTER TABLE mytable SET (autovacuum_vacuum_scale_factor = 0.01);\n</code></pre></p> <p>Also tune <code>autovacuum_analyse_scale_factor</code> accordingly.</p>"},{"location":"tuning/tune_parameters/#6-autovacuum_max_workers","title":"6. <code>autovacuum_max_workers</code>","text":"<p>Set this to the number of CPUs as a default. For more info, see: PostgreSQL Vacuum Tuning Guide \u2013 General Recommendations</p>"},{"location":"tuning/tune_parameters/#7-max_parallel_workers","title":"7. <code>max_parallel_workers</code>","text":"<p>Set this to 4\u00d7 the number of CPUs as a default. For more info, see: PostgreSQL Parallelization Tuning Guide \u2013 max_parallel_workers</p>"},{"location":"tuning/tune_parameters/#8-max_parallel_workers_per_gather","title":"8. <code>max_parallel_workers_per_gather</code>","text":"<p>Set this to the number of CPUs by default. For more info, see: PostgreSQL Parallelization Tuning Guide \u2013 max_parallel_workers_per_gather</p>"},{"location":"tuning/tune_parameters/#9-log_min_duration-and-log_duration","title":"9. <code>log_min_duration</code> and <code>log_duration</code>","text":"<ul> <li>Check with the business if there are any performance requirements and set <code>log_min_duration</code> accordingly.  </li> <li>When no business requirement exists, use 10s as default.  </li> <li>Set <code>log_duration</code> to on.  </li> </ul> <p>For more info, see: PostgreSQL Logging Tuning Guide \u2013 log_autovacuum_min_duration</p>"},{"location":"tuning/tune_parameters/#10-log_checkpoints","title":"10. <code>log_checkpoints</code>","text":"<p>Set to on, and monitor the number of checkpoints per minute. Alert when there are more than 10 per minute.</p>"},{"location":"tuning/tune_parameters/#11-log_statement","title":"11. <code>log_statement</code>","text":"<p>Set to ddl to ensure schema changes are logged.</p>"},{"location":"tuning/tune_parameters/#12-log_temp_files","title":"12. <code>log_temp_files</code>","text":"<p>Set this equal to the size of <code>work_mem</code>, and consider increasing <code>work_mem</code> when: - Query performance seems poor - There is enough free memory - Many temporary files are logged</p>"},{"location":"tuning/vacuum_tuning/","title":"PostgreSQL Vacuum Tuning Guide","text":"<p>This guide describes the options available to tune (auto)vacuum.  </p> <p>In most situations, the autovacuum tuning options are the most optimal values, but in some corner cases, tuning vacuum really helps.  </p> <p>This page describes the options and when and how to tune them.</p> <p>Note</p> <ol> <li>Never change a winning team. Use defaults unless you need to change them.  </li> <li>Improving storage or increasing CPU power usually is more beneficial.  </li> <li>There is a tradeoff between performance for vacuum vs performance for user processes.  </li> <li>Too little vacuum also has a counterproductive effect where statistics are behind too much.</li> </ol>"},{"location":"tuning/vacuum_tuning/#generic","title":"Generic","text":"<p>autovacuum_vacuum_ prefixed parameter</p> <p>Many parameters prefixed with <code>autovacuum_vacuum_</code> have a counterpart without <code>autovacuum_</code> (so just <code>vacuum_</code> as a prefix).  </p> <p>In these cases, the <code>autovacuum_vacuum_</code> prefixed parameters overrule the <code>vacuum_</code> prefixed items when autovacuum triggers a vacuum.</p>"},{"location":"tuning/vacuum_tuning/#parameters","title":"Parameters","text":""},{"location":"tuning/vacuum_tuning/#1-autovacuum_vacuum_cost_limit-and-autovacuum_vacuum_cost_delay","title":"1. <code>autovacuum_vacuum_cost_limit</code> and <code>autovacuum_vacuum_cost_delay</code>","text":"<p>These parameters set values for <code>vacuum_cost_limit</code> and <code>vacuum_cost_delay</code> when autovacuum runs a vacuum process. They are set softer, so that vacuums run more in the background when run by autovacuum compared to manually with default settings.</p> <p>Basic working: All vacuum operations have cost. The vacuum process tracks cost, and once <code>vacuum_cost_limit</code> is reached, the process halts for <code>vacuum_cost_delay</code>. Together, they throttle a vacuum \u2014 higher <code>vacuum_cost_limit</code> and lower <code>vacuum_cost_delay</code> make vacuum run more aggressively.</p> <p>Tuning advice: In most cases, the defaults are optimal. When autovacuums take too long and the system has available resources, raising <code>autovacuum_vacuum_cost_limit</code> and/or lowering <code>autovacuum_vacuum_cost_delay</code> can make vacuum run more in the foreground.  </p> <p>Usually, faster storage is the better solution.</p>"},{"location":"tuning/vacuum_tuning/#2-autovacuum_vacuum_threshold-and-autovacuum_vacuum_scale_factor","title":"2. <code>autovacuum_vacuum_threshold</code> and <code>autovacuum_vacuum_scale_factor</code>","text":"<p>These parameters define when a table is due for vacuum.</p> <ul> <li><code>autovacuum_vacuum_threshold</code> \u2192 Number of dead tuples that could trigger a vacuum.  </li> <li><code>autovacuum_vacuum_scale_factor</code> \u2192 Fraction (percentage) of dead tuples in the table that triggers vacuum.  </li> </ul> <p>Unexpectedly, not the minimum, but the sum of both is used by autovacuum to decide on running a vacuum.</p> <p>Tuning advice: For most tables, defaults work fine. For large tables (&gt;100 GB), decreasing scale factor (e.g., 0.01 instead of 0.2) triggers autovacuum more often and avoids cluttered datafiles and bad statistics.  </p> <p>These settings can be set per table using: <code>ALTER TABLE mytable SET (key = value);</code></p>"},{"location":"tuning/vacuum_tuning/#3-autovacuum_analyse_threshold-and-autovacuum_analyse_scale_factor","title":"3. <code>autovacuum_analyse_threshold</code> and <code>autovacuum_analyse_scale_factor</code>","text":"<p>Vacuum can run with or without an analyze.  </p> <p>These settings work similarly to <code>autovacuum_vacuum_threshold</code> and <code>autovacuum_vacuum_scale_factor</code>, but instead of triggering a vacuum, they control whether a vacuum should also trigger an analyze (to update statistics).</p>"},{"location":"tuning/vacuum_tuning/#4-autovacuum_max_workers","title":"4. <code>autovacuum_max_workers</code>","text":"<p>This parameter controls how many vacuums run in parallel.  </p> <p>When a cluster has many small tables, increasing this value can significantly improve vacuum performance. However, tuning this also increases autovacuum\u2019s impact on system performance.</p> <p>A good approach is to pair this with adjustments to: - <code>autovacuum_vacuum_cost_limit</code> / <code>autovacuum_vacuum_cost_delay</code>, or - <code>autovacuum_naptime</code> </p> <p>A sane default is to tune this to match the number of vCPUs in the system.</p>"},{"location":"tuning/vacuum_tuning/#5-autovacuum_naptime","title":"5. <code>autovacuum_naptime</code>","text":"<p>This parameter defines how long a vacuum worker sleeps between running a vacuum on a table.</p>"},{"location":"tuning/vacuum_tuning/#general-recommendations","title":"General Recommendations","text":"<p>If autovacuum seems not to keep up, consider the following in order:</p> <ol> <li>Check for <code>io_waits</code>, queue lengths, etc., and improve storage performance if needed.  </li> <li>Increase autovacuum_max_workers (double the number of CPU\u02bcs  power unused during vacuum\u02bcs) .  </li> <li>Add CPUs (and increase <code>autovacuum_max_workers</code> accordingly).  </li> <li>Tune autovacuum_vacuum_cost_limit and autovacuum_vacuum_cost_delay to make vacuum run more in the foreground.  </li> <li>Consider doing this only for specific tables if the issue is not system-wide.</li> </ol>"},{"location":"users-guide/client_connection_failover/","title":"Client Connection Failover","text":"<p>In the Postgres environment, clusters are provided that consist of 3 or more database servers and a backup server. In some environments, currently the two DVP clusters and the POC environment, there are also two proxy servers with a VIP address.</p> <p>In environments with a VIP address, accessing the database is straightforward. By using the VIP, you always connect to the primary database.</p> <p>In environments without VIP and proxy servers, there are usually three database servers where it is not known which one is the primary; this can also change, for example, during patching. To connect to the primary database, there are a few options.</p>"},{"location":"users-guide/client_connection_failover/#requirements-and-dependencies","title":"Requirements and dependencies","text":"<p>The database(s) can be accessed in various ways, each with its own specifics; we describe the following:</p> <ul> <li>psql on Linux  </li> <li>PGAdmin on Windows  </li> <li>DBeaver on Windows</li> </ul>"},{"location":"users-guide/client_connection_failover/#using-psql-service-files","title":"Using psql service files","text":"<p>Using <code>psql</code> on Windows allows you to utilize the 'service file' (<code>.pg_service.conf</code> in <code>$HOME</code>) for configuration, including connection details. Below is an example of how to connect to either a primary or slave server without knowing which one it is. In the 'service file', this can be specified as follows:</p>"},{"location":"users-guide/client_connection_failover/#master-hostgurus-pgsdb-server1intcorpcom-gurus-pgsdb-server2intcorpcom-gurus-pgsdb-server3intcorpcom-port5432-target_session_attrsread-write-sslmodeverify-full-standby-hostgurus-pgsdb-server1intcorpcom-gurus-pgsdb-server2intcorpcom-gurus-pgsdb-server3intcorpcom-port5432-target_session_attrsread-only","title":"<pre><code>#[master]\n\nhost=gurus-pgsdb-server1.int.corp.com, gurus-pgsdb-server2.int.corp.com, gurus-pgsdb-server3.int.corp.com\nport=5432\ntarget_session_attrs=read-write\nsslmode=verify-full\n\n#[standby]\n\nhost=gurus-pgsdb-server1.int.corp.com, gurus-pgsdb-server2.int.corp.com, gurus-pgsdb-server3.int.corp.com\nport=5432\ntarget_session_attrs=read-only\n</code></pre>","text":"<p>This way, the connection can be made with <code>service=master</code> to the primary database and with <code>service=standby</code> to any standby database:</p>"},{"location":"users-guide/client_connection_failover/#psql-on-linux","title":"psql on Linux","text":"<p>The situation:</p> <pre><code>gurus_pgsdb_server1 (master)\n\n\u251c\u2500gurus\\_pgsdb\\_server2\n\u251c\u2500gurus_pgsdb_server3\n\n$ hostname --fqdn\nscc-pgsdb-server2.int.corp.com\n\n$ psql service=master\n\npsql (14.5)\nSSL connection (protocol: TLSv1.3, cipher: TLS_AES_256_GCM_SHA384, bits: 256, compression: off)\nType \"help\" for help.\n\npostgres=# select pg_is_in_recovery();\npg_is_in_recovery\n-----------------\nf\n(1 row)\n\npostgres=# exit;\n\n## Database is not recovering and is therefore the primary!\n\n$ psql service=standby\n\npsql (14.5)\n</code></pre> <p>SSL connection (protocol: TLSv1.3, cipher: TLS_AES_256_GCM_SHA384, bits: 256, compression: off)</p> <p>Enter \"help\" for assistance.</p> <p>The indication of standby and master comes from the 'service file' and can be filled or expanded according to your own understanding.</p> <pre><code>SELECT pg_is_in_recovery();\n`pg_is_in_recovery`\n-----------------\nt\n(1 row)\n\npostgres=# exit;\n</code></pre> <p>Now we see that the database is in <code>recovery_mode</code>, which means it's a standby/slave database.</p>"},{"location":"users-guide/client_connection_failover/#pgadmin-for-windows","title":"PGAdmin for Windows","text":"<p>For PGAdmin, it also applies that this can make use of the aforementioned 'service file'; it works identically to what has been described above. If PGAdmin is used on Windows, then the peculiarity is the name and location of the 'service file', namely:</p> <p>\\%APPDATA%\\postgresql.pg_service.conf (where\u00a0\\%APPDATA\\% refers to the Application Data subdirectory in the user's profile), where on Linux this file is: ~/.pg_service.conf</p> <p>In the windows of PGAdmin, this can be easily filled in further.</p> <p>The 'service file' contains the following:</p> <pre><code># [aermaster]\n\nport=5432\ntarget_session_attrs=read-write\n</code></pre> <p>So if we make the connection this way, it ends up on the master!</p>"},{"location":"users-guide/client_connection_failover/#dbeaver","title":"Dbeaver","text":"<p>For DBeaver on Windows, it works a bit differently; here, the JDBC URL is used, which needs to be entered in the DBeaver window, somewhat depending on the version.</p> <p>An example is: <code>jdbc:postgresql://node1,node2,node3/accounting?targetServerType=primary</code></p> <p>This ensures that the connection is made to the master database where read/write actions are possible.</p> <p>NB: Screenprint is a newer version than what's present on the management server; this is version 22.3.0.</p> <p>If the connection is not made 'correctly', e.g., connected to a readonly (slave) environment, the following 'error message' may be shown when an attempt is made to write something:</p> <pre><code>Caused by: liquibase.exception.DatabaseException: ERROR: cannot execute CREATE TABLE in a read-only transaction [Failed SQL: (0)]\n</code></pre> <p>This will then be displayed, an error message, but clearly explainable!</p>"},{"location":"users-guide/clients/","title":"Clients Introduction","text":"<p>There are many different programming languages that can create a connection to PostgreSQL, and most of them have their own client.</p> <p>This documentation helps the end user get started with successfully setting up a client connection.</p>"},{"location":"users-guide/clients/#dependencies","title":"Dependencies","text":"<ul> <li>JDBC documentation (Java)</li> <li>JDBC</li> <li> <p>https://jdbc.postgresql.org/documentation/ssl/</p> </li> <li> <p>libpq documentation (Python =&gt; psycopg2, C clients, PostgreSQL tools)</p> </li> <li>https://jdbc.postgresql.org/documentation/ssl/</li> <li> <p>https://www.postgresql.org/docs/current/libpq-connect.html#LIBPQ-PARAMKEYWORDS</p> </li> <li> <p>nPGSQL (.NET)</p> </li> <li> <p>https://www.npgsql.org/doc/</p> </li> <li> <p>Information for the applicant:\u00a0mail to the applicant</p> </li> <li> <p>pg_service.conf information</p> </li> <li>https://www.postgresql.org/docs/current/libpq-pgservice.html</li> <li> <p>pg_service</p> </li> <li> <p>Client certificates:</p> </li> <li>General information</li> <li>Generate new certificates</li> <li>openssl commands to convert and read</li> </ul>"},{"location":"users-guide/clients/#instructions","title":"Instructions","text":"<p>Depending on the driver, the configuration must be done in a different way.</p> <p>Check the Dependencies list for the relevant type of driver and read the associated documentation.</p> <p>Furthermore, the following should be taken into account:</p> <ul> <li>Use the latest version of the driver whenever possible</li> <li>At a minimum, use the version that was released after the used PostgreSQL Major. Example:<ul> <li>PostgreSQL 14</li> <li>https://www.postgresql.org/support/versioning/ =&gt; September 30, 2021</li> <li>https://mvnrepository.com/artifact/org.postgresql/postgresql =&gt; at least 42.3.0</li> </ul> </li> <li>The client certificate, private key, and root certificate must be readable by the application user account.</li> <li>Ensure the path is configured correctly. See mail to the requester for options to deliver the information to the end-user.</li> <li>Note: Private keys must only be sent encrypted!!!</li> <li>Direct connection to PostgreSQL (port 5432 of the database hosts)</li> <li>has several advantages<ul> <li>A direct connection avoids (software / network) hops and associated latencies</li> <li>The hba file can be configured more specifically</li> </ul> </li> <li>however, it also has disadvantages<ul> <li>follow the master is 100% dependent on the intelligence of the driver and the software that uses it</li> <li>See the following documentation for Client Connect Failover (follow the master)</li> <li>jdbc: client_connection_failover</li> <li>libpq: https://www.postgresql.org/docs/current/libpq-connect.html#LIBPQ-PARAMKEYWORDS<ul> <li>Check out target_session_attrs</li> <li>Options are version-dependent</li> </ul> </li> </ul> </li> </ul>"},{"location":"users-guide/jdbc/","title":"JDBC Introduction","text":"<p>The JDBC client can be configured with a JDBC URL.</p> <p>This work instruction provides some suggestions for the format.</p> <p>The SSL parameters are included (for convenience).</p>"},{"location":"users-guide/jdbc/#dependencies","title":"Dependencies","text":"<ul> <li>server request</li> <li>set up cluster</li> <li>client connection information</li> <li>Client certificates</li> </ul>"},{"location":"users-guide/jdbc/#instruction","title":"Instruction","text":"<p>Some examples of JDB URLs:</p> <pre><code>#rw:\npostgres://{user}@{db hosts separated by ,}:5432/{database name}?sslmode=verify-full&amp;sslcert=/home/{user}/.postgresql/{user}.crt&amp;sslkey=/home/{user}/.postgresql/{user}.key&amp;sslrootcert=/home/{user}/.postgresql/root.crt\n\n#proxy:\npostgres://{user}@{db hosts separated by ,}:25432/{database name}?sslmode=verify-full&amp;sslcert=/home/{user}/.postgresql/{user}.crt&amp;sslkey=/home/{user}/.postgresql/{user}.key&amp;sslrootcert=/home/{user}/.postgresql/root.crt\n\n#vip_rw:\npostgres://{user}@{vip fqdn}:5432/{database name}?sslmode=verify-full&amp;sslcert=/home/{user}/.postgresql/{user}.crt&amp;sslkey=/home/{user}/.postgresql/{user}.key&amp;sslrootcert=/home/{user}/.postgresql/root.crt\n\n# vip_ro\npostgres://{user}@{vip fqdn}:5433/{database name}?sslmode=verify-full&amp;sslcert=/home/{user}/.postgresql/{user}.crt&amp;sslkey=/home/{user}/.postgresql/{user}.key&amp;sslrootcert=/home/{user}/.postgresql/root.crt\n</code></pre> <p>Starting point is that:</p> <ul> <li><code>{user}</code> is replaced by the Postgres user.  </li> <li>This must match the Postgres user.  </li> <li>This must match the Common Name of the client certificate:  </li> </ul> <pre><code>openssl x509 -text -noout -in /home/{user}/.postgresql/{user}.crt | sed -n '/Subject:/{s/.\\*= //;p}'\n- {vip fqdn} must be replaced with the IP address of the VIP\n- {database name} must be replaced with the name of the Postgres database\n- {list of all hosts, separated by ,} must be replaced with a list of the servers, etc.\n\ngurus-pgsdb-server1.int.corp.com, gurus-pgsdb-server2.int.corp.com, gurus-pgsdb-server3.int.corp.com\n- The certificate files are indeed stored in the respective subdirectory.\n</code></pre>"}]}